# -*- coding: utf-8 -*-
# Olive Young ëª¨ë°”ì¼ ë¸Œëœë“œ ë­í‚¹(1~100) ìˆ˜ì§‘ -> ì›”ë³„ ì‹œíŠ¸ Excel ì €ì¥ -> Google Drive ì—…ë¡œë“œ -> Slack ì•Œë¦¼(ì•„ë§ˆì¡´ í¬ë§·)
# ê·œì¹™
# - ì›” ë°”ë€Œë©´ ê°™ì€ íŒŒì¼ ë‚´ ìƒˆ ì‹œíŠ¸(ì˜ˆ: "25ë…„ 9ì›”")
# - ì—°ë„ ë°”ë€Œë©´ íŒŒì¼ëª…: "ì˜¬ë¦¬ë¸Œì˜_ë¸Œëœë“œ_ë­í‚¹_YYYY.xlsx"
# - Slack í¬ë§·: TOP 10 (ì „ì¼ ëŒ€ë¹„ ë³€ë™ í‘œì‹œ), ê¸‰ìƒìŠ¹/ë‰´ë¸Œëœë“œ/ê¸‰í•˜ë½/ë­í¬ ì•„ì›ƒ
# - ê¸‰ìƒìŠ¹/ê¸‰í•˜ë½: ì „ì²´ Top100 ì¤‘ Â±10ê³„ë‹¨ ì´ìƒ, ê° ìµœëŒ€ 5ê°œ
# - ë­í¬ ì•„ì›ƒ: ì „ì¼ Top70ì— ìˆì—ˆìœ¼ë‚˜ ê¸ˆì¼ Top100ì— ì—†ìŒ, ìµœëŒ€ 5ê°œ

import os
import io
import re
import json
import logging
from datetime import datetime, timedelta, timezone

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter

from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload
from google.oauth2.credentials import Credentials as UserCredentials
from google.auth.transport.requests import Request as GoogleRequest

try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except Exception:
    PLAYWRIGHT_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

SLACK_WEBHOOK = os.environ.get("SLACK_WEBHOOK_URL", "").strip()
GOOGLE_CLIENT_ID = os.environ.get("GOOGLE_CLIENT_ID", "").strip()
GOOGLE_CLIENT_SECRET = os.environ.get("GOOGLE_CLIENT_SECRET", "").strip()
GOOGLE_REFRESH_TOKEN = os.environ.get("GOOGLE_REFRESH_TOKEN", "").strip()
GDRIVE_FOLDER_ID = os.environ.get("GDRIVE_FOLDER_ID", "").strip()

TARGET_URL = "https://m.oliveyoung.co.kr/m/mtn?menu=ranking&tab=brands"
OUT_DIR = "out"

# -------------------- ê³µí†µ --------------------
def kst_now():
    return datetime.now(timezone.utc) + timedelta(hours=9)

def make_session_mobile():
    s = requests.Session()
    s.mount("https://", HTTPAdapter(max_retries=Retry(total=3, backoff_factor=1,
                                                     status_forcelist=[429,500,502,503,504])))
    s.headers.update({
        "User-Agent": ("Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) "
                       "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"),
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8",
        "Referer": "https://m.oliveyoung.co.kr/",
    })
    return s

def send_slack(text: str):
    if not SLACK_WEBHOOK:
        logging.warning("SLACK_WEBHOOK_URL ë¯¸ì„¤ì •")
        return
    try:
        requests.post(SLACK_WEBHOOK, json={"text": text}, timeout=10)
    except Exception:
        logging.exception("Slack ì „ì†¡ ì‹¤íŒ¨")

# -------------------- Drive --------------------
def drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN):
        logging.warning("Google OAuth í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
        return None
    try:
        creds = UserCredentials(
            None,
            refresh_token=GOOGLE_REFRESH_TOKEN,
            client_id=GOOGLE_CLIENT_ID,
            client_secret=GOOGLE_CLIENT_SECRET,
            token_uri="https://oauth2.googleapis.com/token",
            scopes=["https://www.googleapis.com/auth/drive.file"],
        )
        creds.refresh(GoogleRequest())
        return build("drive", "v3", credentials=creds, cache_discovery=False)
    except Exception:
        logging.exception("Drive ì´ˆê¸°í™” ì‹¤íŒ¨")
        return None

def find_file(service, folder_id, name):
    try:
        q = f"name='{name}' and mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'"
        if folder_id:
            q += f" and '{folder_id}' in parents"
        res = service.files().list(q=q, pageSize=1, fields="files(id,name)").execute()
        fs = res.get("files", [])
        return fs[0] if fs else None
    except Exception:
        logging.exception("find_file ì‹¤íŒ¨")
        return None

def download_bytes(service, file_id):
    try:
        req = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, req)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        fh.seek(0)
        return fh.read()
    except Exception:
        logging.exception("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        return None

def upload_new(service, folder_id, filename, data: bytes):
    try:
        media = MediaIoBaseUpload(io.BytesIO(data),
                                  mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                  resumable=False)
        body = {"name": filename}
        if folder_id:
            body["parents"] = [folder_id]
        return service.files().create(body=body, media_body=media, fields="id,webViewLink").execute()
    except Exception:
        logging.exception("ì—…ë¡œë“œ ì‹¤íŒ¨")
        return None

def update_file(service, file_id, data: bytes):
    try:
        media = MediaIoBaseUpload(io.BytesIO(data),
                                  mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                  resumable=False)
        return service.files().update(fileId=file_id, media_body=media).execute()
    except Exception:
        logging.exception("ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
        return None

# -------------------- ìˆ˜ì§‘ --------------------
# footer/í—¤ë” ì˜¤íƒ ë°©ì§€: "nìœ„" íŒ¨í„´ê³¼ í•¨ê»˜ ë¸Œëœë“œëª… í›„ë³´ë¥¼ ì°¾ì•„ 1~100 ìˆœìœ¼ë¡œ ì •ë ¬
_RANK_PAT = re.compile(r"(^|\s)(\d{1,3})\s*ìœ„(?![^\n]*OUT)", re.M)

def is_brand_like(s: str) -> bool:
    if not s:
        return False
    s = s.strip()
    if len(s) < 1 or len(s) > 40:
        return False
    if any(x in s for x in ["ë¸Œëœë“œ", "ë”ë³´ê¸°", "ì¥ë°”êµ¬ë‹ˆ", "ë¡œê·¸ì¸", "ì´ë²¤íŠ¸", "ê³ ê°ì„¼í„°",
                             "ì¸ìŠ¤íƒ€ê·¸ë¨", "í˜ì´ìŠ¤ë¶", "ìœ íŠœë¸Œ", "ëŒ€í‘œì „í™”", "ì±„íŒ…", "ì‚¬ì—…ì", "ê°œì¸ì •ë³´"]):
        return False
    return True

def extract_rank_brand_from_li(li):
    """li ìš”ì†Œì—ì„œ (rank, brand) ì¶”ì¶œ. ì‹¤íŒ¨ ì‹œ (None, None)"""
    try:
        txt = (li.inner_text() or "").strip()
        m = _RANK_PAT.search(txt)
        rank = None
        if m:
            rank = int(m.group(2))
            if not (1 <= rank <= 100):
                rank = None
        # ëª…ì‹œì  ë¸Œëœë“œ ì…€ë ‰í„° ìš°ì„ 
        for sel in [".brand_name", ".brand-name", ".brandNm", ".tx_brand", ".name", ".tit", ".title", "strong"]:
            el = li.query_selector(sel)
            if el:
                nm = (el.inner_text() or "").strip()
                nm = re.sub(r"\s{2,}", " ", nm)
                if is_brand_like(nm):
                    return rank, nm
        # ì´ë¯¸ì§€ alt/aria-label í™œìš©
        for sel in ["img[alt]", "[aria-label]"]:
            el = li.query_selector(sel)
            if el:
                val = (el.get_attribute("alt") or el.get_attribute("aria-label") or "").strip()
                # "ë©”ë””í 1ìœ„" ê°™ì€ ê²½ìš° ë¶„ë¦¬
                val = re.sub(r"\b\d{1,3}\s*ìœ„\b", "", val).strip()
                if is_brand_like(val):
                    return rank, val
        # í…ìŠ¤íŠ¸ ë¼ì¸ì—ì„œ ê°€ì¥ ë¸Œëœë“œìŠ¤ëŸ¬ìš´ í•œ ì¤„ ì„ íƒ
        lines = [re.sub(r"\s{2,}", " ", x.strip()) for x in txt.split("\n") if x.strip()]
        lines = [re.sub(r"\b\d{1,3}\s*ìœ„\b", "", x).strip() for x in lines]
        cand = sorted(lines, key=lambda x: (-is_brand_like(x), -len(x)))
        for c in cand:
            if is_brand_like(c):
                return rank, c
        return rank, None
    except Exception:
        return None, None

def try_internal_api():
    # ë§Œì•½ ëª¨ë°”ì¼ ë‚´ë¶€ APIê°€ ì—´ë ¤ ìˆìœ¼ë©´ ì‚¬ìš©(ì—†ìœ¼ë©´ None ë°˜í™˜)
    session = make_session_mobile()
    candidates = [
        ("https://m.oliveyoung.co.kr/m/api/best/getBrandRankingList.do", {}),
        ("https://m.oliveyoung.co.kr/m/api/best/getBestBrandList.do", {}),
        ("https://m.oliveyoung.co.kr/m/api/best/brandRankList.do", {}),
    ]
    for url, params in candidates:
        try:
            r = session.get(url, params=params, timeout=10)
            if r.status_code != 200:
                continue
            data = r.json()
            for k in ["list", "rows", "items", "brandList", "data", "result"]:
                arr = data.get(k)
                if isinstance(arr, list) and arr:
                    out = []
                    for it in arr:
                        nm = it.get("brandNm") or it.get("brandName") or it.get("nm") or it.get("name")
                        rk = it.get("rank") or it.get("rk") or it.get("ord")
                        if nm and str(nm).strip():
                            out.append((int(rk) if rk else None, str(nm).strip()))
                    out = [(rk, nm) for rk, nm in out if nm]
                    if out:
                        out.sort(key=lambda x: (x[0] if x[0] else 9999))
                        return [nm for rk, nm in out][:100]
        except Exception:
            continue
    return None

def scrape_brands_top100():
    # 1) ë‚´ë¶€ API í›„ë³´
    brands = try_internal_api()
    if brands and len(brands) >= 20:
        return brands[:100]

    # 2) Playwrightë¡œ ëª¨ë°”ì¼ í˜ì´ì§€ ë Œë”ë§ í›„ ì¶”ì¶œ
    if not PLAYWRIGHT_AVAILABLE:
        logging.warning("Playwright ë¯¸ì„¤ì¹˜")
        return []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"])
        device = p.devices.get("iPhone 12") or {}
        context = browser.new_context(**device, locale="ko-KR")
        page = context.new_page()
        page.goto(TARGET_URL, wait_until="networkidle", timeout=60000)
        # ë­í‚¹ íƒ­ í™•ì‹¤íˆ ë¡œë“œ
        page.wait_for_timeout(1500)

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´(ë¬´í•œ ë¡œë”© ëŒ€ë¹„)
        for _ in range(6):
            page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
            page.wait_for_timeout(500)

        # 'ë¸Œëœë“œ'ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ë­í‚¹ ì»¨í…Œì´ë„ˆ ìš°ì„  íƒìƒ‰
        containers = page.query_selector_all("section, div, ul, ol")
        rank_items = []
        for c in containers:
            try:
                part_text = (c.inner_text() or "")
                if ("ë¸Œëœë“œ" in part_text) and ("ë­í‚¹" in part_text or "ìˆœìœ„" in part_text):
                    lis = c.query_selector_all("li")
                    for li in lis:
                        rk, nm = extract_rank_brand_from_li(li)
                        if rk and nm:
                            rank_items.append((rk, nm))
            except Exception:
                pass

        # ë³´ì¡°: í˜ì´ì§€ ì „ì²´ì—ì„œ li ìŠ¤ìº”(í‘¸í„° ì˜¤íƒ ë°©ì§€ ìœ„í•´ 'nìœ„' í•„í„° í•„ìˆ˜)
        if not rank_items:
            for li in page.query_selector_all("li"):
                rk, nm = extract_rank_brand_from_li(li)
                if rk and nm:
                    rank_items.append((rk, nm))

        browser.close()

    # ë­í¬ ì¤‘ë³µ ì œê±° & ì •ë ¬
    by_rank = {}
    for rk, nm in rank_items:
        if 1 <= rk <= 100 and nm and is_brand_like(nm):
            if rk not in by_rank:
                by_rank[rk] = nm
    brands = [by_rank.get(i, "") for i in range(1, 101)]
    brands = [b for b in brands if b]
    return brands[:100]

# -------------------- ì—‘ì…€ --------------------
def month_sheet(dt: datetime) -> str:
    return f"{dt.year % 100}ë…„ {dt.month}ì›”"

def file_name(dt: datetime) -> str:
    return f"ì˜¬ë¦¬ë¸Œì˜_ë¸Œëœë“œ_ë­í‚¹_{dt.year}.xlsx"

def ensure_header(ws):
    if ws.max_row == 1 and ws.max_column == 1 and ws["A1"].value is None:
        ws.append(["ë‚ ì§œ"] + [f"{i}ìœ„" for i in range(1, 101)])
        ws.column_dimensions["A"].width = 14
        for c in range(2, 102):
            ws.column_dimensions[get_column_letter(c)].width = 12

def write_today(ws, dt: datetime, brands):
    d = dt.date().isoformat()
    row = None
    for r in range(2, ws.max_row + 1):
        if str(ws.cell(r, 1).value) == d:
            row = r
            break
    if row is None:
        row = ws.max_row + 1
    ws.cell(row=row, column=1, value=d)
    for i in range(100):
        ws.cell(row=row, column=2 + i, value=(brands[i] if i < len(brands) else ""))

def read_prev_map(wb, now_dt: datetime):
    """ì „ì¼(ë˜ëŠ” ì§ì „) ë­í¬ ë§µ brand->rank. ê°™ì€ ë‹¬ì—ì„œ ì „ì¼ ì—†ìœ¼ë©´ ì´ì „ ë‹¬ ì‹œíŠ¸ì˜ ë§ˆì§€ë§‰ í–‰ ì‚¬ìš©."""
    def row_to_map(ws, row):
        m = {}
        for i in range(1, 101):
            name = (ws.cell(row=row, column=1 + i).value or "").strip() if ws.cell(row=row, column=1 + i).value else ""
            if name:
                m[name] = i
        return m

    # í›„ë³´: ê°™ì€ ì‹œíŠ¸ì—ì„œ ì˜¤ëŠ˜ ì´ì „ ë‚ ì§œ ì¤‘ ê°€ì¥ ìµœê·¼
    cur = month_sheet(now_dt)
    cand = []
    if cur in wb.sheetnames:
        ws = wb[cur]
        for r in range(2, ws.max_row + 1):
            try:
                d = str(ws.cell(r, 1).value)
                if d and d < now_dt.date().isoformat():
                    cand.append((ws, r))
            except Exception:
                continue
    # ì´ì „ ë‹¬ ì‹œíŠ¸(ìˆìœ¼ë©´) ë§ˆì§€ë§‰ í–‰
    prev_month_dt = (now_dt.replace(day=1) - timedelta(days=1))
    prev_name = month_sheet(prev_month_dt)
    if prev_name in wb.sheetnames:
        ws2 = wb[prev_name]
        if ws2.max_row >= 2:
            cand.append((ws2, ws2.max_row))

    if not cand:
        return {}
    # ê°€ì¥ ìµœê·¼ í–‰ ì„ íƒ(ë§ˆì§€ë§‰ í›„ë³´)
    ws, r = cand[-1]
    return row_to_map(ws, r)

# -------------------- ìŠ¬ë™ ë©”ì‹œì§€ --------------------
def format_delta(today_rank, prev_rank):
    if prev_rank is None:
        return "NEW"
    diff = prev_rank - today_rank
    if diff > 0:
        return f"â†‘{diff}"
    elif diff < 0:
        return f"â†“{abs(diff)}"
    else:
        return "â€”"

def build_sections(today_list, prev_map):
    # today_map
    today_map = {b: i+1 for i, b in enumerate(today_list)}
    # TOP10
    top10_lines = []
    for i in range(min(10, len(today_list))):
        b = today_list[i]
        pr = prev_map.get(b)
        delta = format_delta(i+1, pr)
        top10_lines.append(f"{i+1}. ({delta}) {b}")

    # ê¸‰ìƒìŠ¹/ê¸‰í•˜ë½
    ups, downs = [], []
    for b, tr in today_map.items():
        pr = prev_map.get(b)
        if pr is None:
            continue
        diff = pr - tr  # +ë©´ ìƒìŠ¹
        if diff >= 10:
            ups.append((diff, b, pr, tr))
        elif diff <= -10:
            downs.append((abs(diff), b, pr, tr))
    ups.sort(key=lambda x: (-x[0], x[3]))      # í° í­ ìš°ì„ 
    downs.sort(key=lambda x: (-x[0], x[3]))    # í° í­ ìš°ì„ 
    ups = [f"- {b} {pr}ìœ„ â†’ {tr}ìœ„ (â†‘{d})" for d, b, pr, tr in ups[:5]]
    downs = [f"- {b} {pr}ìœ„ â†’ {tr}ìœ„ (â†“{d})" for d, b, pr, tr in downs[:5]]

    # ë‰´ë¸Œëœë“œ(IN)
    new_in = [b for b in today_list if b not in prev_map]
    new_in_lines = [f"- {b} NEW â†’ {today_map[b]}ìœ„" for b in new_in[:5]]

    # ë­í¬ ì•„ì›ƒ(ì „ì¼ <=70 ì´ì—ˆëŠ”ë° ê¸ˆì¼ Top100ì—ì„œ ì‚¬ë¼ì§)
    prev_top70 = [(b, r) for b, r in prev_map.items() if r <= 70]
    outs = []
    today_set = set(today_list)
    for b, r in sorted(prev_top70, key=lambda x: x[1]):
        if b not in today_set:
            outs.append((b, r))
    out_lines = [f"- {b} {r}ìœ„ â†’ OUT" for b, r in outs[:5]]

    inout_summary = f"{len(new_in)}ê°œ IN, {len(outs)}ê°œ OUT"

    return top10_lines, ups, new_in_lines, downs, out_lines, inout_summary

# -------------------- ë©”ì¸ --------------------
def main():
    now = kst_now()
    logging.info("ë¸Œëœë“œ ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘")

    brands = scrape_brands_top100()
    if len(brands) < 20:
        send_slack("âŒ *ì˜¬ë¦¬ë¸Œì˜ ëª¨ë°”ì¼ ë¸Œëœë“œ ë­í‚¹* ìˆ˜ì§‘ ì‹¤íŒ¨ (ë°ì´í„° ë¶€ì¡±)")
        return 1

    # Excel íŒŒì¼ ì¤€ë¹„
    svc = drive_service()
    fname = file_name(now)
    meta = find_file(svc, GDRIVE_FOLDER_ID, fname)

    wb = None
    if meta:
        data = download_bytes(svc, meta["id"])
        if data:
            wb = load_workbook(io.BytesIO(data))
    if wb is None:
        wb = Workbook()

    sheet = month_sheet(now)
    if sheet in wb.sheetnames:
        ws = wb[sheet]
    else:
        ws = wb.create_sheet(title=sheet)
    if "Sheet" in wb.sheetnames and len(wb.sheetnames) > 1:
        try:
            wb.remove(wb["Sheet"])
        except Exception:
            pass

    ensure_header(ws)
    prev_map = read_prev_map(wb, now)
    write_today(ws, now, brands)

    # ì €ì¥ & ì—…ë¡œë“œ
    os.makedirs(OUT_DIR, exist_ok=True)
    path = os.path.join(OUT_DIR, fname)
    wb.save(path)
    with open(path, "rb") as f:
        xbytes = f.read()

    view_link = None
    if meta:
        update_file(svc, meta["id"], xbytes)
        try:
            view_link = svc.files().get(fileId=meta["id"], fields="webViewLink").execute().get("webViewLink")
        except Exception:
            pass
    else:
        created = upload_new(svc, GDRIVE_FOLDER_ID, fname, xbytes)
        view_link = (created or {}).get("webViewLink")

    # ìŠ¬ë™ ë©”ì‹œì§€
    top10, ups, newins, downs, outs, inout_summary = build_sections(brands, prev_map)
    msg = [
        f"*ì˜¬ë¦¬ë¸Œì˜ ëª¨ë°”ì¼ ë¸Œëœë“œ ë­í‚¹ 100* â€” {now.strftime('%Y-%m-%d')}",
        f"- ì›” ì‹œíŠ¸: `{sheet}` / íŒŒì¼: `{fname}`",
        "",
        "*TOP 10*",
        *top10,
        "",
        "ğŸ”¥ *ê¸‰ìƒìŠ¹* (10ê³„ë‹¨â†‘, ìµœëŒ€ 5ê°œ)" if ups else "ğŸ”¥ *ê¸‰ìƒìŠ¹*: í•´ë‹¹ ì—†ìŒ",
        *(ups if ups else []),
        "",
        "ğŸ†• *ë‰´ë¸Œëœë“œ* (ì˜¤ëŠ˜ Top100 ì‹ ê·œ, ìµœëŒ€ 5ê°œ)" if newins else "ğŸ†• *ë‰´ë¸Œëœë“œ*: í•´ë‹¹ ì—†ìŒ",
        *(newins if newins else []),
        "",
        "ğŸ“‰ *ê¸‰í•˜ë½* (10ê³„ë‹¨â†“, ìµœëŒ€ 5ê°œ)" if downs else "ğŸ“‰ *ê¸‰í•˜ë½*: í•´ë‹¹ ì—†ìŒ",
        *(downs if downs else []),
        "",
        "â¬…ï¸ *ë­í¬ ì•„ì›ƒ* (ì „ì¼ Top70 â†’ ê¸ˆì¼ OUT, ìµœëŒ€ 5ê°œ)" if outs else "â¬…ï¸ *ë­í¬ ì•„ì›ƒ*: í•´ë‹¹ ì—†ìŒ",
        *(outs if outs else []),
        "",
        f"â¡ï¸ ë­í¬ ì¸&ì•„ì›ƒ ìš”ì•½: {inout_summary}",
    ]
    if view_link:
        msg.append(f"\n<{view_link}|Google Driveì—ì„œ ì—´ê¸°>")
    send_slack("\n".join(msg))

    logging.info("ì™„ë£Œ")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
