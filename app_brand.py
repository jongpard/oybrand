import asyncio
import os
import re
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from calendar import monthrange
from collections import OrderedDict

import json
import requests

from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment, Font

from playwright.async_api import async_playwright

# Google Drive
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.credentials import Credentials

# -------------------------
# ì„¤ì •
# -------------------------
KST = ZoneInfo("Asia/Seoul")
URL = "https://m.oliveyoung.co.kr/m/mtn?menu=ranking&tab=brands"

OUTPUT_DIR = "data"
XLSX_NAME = "ì˜¬ë¦¬ë¸Œì˜_ë¸Œëœë“œ_ìˆœìœ„.xlsx"
OUTPUT_PATH = os.path.join(OUTPUT_DIR, XLSX_NAME)

# í™˜ê²½ë³€ìˆ˜(Secrets)
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL", "")
GDRIVE_FOLDER_ID   = os.environ.get("GDRIVE_FOLDER_ID", "")
GOOGLE_CLIENT_ID   = os.environ.get("GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.environ.get("GOOGLE_CLIENT_SECRET", "")
GOOGLE_REFRESH_TOKEN = os.environ.get("GOOGLE_REFRESH_TOKEN", "")

# -------------------------
# Playwright (ëª¨ë°”ì¼) í¬ë¡¤ë§
# -------------------------
async def close_banners(page):
    # ë³´ì´ëŠ” íŒì—…/ì•±ìœ ë„ ë‹«ê¸° ì‹œë„ (ìˆì–´ë„ ì—†ì–´ë„ í†µê³¼)
    candidates = [
        "text=ë‹«ê¸°", "text=ì·¨ì†Œ", "text=ë‚˜ì¤‘ì—", "role=button[name='ë‹«ê¸°']",
        "button[aria-label*='ë‹«ê¸°']", "[class*='btn_close']"
    ]
    for sel in candidates:
        try:
            el = await page.query_selector(sel)
            if el:
                await el.click(timeout=1000)
        except Exception:
            pass
    try:
        await page.mouse.wheel(0, 600)
        await page.wait_for_timeout(400)
    except Exception:
        pass

async def scroll_to_bottom(page, pause_ms=700, max_loops=24):
    last_h = 0
    for _ in range(max_loops):
        try:
            h = await page.evaluate("document.body.scrollHeight")
            if h == last_h:
                break
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(pause_ms)
            last_h = h
        except Exception:
            break

async def parse_brands(page):
    """ë¸Œëœë“œëª… Top100 ì¶”ì¶œ(êµ¬ì¡° ë³€í™” ëŒ€ë¹„ ë‹¤ì¤‘ ì…€ë ‰í„°)"""
    texts = []

    # 1) ë¸Œëœë“œ ì „ìš© í´ë˜ìŠ¤ ì¶”ì •
    preferred = [
        ".brand", ".brandName", ".tx_brand", "span[class*='brand']",
        "strong[class*='brand']", "em[class*='brand']",
    ]
    for sel in preferred:
        nodes = await page.query_selector_all(sel)
        for n in nodes:
            try:
                t = (await n.inner_text()).strip()
                if t:
                    texts.append(t)
            except Exception:
                pass

    # 2) ë¦¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì—ì„œ í›„ë³´ ì¶”ì¶œ(ë°±ì—…)
    if len(texts) < 80:
        list_sels = ["ul li", "ol li", "[class*='rank'] li", "[class*='list'] li"]
        for lsel in list_sels:
            items = await page.query_selector_all(lsel)
            for li in items:
                try:
                    raw = (await li.inner_text()).strip()
                    if not raw:
                        continue
                    lines = [re.sub(r"\s+", " ", s).strip() for s in raw.splitlines()]
                    cand = None
                    for s in lines:
                        if len(s) <= 1:
                            continue
                        s2 = re.sub(r"^[#\d\.\-\)\(]+", "", s).strip()
                        if (
                            s2
                            and not re.match(r"^\d+$", s2)
                            and "ë­í‚¹" not in s2
                            and "ë¸Œëœë“œ" not in s2
                            and "TOP" not in s2.upper()
                        ):
                            cand = s2
                            break
                    if cand:
                        texts.append(cand)
                except Exception:
                    pass

    uniq = list(OrderedDict.fromkeys([t.strip() for t in texts if t.strip()]))
    cleaned = []
    for t in uniq:
        if 2 <= len(t) <= 30 and len(t.split()) <= 6:
            cleaned.append(t)
    return cleaned[:100]

async def scrape_top100():
    async with async_playwright() as p:
        iphone = p.devices.get("iPhone 13 Pro")
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(**iphone, locale="ko-KR")
        page = await context.new_page()

        await page.goto(URL, wait_until="domcontentloaded", timeout=60_000)
        await page.wait_for_timeout(1200)
        await close_banners(page)
        await scroll_to_bottom(page)

        brands = await parse_brands(page)

        await context.close()
        await browser.close()
        return brands

# -------------------------
# ì—‘ì…€: ì›” ì‹œíŠ¸ ìë™ ìƒì„±/ê°±ì‹ 
# -------------------------
def month_sheet_name(dt: datetime) -> str:
    return f"{dt.strftime('%y')}ë…„ {dt.month}ì›”"  # ì˜ˆ: 25ë…„ 9ì›”

def ensure_month_sheet(wb, dt: datetime):
    name = month_sheet_name(dt)
    if name in wb.sheetnames:
        ws = wb[name]
    else:
        ws = wb.create_sheet(title=name)
        setup_layout(ws, dt)
    return ws

def setup_layout(ws, dt: datetime):
    last_day = monthrange(dt.year, dt.month)[1]

    # íƒ€ì´í‹€
    ws["A1"] = "ë¸Œëœë“œ ìˆœìœ„ (ì˜¬ë¦¬ë¸Œì˜ ì•± ê¸°ì¤€)"
    ws["A1"].font = Font(bold=True, size=12)
    ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=1 + last_day)

    # 2í–‰: 1~ë§ì¼
    ws["A2"] = "ì¼ì"
    for d in range(1, last_day + 1):
        ws.cell(row=2, column=1 + d).value = f"{d}ì¼"

    # 3í–‰: ìš”ì¼
    ws["A3"] = "ìš”ì¼"
    for d in range(1, last_day + 1):
        wd = date(dt.year, dt.month, d).weekday()
        ws.cell(row=3, column=1 + d).value = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][wd]

    # 4í–‰: ë¹„ê³ 
    ws["A4"] = "ë¹„ê³ "

    # 5í–‰~: ìˆœìœ„
    for r in range(1, 101):
        ws.cell(row=4 + r, column=1).value = r

    # ì •ë ¬/í­
    for r in range(1, 5 + 100):
        for c in range(1, 1 + last_day + 1):
            ws.cell(row=r, column=c).alignment = Alignment(vertical="center")
    ws.column_dimensions["A"].width = 8
    for d in range(1, last_day + 1):
        ws.column_dimensions[get_column_letter(1 + d)].width = 18

def write_today(ws, now: datetime, brands):
    col = 1 + now.day  # A=1
    for i in range(100):
        ws.cell(row=5 + i, column=col).value = brands[i] if i < len(brands) else None

def read_rank_map(ws, day: int):
    """í•´ë‹¹ ì‹œíŠ¸ì˜ day ì—´ì„ {ë¸Œëœë“œ: ìˆœìœ„}ë¡œ ë³€í™˜"""
    col = 1 + day
    ranks = {}
    for i in range(100):
        name = ws.cell(row=5 + i, column=col).value
        if name:
            ranks[str(name).strip()] = i + 1
    return ranks

def get_yesterday_rank_map(wb, now: datetime):
    # ê°™ì€ ë‹¬ì— ì „ì¼ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì›” ë§ˆì§€ë§‰ë‚ ì„ ì°¾ì•„ë´„
    y = now - timedelta(days=1)

    # ì „ì¼ì´ ê°™ì€ ë‹¬
    sheet_name_today = month_sheet_name(now)
    sheet_name_y = month_sheet_name(y)

    if sheet_name_y in wb.sheetnames:
        ws_y = wb[sheet_name_y]
        # ì „ì¼ ì—´ì´ ì¡´ì¬(í—¤ë” ì‘ì„±ë˜ì–´ìˆìŒ)í•˜ë©´ ì½ê¸°
        try:
            return read_rank_map(ws_y, y.day)
        except Exception:
            pass

    # ì—†ìœ¼ë©´ ë¹ˆ dict
    return {}

def save_excel_and_get_yesterday_map(brands):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    now = datetime.now(KST)

    if os.path.exists(OUTPUT_PATH):
        wb = load_workbook(OUTPUT_PATH)
    else:
        wb = Workbook()
        if "Sheet" in wb.sheetnames and len(wb.sheetnames) == 1:
            wb.remove(wb["Sheet"])

    ws = ensure_month_sheet(wb, now)

    # ë ˆì´ì•„ì›ƒ ë³´ì •(í˜¹ì‹œ ë¹„ì–´ìˆë‹¤ë©´)
    if ws.cell(row=2, column=2).value is None:
        setup_layout(ws, now)

    # ì „ì¼ ë­í¬ ë§µ ë¨¼ì € êµ¬í•¨
    ymap = get_yesterday_rank_map(wb, now)

    # ì˜¤ëŠ˜ ì“°ê¸°
    write_today(ws, now, brands)

    wb.save(OUTPUT_PATH)
    return ymap, now

# -------------------------
# Slack: Top10 ì•Œë¦¼ (ì „ì¼ ëŒ€ë¹„ ë“±ë½ í‘œê¸°)
# ê¸°ì¡´ ê·œì¹™: (â†‘n)/(â†“n)/(-)/(new)
# -------------------------
def build_delta(today_rank, yesterday_rank):
    if yesterday_rank is None:
        return "(new)"
    diff = yesterday_rank - today_rank
    if diff > 0:
        return f"(â†‘{diff})"
    elif diff < 0:
        return f"(â†“{abs(diff)})"
    else:
        return "(-)"

def post_slack_top10(brands, ymap, now):
    if not SLACK_WEBHOOK_URL:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URL ë¯¸ì„¤ì • â€” ìŠ¬ë™ ì „ì†¡ ìƒëµ")
        return

    top10 = brands[:10]
    lines = []
    for idx, name in enumerate(top10, start=1):
        y_rank = ymap.get(name)
        delta = build_delta(idx, y_rank)
        lines.append(f"{idx}. {delta} {name}")

    title = f"ğŸ“Š ì˜¬ë¦¬ë¸Œì˜ ë°ì¼ë¦¬ ë¸Œëœë“œ ë­í‚¹ Top10 â€” {now.strftime('%Y-%m-%d')} (KST)"
    body = "\n".join(lines)

    payload = {
        "blocks": [
            {"type": "header", "text": {"type": "plain_text", "text": title, "emoji": True}},
            {"type": "section", "text": {"type": "mrkdwn", "text": body}},
        ]
    }
    try:
        r = requests.post(SLACK_WEBHOOK_URL, data=json.dumps(payload), headers={"Content-Type":"application/json"}, timeout=10)
        r.raise_for_status()
        print("[ìŠ¬ë™] Top10 ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        print(f"[ìŠ¬ë™] ì „ì†¡ ì‹¤íŒ¨: {e}")

# -------------------------
# Google Drive ì—…ë¡œë“œ(ë™ì¼ íŒŒì¼ëª… ì¡´ì¬ ì‹œ ì—…ë°ì´íŠ¸)
# -------------------------
def build_drive_service():
    if not (GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET and GOOGLE_REFRESH_TOKEN and GDRIVE_FOLDER_ID):
        print("[ê²½ê³ ] êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì‹œí¬ë¦¿ì´ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    creds = Credentials(
        token=None,
        refresh_token=GOOGLE_REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=GOOGLE_CLIENT_ID,
        client_secret=GOOGLE_CLIENT_SECRET,
        scopes=["https://www.googleapis.com/auth/drive"]
    )
    return build("drive", "v3", credentials=creds)

def find_file_in_folder(service, folder_id, name):
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id, name)", pageSize=1).execute()
    files = res.get("files", [])
    return files[0]["id"] if files else None

def upload_or_update_to_drive(filepath, folder_id):
    service = build_drive_service()
    if not service:
        return

    file_id = find_file_in_folder(service, folder_id, os.path.basename(filepath))
    media = MediaFileUpload(filepath, mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", resumable=True)

    if file_id:
        # ì—…ë°ì´íŠ¸
        service.files().update(fileId=file_id, media_body=media).execute()
        print(f"[ë“œë¼ì´ë¸Œ] ê¸°ì¡´ íŒŒì¼ ê°±ì‹  ì™„ë£Œ: {filepath}")
    else:
        file_metadata = {"name": os.path.basename(filepath), "parents": [folder_id]}
        service.files().create(body=file_metadata, media_body=media, fields="id").execute()
        print(f"[ë“œë¼ì´ë¸Œ] ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {filepath}")

# -------------------------
# main
# -------------------------
async def main():
    brands = await scrape_top100()
    if not brands:
        raise RuntimeError("ë¸Œëœë“œëª…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„° í™•ì¸ í•„ìš”")

    ymap, now = save_excel_and_get_yesterday_map(brands)
    post_slack_top10(brands, ymap, now)
    upload_or_update_to_drive(OUTPUT_PATH, GDRIVE_FOLDER_ID)

if __name__ == "__main__":
    asyncio.run(main())
