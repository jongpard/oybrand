# scripts/weekly_report_plus.py
# -*- coding: utf-8 -*-
import os, re, glob, json, math, argparse
import pandas as pd
from collections import Counter
from datetime import timedelta, timezone

KST = timezone(timedelta(hours=9))

SRC_INFO = {
    'oy_kor':    {'topn':100, 'hints':['ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹','oliveyoung_kor','oy_kor'], 'currency':'KRW'},
    'oy_global': {'topn':100, 'hints':['ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ','oliveyoung_global','oy_global'], 'currency':'KRW'},
    'amazon_us': {'topn':100, 'hints':['ì•„ë§ˆì¡´US','amazon_us','amazonUS'],       'currency':'USD'},
    'qoo10_jp':  {'topn':200, 'hints':['íí…ì¬íŒ¬','qoo10','Qoo10'],               'currency':'JPY'},
    'daiso_kr':  {'topn':200, 'hints':['ë‹¤ì´ì†Œëª°','daiso'],                       'currency':'KRW'},
}
ALL_SRCS = list(SRC_INFO.keys())

# === ì»¬ëŸ¼ ë™ì˜ì–´(ì œí’ˆëª…ì€ raw_name ìµœìš°ì„ , ê·¸ëŒ€ë¡œ í‘œê¸°) ===
COLS = {
    'rank':        ['rank','ìˆœìœ„','ranking','ë­í‚¹'],
    'raw_name':    ['raw_name','raw','rawProduct','rawTitle'],
    'product':     ['product','ì œí’ˆëª…','ìƒí’ˆëª…','name','title','goods_name','goodsNm','prdNm','prdtName','displayName','itemNm','ìƒí’ˆ','item_name','item'],
    'brand':       ['brand','ë¸Œëœë“œ','brand_name','brandNm','ë¸Œëœë“œëª…'],
    'url':         ['url','ë§í¬','product_url','link','detail_url'],
    'date':        ['date','ë‚ ì§œ','ìˆ˜ì§‘ì¼','crawl_date','created_at'],
    'price':       ['price','ê°€ê²©','sale_price','selling_price'],
    'orig_price':  ['orig_price','ì •ê°€','original_price','ì†Œë¹„ìê°€','list_price'],
    'discount':    ['discount_rate','í• ì¸ìœ¨','discount','discountPercent'],
    'goodsNo':     ['goodsNo','goods_no','goodsno','ìƒí’ˆë²ˆí˜¸','ìƒí’ˆì½”ë“œ'],
    'productId':   ['productId','product_id','prdtNo','ìƒí’ˆID','ìƒí’ˆì•„ì´ë””','ìƒí’ˆì½”ë“œ'],
    'asin':        ['asin','ASIN'],
    'product_code':['product_code','productCode','ìƒí’ˆì½”ë“œ','item_code'],
    'pdNo':        ['pdNo','pdno','ìƒí’ˆë²ˆí˜¸','ìƒí’ˆì½”ë“œ'],
}

# === í‚¤ì›Œë“œ ì‚¬ì „(ì œí’ˆí˜•íƒœ/íš¨ëŠ¥/ë§ˆì¼€íŒ…) â€” % ì ìœ  ê³„ì‚°ìš© ===
KW_PRODUCT = {
    'íŒ¨ë“œ': r'(íŒ¨ë“œ|pad)',
    'ë§ˆìŠ¤í¬íŒ©': r'(ë§ˆìŠ¤í¬íŒ©|ë§ˆìŠ¤í¬|sheet\s*mask|mask\s*pack)',
    'ì•°í”Œ/ì„¸ëŸ¼': r'(ì•°í”Œ|ì„¸ëŸ¼|ampoule|serum)',
    'í† ë„ˆ/ìŠ¤í‚¨': r'(í† ë„ˆ|ìŠ¤í‚¨|toner)',
    'í¬ë¦¼': r'(í¬ë¦¼|cream|moisturizer)',
    'í´ë Œì €': r'(í´ë Œì§•|í´ë Œì €|cleanser|ì›Œì‹œ|wash)',
    'ì„ ì¼€ì–´': r'(ì„ í¬ë¦¼|sunscreen|sun\s*cream|uv)',
    'ë¦½': r'(ë¦½|í‹´íŠ¸|lip|tint|balm)',
}
KW_EFFICACY = {
    'ì§„ì •': r'(ì§„ì •|soothing|calming)',
    'ë³´ìŠµ': r'(ë³´ìŠµ|ìˆ˜ë¶„|hydration|moistur)',
    'ë¯¸ë°±/í†¤ì—…': r'(ë¯¸ë°±|í†¤ì—…|whiten|brighten)',
    'íƒ„ë ¥/ë¦¬í”„íŒ…': r'(íƒ„ë ¥|ë¦¬í”„íŒ…|firming|lifting|elastic)',
    'íŠ¸ëŸ¬ë¸”/ì—¬ë“œë¦„': r'(íŠ¸ëŸ¬ë¸”|ì—¬ë“œë¦„|acne|blemish)',
    'ëª¨ê³µ': r'(ëª¨ê³µ|pore)',
    'ê°ì§ˆ/í•„ë§': r'(ê°ì§ˆ|í•„ë§|peel|AHA|BHA|PHA)',
    'ì£¼ë¦„': r'(ì£¼ë¦„|wrinkle|anti[-\s]?aging)',
}
KW_MARKETING = {
    'ê¸°íš/ì„¸íŠ¸': r'(ê¸°íš|ì„¸íŠ¸|set|kit|bundle)',
    '1+1/ì¦ì •': r'(1\+1|2\+1|ì¦ì •|ì¦ëŸ‰|ë¤)',
    'í•œì •/NEW': r'(í•œì •|ë¦¬ë¯¸í‹°ë“œ|limited|NEW|new\b|ì‹ ìƒ)',
    'ì¿ í°/ë”œ': r'(ì¿ í°|coupon|ë”œ|deal|íŠ¹ê°€|sale|ì„¸ì¼|event|í”„ë¡œëª¨ì…˜|promotion)',
    'PICK/ì½œë¼ë³´': r'(ì˜¬ì˜í”½|PICK|pick|ì½œë¼ë³´|collab)',
}

STOPWORDS = set("""
ì˜ ê°€ ì´ ì€ ëŠ” ì„ ë¥¼ ì— ì—ì„œ ìœ¼ë¡œ ë„ ê³¼ ì™€ ë° ( ) , . : Â· - & x X + the and or for of with
ì„¸íŠ¸ 1+1 2+1 10ê°œì… 20ë§¤ 30g 50ml 100ml 200ml pack set
""".split())

CATEGORY_RULES = [
    ("ë§ˆìŠ¤í¬íŒ©", r"(ë§ˆìŠ¤í¬íŒ©|íŒ©|sheet\s*mask|mask\s*pack)"),
    ("ì„ ì¼€ì–´", r"(ì„ í¬ë¦¼|ìì™¸ì„ |sun\s*cream|sunscreen|uv)"),
    ("í´ë Œì €", r"(í´ë Œì§•|í´ë Œì €|foam|cleanser|wash)"),
    ("í† ë„ˆ/ìŠ¤í‚¨", r"(í† ë„ˆ|ìŠ¤í‚¨|toner)"),
    ("ì—ì„¼ìŠ¤/ì„¸ëŸ¼", r"(ì—ì„¼ìŠ¤|ì„¸ëŸ¼|ì•°í”Œ|serum|essence|ampoule)"),
    ("ë¡œì…˜/ì—ë©€ì „", r"(ë¡œì…˜|ì—ë©€ì „|lotion|emulsion)"),
    ("í¬ë¦¼", r"(í¬ë¦¼|cream|moisturizer)"),
    ("ë¦½", r"(ë¦½|í‹´íŠ¸|ë¦½ë°¤|lip|tint|balm)"),
    ("ì•„ì´", r"(ì•„ì´í¬ë¦¼|eye\s*cream|ì•„ì´|ì•„ì´íŒ¨ì¹˜)"),
    ("í—¤ì–´", r"(ìƒ´í‘¸|íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸|í—¤ì–´|hair)"),
    ("ë°”ë””", r"(ë°”ë””|body|ë°”ìŠ¤)"),
    ("í–¥ìˆ˜", r"(í–¥ìˆ˜|í¼í“¸|eau|parfum|perfume)"),
    ("ë„êµ¬/ê¸°ê¸°", r"(ê¸°ê¸°|ë””ë°”ì´ìŠ¤|ë¡¤ëŸ¬|device|tool)"),
]

# ---------- ìœ í‹¸ ----------
def pick(df, names):
    for c in names:
        if c in df.columns: return c
    return None

def pick_loose_product(df):
    rn = pick(df, COLS['raw_name'])
    if rn: return rn
    pn = pick(df, COLS['product'])
    if pn: return pn
    patt = re.compile(r"(product|name|title|ìƒí’ˆ|ì œí’ˆ|í’ˆëª…|ì•„ì´í…œ)", re.IGNORECASE)
    for c in df.columns:
        if patt.search(str(c)): return c
    return None

def infer_source(path:str):
    base = os.path.basename(path).lower()
    parent = os.path.basename(os.path.dirname(path)).lower()
    for src, info in SRC_INFO.items():
        for h in info['hints']:
            h = h.lower()
            if h in base or h in parent: return src
    return None

def infer_date_from_filename(fn:str):
    m = re.search(r'(\d{4}-\d{2}-\d{2})', os.path.basename(fn))
    return pd.to_datetime(m.group(1)) if m else pd.NaT

def read_csv_any(path:str)->pd.DataFrame:
    for enc in ('utf-8-sig','cp949','utf-8','euc-kr'):
        try: return pd.read_csv(path, encoding=enc)
        except Exception: pass
    return pd.read_csv(path)

def extract_key(src:str, row, url:str|None):
    u = str(url or "")
    if src == 'oy_kor':
        for c in COLS['goodsNo']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'goodsNo=([0-9A-Za-z\-]+)', u);  return m.group(1) if m else None
    if src == 'oy_global':
        for c in COLS['productId']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'(?:productId|prdtNo)=([0-9A-Za-z\-]+)', u); return m.group(1) if m else None
    if src == 'amazon_us':
        for c in COLS['asin']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip().upper()
        m = re.search(r'/([A-Z0-9]{10})(?:[/?#]|$)', u.upper()); return m.group(1) if m else None
    if src == 'qoo10_jp':
        for c in COLS['product_code']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'product_code=([0-9A-Za-z\-]+)', u)
        if m: return m.group(1)
        m2 = re.search(r'/(\d{6,})', u)
        return m2.group(1) if m2 else None
    if src == 'daiso_kr':
        for c in COLS['pdNo']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'pdNo=([0-9A-Za-z\-]+)', u); return m.group(1) if m else None
    return None

def fmt_money(v, src):
    if v is None or (isinstance(v,float) and (pd.isna(v) or math.isnan(v))): return None
    cur = SRC_INFO[src]['currency']
    if cur == 'USD':  return f"${v:,.0f}"
    if cur == 'JPY':  return f"Â¥{v:,.0f}"
    return f"â‚©{v:,.0f}"  # default KRW

# ---------- í”„ë¡œëª¨ì…˜ í”Œë˜ê·¸ ----------
PROMO_RE = re.compile(
    r"(ì˜¬ì˜í”½|PICK|pick|íŠ¹ê°€|ê¸°íš|ì¦ì •|ì„¸ì¼|sale|event|í–‰ì‚¬|í•œì •|ë¦¬ë¯¸í‹°ë“œ|1\+1|2\+1|ë”ë¸”\s*ê¸°íš|ì¦ëŸ‰|ì¿ í°|coupon|deal|ë”œ|gift|bundle|promotion)",
    re.IGNORECASE
)
def is_promo(name:str)->bool:
    n = (name or "")
    return bool(PROMO_RE.search(n))

# ---------- ì¸í”Œë£¨ì–¸ì„œ ë™ì  ì¶”ì¶œ ----------
INF_BLACKLIST = {'ì˜¬ì˜','ì˜¬ë¦¬ë¸Œì˜','ì˜¬ì˜í”½','ì›ë”','MD','ì—ë””í„°','ë¸Œëœë“œ','editor','brand','oliveyoung','ìŠ¤í† ì–´','ê³µì‹','í”½'}
def extract_influencers_dynamic(name:str):
    """raw ì œí’ˆëª…ì—ì„œ ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„ í›„ë³´ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œ"""
    if not name: return set()
    text = str(name)
    found = set()

    # 1) "OO í”½ / OO PICK / OO Pick"
    for m in re.finditer(r'([ê°€-í£A-Za-z]{2,20})\s*(?:ì˜\s*)?(?:í”½|PICK|Pick)\b', text):
        nm = m.group(1).strip()
        if nm not in INF_BLACKLIST and nm.lower() not in {x.lower() for x in INF_BLACKLIST}:
            found.add(nm)

    # 2) ì½œë¼ë³´/with/Ã—/X
    for m in re.finditer(r'([ê°€-í£A-Za-z]{2,20})\s*(?:Ã—|x|X|with|ì½œë¼ë³´|collab(?:oration)?)\s*([ê°€-í£A-Za-z]{2,20})', text, re.IGNORECASE):
        for nm in (m.group(1), m.group(2)):
            nm = nm.strip()
            if nm not in INF_BLACKLIST and nm.lower() not in {x.lower() for x in INF_BLACKLIST}:
                # ìˆ«ì/ë‹¨ìœ„/ì¼ë°˜ í‚¤ì›Œë“œ ì œê±°
                if not re.search(r'\d|ml|g|pack|set|ê¸°íš|íŠ¹ê°€|ì„¸íŠ¸', nm, re.IGNORECASE):
                    found.add(nm)
    return found

# ---------- ë² ì´ìŠ¤ í…Œì´ë¸” ----------
def daily_topn_base(df, topn):
    # ë‚ ì§œë³„ ì •ë ¬â†’key ì¤‘ë³µ ì œê±°â†’TopN ìŠ¬ë¼ì´ìŠ¤
    d = (df.sort_values(['date','rank'])
            .drop_duplicates(['date','key'])
            .groupby(df['date'].dt.date, group_keys=False)
            .apply(lambda x: x.nsmallest(topn, 'rank')))
    d['day'] = d['date'].dt.date
    return d

def week_range_for_source(ud:pd.DataFrame, src:str):
    dts = ud.loc[ud['source'].eq(src), 'date']
    if dts.empty: return None
    last = pd.to_datetime(dts.max())
    end = last + pd.Timedelta(days=(6 - last.weekday()))  # ê·¸ ì£¼ ì¼ìš”ì¼
    start = end - pd.Timedelta(days=6)                    # ê·¸ ì£¼ ì›”ìš”ì¼
    return start.normalize(), end.normalize()

def _arrow_rank(diff_rank: float) -> str:
    # diff_rank = prev_mean - cur_mean (ê°œì„ ì´ë©´ ì–‘ìˆ˜)
    if diff_rank is None or (isinstance(diff_rank,float) and math.isnan(diff_rank)): return "â€”"
    d = int(round(diff_rank))
    if d > 0:  return f"â†‘{d}"
    if d < 0:  return f"â†“{abs(d)}"
    return "â€”"

# ---------- ë¡œë”© ----------
def load_unified(data_dir:str)->pd.DataFrame:
    paths = glob.glob(os.path.join(data_dir,'**','*.csv'), recursive=True)
    rows = []
    for p in paths:
        src = infer_source(p)
        if not src: continue
        try:
            df = read_csv_any(p)
        except Exception:
            continue

        date_col = pick(df, COLS['date'])
        dates = pd.to_datetime(df[date_col], errors='coerce') if date_col else pd.Series([infer_date_from_filename(p)]*len(df))
        rank_col = pick(df, COLS['rank'])
        if not rank_col: continue

        prod_col = pick_loose_product(df)     # raw_name ìš°ì„ , ê·¸ëŒ€ë¡œ ì‚¬ìš©
        brand_col = pick(df, COLS['brand'])
        url_col   = pick(df, COLS['url'])
        price_col = pick(df, COLS['price'])
        orig_col  = pick(df, COLS['orig_price'])
        disc_col  = pick(df, COLS['discount'])

        for i, r in df.iterrows():
            nm = str(r.get(prod_col)) if prod_col else None
            br = str(r.get(brand_col)) if brand_col else None
            # ì•„ë§ˆì¡´ ë¸Œëœë“œ ì •ê·œí™”: 'Amazon' â†’ 'Amazon Basics'
            if src == 'amazon_us' and br:
                if br.strip().lower() == 'amazon':
                    br = 'Amazon Basics'

            rows.append({
                'source': src,
                'date': dates.iloc[i],
                'rank': pd.to_numeric(r.get(rank_col), errors='coerce'),
                'product': nm,
                'brand': br,
                'url': r.get(url_col) if url_col else None,
                'price': pd.to_numeric(r.get(price_col), errors='coerce') if price_col else None,
                'orig_price': pd.to_numeric(r.get(orig_col), errors='coerce') if orig_col else None,
                'discount_rate': pd.to_numeric(r.get(disc_col), errors='coerce') if disc_col else None,
                'key': extract_key(src, r, r.get(url_col) if url_col else None),
                'promo': is_promo(nm),
                'infl': list(extract_influencers_dynamic(nm)),
            })

    ud = pd.DataFrame(rows, columns=[
        'source','date','rank','product','brand','url',
        'price','orig_price','discount_rate','key','promo','infl'
    ])
    ud = ud.dropna(subset=['source','date','rank','key'])
    return ud

# ---------- ì§‘ê³„ ----------
def summarize_week(ud:pd.DataFrame, src:str, min_days:int=3):
    res = {
        'range': 'ë°ì´í„° ì—†ìŒ',
        'top10_lines': ['ë°ì´í„° ì—†ìŒ'],
        'brand_lines': ['ë°ì´í„° ì—†ìŒ'],
        'inout': '',
        'heroes': [],
        'flash': [],
        'discount_all': None,
        'discount_promo': None,
        'discount_nonpromo': None,
        'discount_delta_same': None,
        'median_price': None,
        'cat_top5': [],
        'kw_lines': [],     # í‚¤ì›Œë“œ(ì œí’ˆí˜•íƒœ/íš¨ëŠ¥/ë§ˆì¼€íŒ…%) + ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„
        'insights': [],
        'stats': {},
    }
    rng = week_range_for_source(ud, src)
    if not rng: return res
    start, end = rng
    topn = SRC_INFO[src]['topn']

    cur = ud[(ud['source'].eq(src)) & (ud['date']>=start) & (ud['date']<=end) & (ud['rank']<=topn)].copy()
    if cur.empty:
        res['range'] = f"{start.date()}~{end.date()}"
        return res

    prev = ud[(ud['source'].eq(src)) &
              (ud['date']>=start - pd.Timedelta(days=7)) &
              (ud['date']<=end - pd.Timedelta(days=7)) &
              (ud['rank']<=topn)].copy()

    hist = ud[(ud['source'].eq(src)) &
              (ud['date']>=start - pd.Timedelta(days=28)) &
              (ud['date']<=start - pd.Timedelta(days=1)) &
              (ud['rank']<=topn)].copy()

    # í•˜ë£¨ ê¸°ì¤€ TopN ë² ì´ìŠ¤
    cur_base  = daily_topn_base(cur, topn)
    prev_base = daily_topn_base(prev, topn)

    # --- ì£¼ê°„ í…Œì´ë¸”(ì ìˆ˜/í‰ê· ìˆœìœ„/ìœ ì§€ì¼ìˆ˜) ---
    tmp = cur_base.copy()
    tmp['__pts'] = topn + 1 - tmp['rank']
    pts = (tmp.groupby('key', as_index=False)
              .agg(points=('__pts','sum'),
                   days=('rank','count'),
                   best=('rank','min'),
                   mean_rank=('rank','mean')))
    pts = pts[pts['days'] >= min_days]

    latest = (cur_base.sort_values('date')
                .groupby('key', as_index=False)
                .agg(product=('product','last'),
                     brand=('brand','last'),
                     url=('url','last')))

    prev_tbl = None; prev_mean_map = {}; prev_days_map = {}
    if not prev_base.empty:
        ptmp = prev_base.copy(); ptmp['__pts'] = topn + 1 - ptmp['rank']
        prev_tbl = (ptmp.groupby('key', as_index=False)
                        .agg(points=('__pts','sum'),
                             days=('rank','count'),
                             best=('rank','min'),
                             mean_rank=('rank','mean')))
        prev_mean_map = dict(zip(prev_tbl['key'], prev_tbl['mean_rank']))
        prev_days_map = dict(zip(prev_tbl['key'], prev_tbl['days']))

    # ì •ë ¬: ì ìˆ˜â†“ â†’ ìœ ì§€ì¼ìˆ˜â†“ â†’ ìµœê³ ìˆœìœ„â†‘
    top = (pts.merge(latest, on='key', how='left')
             .sort_values(['points','days','best'], ascending=[False, False, True])
             .head(10))

    # Top10: (ìœ ì§€ nì¼ Â· í‰ê·  xx.xìœ„) (NEW/â†‘/â†“/â€”)
    top_lines = []
    for i, r in enumerate(top.itertuples(), 1):
        key = getattr(r,'key')
        nm = getattr(r,'product') or getattr(r,'brand') or key
        u  = getattr(r,'url') or ''
        label = f"<{u}|{nm}>" if u else nm

        cur_mean  = getattr(r,'mean_rank')
        prev_mean = prev_mean_map.get(key)
        prev_days = prev_days_map.get(key, 0)
        delta_txt = "NEW" if (prev_mean is None or prev_days < min_days) else _arrow_rank(prev_mean - cur_mean)
        mean_txt = f"{round(float(cur_mean),1)}ìœ„" if cur_mean is not None else "-"

        top_lines.append(f"{i}. {label} (ìœ ì§€ {int(getattr(r,'days'))}ì¼ Â· í‰ê·  {mean_txt}) ({delta_txt})")

    # ë¸Œëœë“œ "ê°œìˆ˜/ì¼" ë¹„êµ
    def brand_daily_avg(base):
        return (base.groupby(['day','brand']).size()
                     .groupby('brand').mean().reset_index(name='per_day'))
    b_now = brand_daily_avg(cur_base).rename(columns={'per_day':'now'})
    b_prev = brand_daily_avg(prev_base).rename(columns={'per_day':'prev'}) if not prev_base.empty else pd.DataFrame(columns=['brand','prev'])
    b = (b_now.merge(b_prev, on='brand', how='left').fillna(0.0)
              .assign(delta=lambda x: x['now'] - x['prev'])
              .sort_values(['now','delta'], ascending=[False, False]).head(12))
    brand_lines = []
    for r in b.itertuples():
        if r.delta > 0:  sign = f"â†‘{round(r.delta,1)}"
        elif r.delta < 0: sign = f"â†“{abs(round(r.delta,1))}"
        else:            sign = "â€”"
        brand_lines.append(f"{r.brand} {round(r.now,1)}ê°œ/ì¼ ({sign})")

    # IN/OUT: ë¹„êµê°€ëŠ¥í•œ ë‚ ë§Œ ê³„ì‚° â†’ ë‹¨ì¼ ê°’
    days = sorted(cur_base['day'].unique())
    prev_days_set = set(prev_base['day'].unique())
    total_in = total_out = 0; valid = 0
    for d in days:
        pd_ = pd.to_datetime(d) - pd.Timedelta(days=1)
        if pd_.date() not in prev_days_set: continue
        cur_set  = set(cur_base.loc[cur_base['day'].eq(d), 'key'])
        prev_set = set(prev_base.loc[prev_base['day'].eq(pd_.date()), 'key'])
        total_in  += len(cur_set - prev_set)
        total_out += len(prev_set - cur_set)
        valid += 1
    swaps = max(total_in, total_out)
    inout_text = "ë¹„êµ ê¸°ì¤€ ì—†ìŒ" if valid == 0 else f"{swaps} (ì¼í‰ê·  {round(swaps/valid,2)} Â· {valid}/{len(days)}ì¼ ë¹„êµ)"

    # ì‹ ê·œ íˆì–´ë¡œ / ë°˜ì§
    hist_keys = set(hist['key'].unique()) if not hist.empty else set()
    heroes = (pts[~pts['key'].isin(hist_keys)]
                .merge(latest, on='key', how='left')
                .sort_values(['points','days','best'], ascending=[False, False, True])
                .head(5))
    flash = (pts[(pts['days']<=2)]
                .merge(latest, on='key', how='left')
                .sort_values(['points','days','best'], ascending=[False, False, True])
                .head(5))

    def to_links(df):
        out = []
        for r in df.itertuples():
            nm = getattr(r,'product') or getattr(r,'brand') or getattr(r,'key')
            u  = getattr(r,'url') or ''
            out.append(f"<{u}|{nm}>" if u else nm)
        return out

    # ê°€ê²©/í• ì¸: ìƒí’ˆë³„ ì£¼ê°„ í†µê³„ â†’ ì¤‘ì•™/í‰ê· 
    wk = (cur_base.groupby('key')
            .agg(price_med=('price','median'),
                 disc_avg=('discount_rate','mean'))
            .reset_index())
    med_price = int(wk['price_med'].dropna().median()) if wk['price_med'].notna().any() else None

    # í”„ë¡œëª¨ì…˜ vs ì¼ë°˜
    promo_base = cur_base[cur_base['promo']==True]
    non_base   = cur_base[cur_base['promo']!=True]
    def _mean_disc(df):
        return round(float(df['discount_rate'].dropna().mean()),2) if df['discount_rate'].notna().any() else None
    disc_all   = _mean_disc(cur_base)
    disc_promo = _mean_disc(promo_base)
    disc_non   = _mean_disc(non_base)

    # ë™ì¼ ìƒí’ˆì˜ í”„ë¡œëª¨ì…˜ æœ‰/ç„¡ ì°¨ì´
    both = (cur_base.groupby(['key','promo'])['discount_rate']
                   .mean().reset_index().pivot(index='key', columns='promo', values='discount_rate').dropna())
    disc_delta_same = None
    if not both.empty:
        both['diff'] = both.get(True, pd.Series()) - both.get(False, pd.Series())
        if both['diff'].notna().any():
            disc_delta_same = round(float(both['diff'].mean()),2)

    # ì¹´í…Œê³ ë¦¬ ìƒìœ„
    def map_cat(name:str)->str:
        nm = (name or "").lower()
        for cat, pat in CATEGORY_RULES:
            if re.search(pat, nm, re.IGNORECASE): return cat
        return "ê¸°íƒ€"
    cats = cur_base.copy()
    cats['__cat'] = cats['product'].map(map_cat)
    cat_top5 = cats.groupby('__cat').size().sort_values(ascending=False).head(5)
    cat_pairs = [f"{c} {int(n)}ê°œ" for c,n in cat_top5.items()]

    # í‚¤ì›Œë“œ(ì œí’ˆí˜•íƒœ/íš¨ëŠ¥/ë§ˆì¼€íŒ… %) + ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    def bucket_share(base, rules):
        cnt = Counter(); total_hits = 0
        for nm in base['product'].dropna().astype(str):
            for label, pat in rules.items():
                if re.search(pat, nm, re.IGNORECASE):
                    cnt[label] += 1; total_hits += 1
        if total_hits == 0: return []
        return [f"{k} {round(v*100/total_hits,1)}%" for k,v in cnt.most_common(5)]
    # ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„ ìˆ˜ì§‘
    infl_cnt = Counter()
    for names in cur_base['infl'].dropna():
        for n in names:
            infl_cnt[n] += 1
    infl_names = [n for n,_ in infl_cnt.most_common(8)]

    kw_lines = []
    p_items = bucket_share(cur_base, KW_PRODUCT)
    e_items = bucket_share(cur_base, KW_EFFICACY)
    m_items = bucket_share(cur_base, KW_MARKETING)
    if p_items: kw_lines.append("â€¢ ì œí’ˆí˜•íƒœ: " + ", ".join(p_items))
    if e_items: kw_lines.append("â€¢ íš¨ëŠ¥: " + ", ".join(e_items))
    if m_items: kw_lines.append("â€¢ ë§ˆì¼€íŒ…: " + ", ".join(m_items))
    if infl_names: kw_lines.append("â€¢ ì¸í”Œë£¨ì–¸ì„œ: " + ", ".join(infl_names))

    # ê¸°ë³¸ í†µê³„Â·ì¸ì‚¬ì´íŠ¸
    uniq_cnt = cur_base['key'].nunique()
    keep_med = int(pts['days'].median()) if not pts.empty else 0

    # ì œí’ˆí˜•íƒœ ìƒìœ„ 2ê°œ ì§‘ì¤‘ë„
    def share_two_top(base, rules):
        cnt = Counter()
        for nm in base['product'].dropna().astype(str):
            for label, pat in rules.items():
                if re.search(pat, nm, re.IGNORECASE): cnt[label]+=1
        total = sum(cnt.values())
        if total==0: return None, None, 0.0
        top2 = cnt.most_common(2)
        share = round((top2[0][1] + (top2[1][1] if len(top2)>1 else 0))*100/total, 1)
        labels = [x[0] for x in top2]
        return labels, total, share
    top2_labels, _, top2_share = share_two_top(cur_base, KW_PRODUCT)

    g_up = b.sort_values('delta', ascending=False).head(1)
    g_dn = b.sort_values('delta', ascending=True).head(1)
    up_txt = f"{g_up.iloc[0]['brand']}(+{round(g_up.iloc[0]['delta'],1)}/ì¼)" if not g_up.empty and g_up.iloc[0]['delta']>0 else None
    dn_txt = f"{g_dn.iloc[0]['brand']}(-{abs(round(g_dn.iloc[0]['delta'],1))}/ì¼)" if not g_dn.empty and g_dn.iloc[0]['delta']<0 else None

    price_bucket = None
    if med_price is not None:
        v = med_price
        if v < 10000: price_bucket = "1ë§Œì› ë¯¸ë§Œ"
        elif v < 20000: price_bucket = "1ë§ŒëŒ€"
        elif v < 30000: price_bucket = "2ë§ŒëŒ€"
        elif v < 40000: price_bucket = "3ë§ŒëŒ€"
        else: price_bucket = "4ë§ŒëŒ€+"

    promo_effect = None
    if (disc_promo is not None) and (disc_non is not None):
        diff = round(disc_promo - disc_non, 2)
        if diff >= 2.0:
            promo_effect = f"í”„ë¡œëª¨ì…˜ í‰ê·  í• ì¸ìœ¨ì´ ì¼ë°˜ ëŒ€ë¹„ +{diff}%p ë†’ìŒ"

    insights = []
    insights.append(f"7ì¼ê°„ Top{topn} ìœ ë‹ˆí¬ ì œí’ˆ ìˆ˜ {uniq_cnt}ê°œ Â· ìœ ì§€ì¼ìˆ˜ ì¤‘ì•™ê°’ {keep_med}ì¼")
    if top2_labels: insights.append(f"ì œí’ˆí˜•íƒœëŠ” {', '.join(top2_labels)} ì¤‘ì‹¬(ìƒìœ„2 í•© {top2_share}%)")
    if up_txt or dn_txt:
        bits = []
        if up_txt: bits.append("ìƒìŠ¹ " + up_txt)
        if dn_txt: bits.append("í•˜ë½ " + dn_txt)
        insights.append(", ".join(bits))
    if price_bucket: insights.append(f"ì£¼ìš” ê°€ê²©ëŒ€ {price_bucket}")
    if promo_effect: insights.append(promo_effect)

    res.update({
        'range': f"{start.date()}~{end.date()}",
        'top10_lines': top_lines,
        'brand_lines': brand_lines,
        'inout': inout_text,
        'heroes': to_links(heroes),
        'flash': to_links(flash),
        'discount_all': disc_all,
        'discount_promo': disc_promo,
        'discount_nonpromo': disc_non,
        'discount_delta_same': disc_delta_same,
        'median_price': med_price,
        'cat_top5': cat_pairs,
        'kw_lines': kw_lines,
        'insights': insights,
        'stats': {'unique_items': uniq_cnt, 'keep_days_median': keep_med, 'topn': topn}
    })
    return res

# ---------- ìŠ¬ë™ í¬ë§· ----------
def format_slack_block(src:str, s:dict)->str:
    title_map = {
        'oy_kor':   "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",
        'oy_global':"ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",
        'amazon_us':"ì•„ë§ˆì¡´ US Top100",
        'qoo10_jp': "íí… ì¬íŒ¬ ë·°í‹° Top200",
        'daiso_kr': "ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200",
    }
    lines = []
    lines.append(f"ğŸ“Š ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title_map.get(src, src)} ({s['range']})")
    lines.append("ğŸ† Top10")
    lines.extend(s['top10_lines'] or ["ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append("ğŸ ë¸Œëœë“œ ê°œìˆ˜(ì¼í‰ê· )")
    lines.extend(s['brand_lines'] or ["ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append(f"ğŸ” ì¸ì•¤ì•„ì›ƒ(êµì²´): {s['inout']}")
    if s['heroes']: lines.append("ğŸ†• ì‹ ê·œ íˆì–´ë¡œ: " + ", ".join(s['heroes']))
    if s['flash']:  lines.append("âœ¨ ë°˜ì§ ì•„ì´í…œ: " + ", ".join(s['flash']))
    if s['cat_top5']: lines.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ ìƒìœ„: " + " Â· ".join(s['cat_top5']))
    if s['kw_lines']:
        lines.append("ğŸ” ì£¼ê°„ í‚¤ì›Œë“œ ë¶„ì„")
        lines.extend(s['kw_lines'])
    # ê°€ê²©/í• ì¸
    tail = []
    if s.get('median_price') is not None: tail.append("ì¤‘ìœ„ê°€ê²© " + (fmt_money(s['median_price'], src) or ""))
    disc_bits = []
    if s.get('discount_all') is not None:    disc_bits.append(f"ì „ì²´ {s['discount_all']:.2f}%")
    if s.get('discount_promo') is not None:  disc_bits.append(f"í”„ë¡œëª¨ì…˜ {s['discount_promo']:.2f}%")
    if s.get('discount_nonpromo') is not None: disc_bits.append(f"ì¼ë°˜ {s['discount_nonpromo']:.2f}%")
    if s.get('discount_delta_same') is not None: disc_bits.append(f"(ë™ì¼ìƒí’ˆ ì°¨ì´ +{s['discount_delta_same']:.2f}%p)")
    if disc_bits: tail.append("í‰ê·  í• ì¸ìœ¨ " + " Â· ".join(disc_bits))
    if tail: lines.append("ğŸ’µ " + " / ".join(tail))
    # ì¸ì‚¬ì´íŠ¸
    if s.get('insights'):
        lines.append("")
        lines.append("ğŸ§  ìµœì¢… ì¸ì‚¬ì´íŠ¸")
        for ln in s['insights']:
            lines.append(f"- {ln}")
    return "\n".join(lines)

# ---------- ì—”íŠ¸ë¦¬(ê°œë³„ ì „ì†¡ ê¸°ë³¸) ----------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--src", nargs="+", choices=ALL_SRCS + ["all"],
                        default=os.getenv("ONLY_SRC","all").split(","))
    parser.add_argument("--split", action="store_true", default=True,
                        help="ì†ŒìŠ¤ë³„ ê°œë³„ ìš”ì•½/ìŠ¬ë™ íŒŒì¼ ìƒì„±(ê¸°ë³¸ ON)")
    parser.add_argument("--data-dir", default=os.getenv("DATA_DIR","./data/daily"))
    parser.add_argument("--min-days", type=int, default=int(os.getenv("MIN_DAYS","3")))
    args = parser.parse_args()

    targets = []
    for s in (args.src if isinstance(args.src, list) else [args.src]):
        targets.extend(ALL_SRCS if s == "all" else [s])
    targets = [t for t in targets if t in ALL_SRCS]

    ud = load_unified(args.data_dir)
    combined = {}
    combined_txt = []

    for src in targets:
        s = summarize_week(ud, src, min_days=args.min_days)
        combined[src] = s
        text = format_slack_block(src, s)
        # ê°œë³„ íŒŒì¼ ì €ì¥
        with open(f"weekly_summary_{src}.json","w",encoding="utf-8") as f:
            json.dump(s, f, ensure_ascii=False, indent=2)
        with open(f"slack_{src}.txt","w",encoding="utf-8") as f:
            f.write(text)
        if not args.split: combined_txt.append(text)

    # íŒŒì´í”„ë¼ì¸ìš© ì „ì²´ JSON
    print(json.dumps(combined, ensure_ascii=False, indent=2))

    if not args.split:
        with open("weekly_slack_message.txt","w",encoding="utf-8") as f:
            f.write("\n\nâ€” â€” â€”\n\n".join(combined_txt))

if __name__ == "__main__":
    main()
