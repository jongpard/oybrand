# -*- coding: utf-8 -*-
"""
Weekly ranking report generator (Slack + JSON)

- Ï£ºÍ∞Ñ Íµ¨Í∞Ñ: Í∞ÄÏû• ÏµúÍ∑º 'ÏôÑÍ≤∞ Ïõî~Ïùº' + ÏßÅÏ†Ñ Ï£º
- Top10 Ï†ïÎ†¨: (-Ïú†ÏßÄÏùº, ÌèâÍ∑†ÏàúÏúÑ, ÏµúÏ†ÄÏàúÏúÑ)
- Îì±ÎùΩ ÌëúÍ∏∞: (NEW/Ïú†ÏßÄ/‚Üën/‚Üìn)
- Ïù∏Ïï§ÏïÑÏõÉ: IN=OUT ‚Üí 'ÏùºÌèâÍ∑† X.YÍ∞ú'
- Ïù∏ÌîåÎ£®Ïñ∏ÏÑú: Ïò¨Î¶¨Î∏åÏòÅ Íµ≠ÎÇ¥Îßå, 'Ïò¨ÏòÅÌîΩ'Í≥º 'PICK(ÏΩúÎùºÎ≥¥)' ÏôÑÏ†Ñ Î∂ÑÎ¶¨
- ÏÑ±Î∂Ñ ÌÇ§ÏõåÎìú: configs/ingredients.txt ÎèôÏ†Å Î°úÎìú(ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Î™©Î°ù)
- Ïñ¥Îñ§ CSVÎùºÎèÑ Í≤∞ÏÜê Ïª¨ÎüºÏù¥ ÏûàÏñ¥ÎèÑ Ï£ΩÏßÄ ÏïäÎèÑÎ°ù Î∞©Ïñ¥ Ï≤òÎ¶¨

Ï∂úÎ†•:
  - slack_{src}.txt
  - weekly_summary_{src}.json

ÏÇ¨Ïö©:
  python scripts/weekly_report_plus.py --src all --data-dir ./data/daily
"""

import argparse
import json
import os
import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import date, timedelta
from typing import Dict, List, Optional, Tuple

import pandas as pd


# ----------------------------- ÏÜåÏä§ Ïä§Ìéô -----------------------------
SRC_SPECS = {
    "oy_kor":    {"title": "Ïò¨Î¶¨Î∏åÏòÅ Íµ≠ÎÇ¥ Top100",     "topn": 100},
    "oy_global": {"title": "Ïò¨Î¶¨Î∏åÏòÅ Í∏ÄÎ°úÎ≤å Top100",   "topn": 100},
    "amazon_us": {"title": "ÏïÑÎßàÏ°¥ US Top100",         "topn": 100},
    "qoo10_jp":  {"title": "ÌÅêÌÖê Ïû¨Ìå¨ Î∑∞Ìã∞ Top200",    "topn": 200},
    "daiso_kr":  {"title": "Îã§Ïù¥ÏÜåÎ™∞ Î∑∞Ìã∞/ÏúÑÏÉù Top200", "topn": 200},
}

FILENAME_HINTS = {
    "oy_kor":    ["Ïò¨Î¶¨Î∏åÏòÅ_Îû≠ÌÇπ", "Ïò¨Î¶¨Î∏åÏòÅÍµ≠ÎÇ¥", "Ïò¨Î¶¨Î∏åÏòÅ Íµ≠ÎÇ¥", "oy_kor"],
    "oy_global": ["Ïò¨Î¶¨Î∏åÏòÅÍ∏ÄÎ°úÎ≤å", "oy_global"],
    "amazon_us": ["ÏïÑÎßàÏ°¥US", "amazon_us", "amazon"],
    "qoo10_jp":  ["ÌÅêÌÖêÏû¨Ìå¨", "ÌÅêÌÖê Ïû¨Ìå¨", "qoo10_jp", "qoo10"],
    "daiso_kr":  ["Îã§Ïù¥ÏÜåÎ™∞", "daiso_kr", "daiso"],
}

# ÌååÏùºÎßàÎã§ ÌëúÍ∏∞Í∞Ä Ï†úÍ∞ÅÍ∞ÅÏù¥Ïñ¥ÏÑú ÌõÑÎ≥¥Î•º ÎÑìÍ≤å Ïû°Ïùå
RANK_COLS  = ["rank", "ÏàúÏúÑ", "Îû≠ÌÇπ", "ranking", "Rank", "ÏàúÎ≤à"]
BRAND_COLS = ["brand", "Î∏åÎûúÎìú", "Brand", "ÏÉÅÌëú", "Ï†úÏ°∞ÏÇ¨/Î∏åÎûúÎìú"]
NAME_COLS  = [
    "raw_name", "Ï†úÌíàÎ™Ö", "ÏÉÅÌíàÎ™Ö", "name", "title", "displayName", "product_name",
    "item_name", "ÏÉÅÌíàÎ™Ö(ÏòµÏÖò)", "ÏÉÅÌíà", "ÌíàÎ™©Î™Ö", "Î™®Îç∏Î™Ö"
]
URL_COLS   = ["url", "URL", "link", "Ï£ºÏÜå", "ÎßÅÌÅ¨", "ÏÉÅÌíàURL", "page_url", "detail_url"]

SKU_KEYS = ["goodsNo", "productId", "asin", "product_code", "pdNo", "sku", "id", "item_id", "url_key"]


# ----------------------- Ïò¨ÏòÅÌîΩ / PICK / ÏÑ±Î∂Ñ -----------------------
RE_OY_PICK = re.compile(r"(Ïò¨ÏòÅÌîΩ|Ïò¨Î¶¨Î∏åÏòÅ\s*ÌîΩ)\b", re.I)
RE_INFL_PICK = re.compile(r"([Í∞Ä-Ìû£A-Za-z0-9.&/_-]+)\s*(ÌîΩ|Pick)\b", re.I)
EXCLUDE_INFL = {"Ïò¨ÏòÅ", "Ïò¨Î¶¨Î∏åÏòÅ", "ÏõîÏò¨ÏòÅ", "ÏõêÌîΩ"}

PAT_MARKETING = {
    "Ïò¨ÏòÅÌîΩ":     r"(Ïò¨ÏòÅÌîΩ|Ïò¨Î¶¨Î∏åÏòÅ\s*ÌîΩ)",
    "ÌäπÍ∞Ä":       r"(ÌäπÍ∞Ä|Ìï´Îîú|ÏÑ∏Ïùº|Ìï†Ïù∏)",
    "ÏÑ∏Ìä∏":       r"(ÏÑ∏Ìä∏|Ìå®ÌÇ§ÏßÄ|Ìä∏Î¶¨Ïò§|ÎìÄÏò§|ÏÑ∏Ìä∏ÌÇ∑|ÌÇ§Ìä∏|ÌÇ∑\b)",
    "Í∏∞Ìöç":       r"(Í∏∞Ìöç|Í∏∞ÌöçÏ†Ñ)",
    "1+1/Ï¶ùÏ†ï":   r"(1\+1|1\+2|Îç§|Ï¶ùÏ†ï|ÏÇ¨ÏùÄÌíà)",
    "ÌïúÏ†ï/NEW":   r"(ÌïúÏ†ï|Î¶¨ÎØ∏Ìã∞Îìú|NEW|Îâ¥\b)",
    "Ïø†Ìè∞/Îîú":    r"(Ïø†Ìè∞|Îîú\b|ÎîúÍ∞Ä|ÌîÑÎ°úÎ™®ÏÖò|ÌîÑÎ°úÎ™®\b)",
}
PAT_MARKETING = {k: re.compile(v, re.I) for k, v in PAT_MARKETING.items()}

DEFAULT_INGRS = [
    "ÌûàÏïåÎ£®Î°†ÏÇ∞","ÏÑ∏ÎùºÎßàÏù¥Îìú","ÎÇòÏù¥ÏïÑÏã†ÏïÑÎßàÏù¥Îìú","Î†àÌã∞ÎÜÄ","Ìé©ÌÉÄÏù¥Îìú","ÏΩúÎùºÍ≤ê",
    "ÎπÑÌÉÄÎØºC","BHA","AHA","PHA","ÌåêÌÖåÎÜÄ","ÏÑºÌÖîÎùº","ÎßàÎç∞Ïπ¥ÏÜåÏÇ¨Ïù¥Îìú",
]

def load_ingredients() -> List[str]:
    path = os.path.join("configs", "ingredients.txt")
    if not os.path.exists(path):
        return DEFAULT_INGRS[:]
    out: List[str] = []
    with open(path, "r", encoding="utf-8") as f:
        for ln in f:
            ln = ln.strip()
            if not ln or ln.startswith("#"):
                continue
            out.append(ln)
    return out or DEFAULT_INGRS[:]

INGR_WORDS = load_ingredients()


# ------------------------------ Ïú†Ìã∏ ------------------------------
def first_existing(cols, candidates) -> Optional[str]:
    for c in candidates:
        if c in cols:
            return c
    lower = {c.lower(): c for c in cols}
    for c in candidates:
        if c.lower() in lower:
            return lower[c.lower()]
    return None

def parse_query(url: str, key: str) -> Optional[str]:
    if not url:
        return None
    m = re.search(r"[?&]" + re.escape(key) + r"=([^&#]+)", url)
    return m.group(1) if m else None

def normalize_key(s: str) -> str:
    return re.sub(r"\s+", "", str(s).lower())

def guess_src_from_filename(fn: str) -> Optional[str]:
    key = normalize_key(fn)
    for src, hints in FILENAME_HINTS.items():
        for h in hints:
            if normalize_key(h) in key:
                return src
    return None

def parse_date_from_filename(fn: str) -> Optional[date]:
    # 2025-08-23 / 2025_08_23 / 2025.08.23 ÌóàÏö©
    m = re.search(r"(20\d{2})[-_\.](\d{2})[-_\.](\d{2})", fn)
    if not m:
        return None
    try:
        return date(int(m.group(1)), int(m.group(2)), int(m.group(3)))
    except Exception:
        return None

def last_complete_week(today: Optional[date] = None) -> Tuple[date, date]:
    today = today or date.today()
    weekday = today.weekday()  # Ïõî=0 ... Ïùº=6
    last_sun = today - timedelta(days=weekday + 1)
    start = last_sun - timedelta(days=6)
    return start, last_sun

def prev_week_range(start: date, end: date) -> Tuple[date, date]:
    return (start - timedelta(days=7), end - timedelta(days=7))

def within(d: date, start: date, end: date) -> bool:
    return start <= d <= end


# ------------------------- Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨/Ï†ïÏ†ú -------------------------
def read_csv_any(path: str) -> pd.DataFrame:
    # ÏÑúÎ°ú Îã§Î•∏ Ïù∏ÏΩîÎî©ÏùÑ ÏµúÎåÄÌïú Ìù°Ïàò
    for enc in ("utf-8", "cp949", "utf-8-sig", "latin1"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path)

def unify_cols(df: pd.DataFrame) -> pd.DataFrame:
    """Ïª¨Îüº Ïù¥Î¶ÑÏù¥ Ï†úÍ∞ÅÍ∞ÅÏù∏ CSVÎ•º ÌëúÏ§Ä Ïª¨ÎüºÏúºÎ°ú ÎßûÏ∂òÎã§.
    - Î∞òÎìúÏãú rank, brand, raw_name, url Ïª¨ÎüºÏùÑ Í∞ÄÏßÑ DataFrameÏùÑ Î∞òÌôò
      (ÏóÜÏúºÎ©¥ Îπà Î¨∏ÏûêÏó¥/NaNÏúºÎ°ú Ï±ÑÏõåÏÑúÎùºÎèÑ ÏÉùÏÑ±)
    """
    cols = list(df.columns)
    out = pd.DataFrame()

    # ÏàúÏúÑ
    r = first_existing(cols, RANK_COLS)
    if r:
        out["rank"] = pd.to_numeric(df[r], errors="coerce")
    else:
        # rankÍ∞Ä ÏóÜÏúºÎ©¥ Ïù¥ ÌååÏùºÏùÄ Î¨¥ÏãúÎê† Í≤É(ÏÉÅÏúÑÏóêÏÑú Ï≤¥ÌÅ¨)
        out["rank"] = pd.Series(dtype="float64")

    # Î∏åÎûúÎìú/Ï†úÌíàÎ™Ö/URL
    b = first_existing(cols, BRAND_COLS)
    n = first_existing(cols, NAME_COLS)
    u = first_existing(cols, URL_COLS)

    out["brand"]    = df[b].fillna("").astype(str) if b else ""
    out["raw_name"] = df[n].fillna("").astype(str) if n else ""
    out["url"]      = df[u].fillna("").astype(str) if u else ""

    # ÌòπÏãúÎùºÎèÑ Ï†ÑÎ∂Ä ÎπÑÎ©¥ ÏµúÏÜåÌïú Í≥µÎ∞± Î¨∏ÏûêÏó¥ ÌòïÌÉúÎ°ú Î≥¥Ïû•
    for col in ("brand", "raw_name", "url"):
        if col not in out.columns:
            out[col] = ""
        out[col] = out[col].fillna("").astype(str)

    return out

def load_files_for_range(src: str, data_dir: str, start: date, end: date) -> List[str]:
    outs = []
    if not os.path.isdir(data_dir):
        print(f"[scan] {src}: data_dir not found: {data_dir}")
        return []
    for fn in os.listdir(data_dir):
        full = os.path.join(data_dir, fn)
        if not os.path.isfile(full):
            continue
        d = parse_date_from_filename(fn)
        if not d or not within(d, start, end):
            continue
        if guess_src_from_filename(fn) == src:
            outs.append(full)
    print(f"[scan] {src}: range {start}~{end} -> {len(outs)} file(s) in {data_dir}")
    return sorted(outs)

def extract_sku(row: Dict, src: str) -> str:
    for k in SKU_KEYS:
        if k in row and str(row[k]).strip():
            return str(row[k]).strip()
    url = str(row.get("url", "") or "")
    if src == "oy_kor":
        return parse_query(url, "goodsNo") or url
    if src == "oy_global":
        return parse_query(url, "productId") or url
    if src == "amazon_us":
        if row.get("asin"):
            return str(row["asin"])
        m = re.search(r"/([A-Z0-9]{10})(?:[/?]|$)", url)
        return m.group(1) if m else url
    if src == "qoo10_jp":
        return parse_query(url, "product_code") or url
    if src == "daiso_kr":
        return parse_query(url, "pdNo") or url
    return url

def load_week_df(src: str, data_dir: str, start: date, end: date, topn: int) -> pd.DataFrame:
    files = load_files_for_range(src, data_dir, start, end)
    frames = []
    for p in files:
        d = parse_date_from_filename(os.path.basename(p))
        df = unify_cols(read_csv_any(p))
        # ÏàúÏúÑ ÏóÜÏúºÎ©¥ Ïä§ÌÇµ
        if "rank" not in df.columns or df["rank"].isna().all():
            continue
        df = df[df["rank"].notnull()].sort_values("rank").head(topn).copy()
        df["date_str"] = (d or start).strftime("%Y-%m-%d")
        frames.append(df)
    if not frames:
        return pd.DataFrame(columns=["rank","brand","raw_name","url","date_str"])
    return pd.concat(frames, ignore_index=True)


# --------------------------- Ï£ºÍ∞Ñ ÌÜµÍ≥Ñ ---------------------------
@dataclass
class ItemStat:
    sku: str
    raw_name: str
    brand: str
    url: str
    days: int
    avg_rank: float
    min_rank: float

def _safe_mode(series: Optional[pd.Series]) -> str:
    if series is None:
        return ""
    s = series.dropna().astype(str)
    if s.empty:
        return ""
    try:
        return s.mode().iloc[0]
    except Exception:
        return s.iloc[0]

def build_stats(src: str, df: pd.DataFrame, topn: int) -> Dict[str, ItemStat]:
    stats: Dict[str, ItemStat] = {}
    if df.empty:
        return stats
    # ÌïÑÏàò Ïª¨Îüº Î≥¥Ïû•
    for col in ("raw_name", "brand", "url"):
        if col not in df.columns:
            df[col] = ""

    df["sku"] = df.apply(lambda r: extract_sku(r, src), axis=1)
    for sku, sub in df.groupby("sku"):
        raw  = _safe_mode(sub.get("raw_name"))
        br   = _safe_mode(sub.get("brand"))
        url  = _safe_mode(sub.get("url"))
        days = sub["date_str"].nunique() if "date_str" in sub else 0
        avg  = float(pd.to_numeric(sub["rank"], errors="coerce").mean())
        minr = float(pd.to_numeric(sub["rank"], errors="coerce").min())
        stats[sku] = ItemStat(sku, raw, br, url, days, avg, minr)
    return stats

def compare_prev(curr: Dict[str, ItemStat], prev: Dict[str, ItemStat]) -> Dict[str, Optional[float]]:
    deltas: Dict[str, Optional[float]] = {}
    for sku, st in curr.items():
        if sku in prev:
            deltas[sku] = prev[sku].avg_rank - st.avg_rank
        else:
            deltas[sku] = None
    return deltas

def arrow(d: Optional[float]) -> str:
    if d is None: return "NEW"
    val = int(round(abs(d)))
    if val == 0: return "Ïú†ÏßÄ"
    return f"‚Üë{val}" if d > 0 else f"‚Üì{val}"

def top10_for_display(stats: Dict[str, ItemStat], deltas: Dict[str, Optional[float]]) -> Tuple[List[str], List[Dict]]:
    items = sorted(stats.values(), key=lambda s: (-s.days, s.avg_rank, s.min_rank))[:10]
    slack_lines, html_items = [], []
    for i, st in enumerate(items, 1):
        ar = arrow(deltas.get(st.sku))
        link_txt = f"<{st.url}|{st.raw_name}>" if st.url else st.raw_name
        slack_lines.append(f"{i}. {link_txt} (Ïú†ÏßÄ {st.days}Ïùº ¬∑ ÌèâÍ∑† {st.avg_rank:.1f}ÏúÑ) ({ar})")
        html_items.append({"idx": i, "name": st.raw_name, "url": st.url, "days": st.days, "avg": round(st.avg_rank, 1), "arrow": ar})
    return slack_lines, html_items

def brand_daily_avg(df: pd.DataFrame) -> Dict[str, float]:
    if df.empty: return {}
    outs = []
    for d, sub in df.groupby("date_str"):
        cnt = Counter([str(x) for x in sub.get("brand", pd.Series([], dtype=str)).fillna("").tolist() if str(x).strip()])
        outs.append(cnt)
    total = Counter()
    for c in outs:
        total.update(c)
    days = max(1, len(outs))
    avg = {k: round(v / days, 1) for k, v in total.items()}
    return dict(sorted(avg.items(), key=lambda x: (-x[1], x[0])))

def inout_avg_per_day(df: pd.DataFrame, src: str) -> float:
    if df.empty: return 0.0
    if "date_str" not in df.columns: return 0.0
    df = df.copy()
    df["sku"] = df.apply(lambda r: extract_sku(r, src), axis=1)
    days = sorted(df["date_str"].unique())
    if len(days) <= 1: return 0.0
    changes = []
    prev_set = set()
    for d in days:
        now = set(df[df["date_str"]==d]["sku"])
        if prev_set:
            changes.append(len(now - prev_set))  # IN == OUT
        prev_set = now
    return round(sum(changes)/len(changes), 1) if changes else 0.0

def hero_and_flash(stats: Dict[str, ItemStat], prev_stats: Dict[str, ItemStat]) -> Tuple[List[str], List[str]]:
    heroes, flashes = [], []
    for sku, st in stats.items():
        if st.days >= 3 and sku not in prev_stats:
            heroes.append(st.raw_name)
        if st.days <= 2:
            flashes.append(st.raw_name)
    return heroes[:10], flashes[:10]

def parse_marketing_and_infl(raw_name: str) -> Tuple[Dict[str, bool], Optional[str]]:
    name = raw_name or ""
    mk = {k: bool(p.search(name)) for k, p in PAT_MARKETING.items()}
    infl = None
    m = RE_INFL_PICK.search(name)
    if m:
        cand = re.sub(r"[\[\](),.|¬∑]", "", m.group(1)).strip()
        if cand and cand not in EXCLUDE_INFL and not RE_OY_PICK.search(name):
            infl = cand
    return mk, infl

def extract_ingredients(raw_name: str, ingr_list=None) -> List[str]:
    name = raw_name or ""
    ingr_list = ingr_list or INGR_WORDS
    out: List[str] = []
    for w in ingr_list:
        if re.search(re.escape(w), name, re.I):
            out.append(w)
    return out

def kw_summary(src: str, df: pd.DataFrame) -> Dict[str, any]:
    out = {
        "unique": 0,
        "marketing": defaultdict(int),
        "influencers": defaultdict(int),
        "ingredients": defaultdict(int),
    }
    if df.empty: return {"unique": 0, "marketing":{}, "influencers":{}, "ingredients":{}}

    df = df.copy()
    if "raw_name" not in df.columns: df["raw_name"] = ""
    df["sku"] = df.apply(lambda r: extract_sku(r, src), axis=1)

    uniq = set()
    seen_mk = set()
    for _, r in df.iterrows():
        sku = r["sku"]
        raw = (r.get("raw_name") or "").strip()
        uniq.add(sku)

        mk, infl = parse_marketing_and_infl(raw)
        for k, v in mk.items():
            if v and (sku, k) not in seen_mk:
                out["marketing"][k] += 1
                seen_mk.add((sku, k))

        if src == "oy_kor" and infl:
            out["influencers"][infl] += 1

        for ing in extract_ingredients(raw, INGR_WORDS):
            out["ingredients"][ing] += 1

    out["unique"] = len(uniq)
    out["marketing"]   = dict(sorted(out["marketing"].items(),   key=lambda x: (-x[1], x[0])))
    out["influencers"] = dict(sorted(out["influencers"].items(), key=lambda x: (-x[1], x[0])))
    out["ingredients"] = dict(sorted(out["ingredients"].items(), key=lambda x: (-x[1], x[0])))
    return out


# --------------------------- Ìè¨Îß∑(Ïä¨Îûô/JSON) ---------------------------
def format_kw_for_slack(kw: Dict[str, any]) -> str:
    if kw.get("unique",0) == 0:
        return "Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå"
    lines = []
    lines.append("üìä *Ï£ºÍ∞Ñ ÌÇ§ÏõåÎìú Î∂ÑÏÑù*")
    lines.append(f"- Ïú†ÎãàÌÅ¨ SKU: {kw['unique']}Í∞ú")
    if kw["marketing"]:
        lines.append("‚Ä¢ *ÎßàÏºÄÌåÖ ÌÇ§ÏõåÎìú*")
        for k, cnt in kw["marketing"].items():
            ratio = round(cnt * 100.0 / max(1, kw["unique"]), 1)
            lines.append(f"  - {k}: {cnt}Í∞ú ({ratio}%)")
    if kw["influencers"]:
        lines.append("‚Ä¢ *Ïù∏ÌîåÎ£®Ïñ∏ÏÑú*")
        for k, cnt in kw["influencers"].items():
            lines.append(f"  - {k}: {cnt}Í∞ú")
    if kw["ingredients"]:
        lines.append("‚Ä¢ *ÏÑ±Î∂Ñ ÌÇ§ÏõåÎìú*")
        for k, cnt in kw["ingredients"].items():
            lines.append(f"  - {k}: {cnt}Í∞ú")
    return "\n".join(lines)

def format_brand_lines(avg_counts: Dict[str, float], limit: int = 15) -> List[str]:
    return [f"{k} {v}Í∞ú/Ïùº" for k, v in list(avg_counts.items())[:limit]]

def build_slack(src: str, range_str: str,
                top10_lines: List[str],
                brand_lines: List[str],
                inout_avg: float,
                heroes: List[str],
                flashes: List[str],
                kw_text: str,
                unique_cnt: int,
                keep_days_mean: float) -> str:
    title = SRC_SPECS[src]["title"]
    lines = []
    lines.append(f"üìà *Ï£ºÍ∞Ñ Î¶¨Ìè¨Ìä∏ ¬∑ {title} ({range_str})*")
    lines.append("")
    lines.append("üèÜ *Top10*")
    lines += (top10_lines or ["Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå"])
    lines.append("")
    lines.append("üì¶ *Î∏åÎûúÎìú Í∞úÏàò(ÏùºÌèâÍ∑†)*")
    lines += (brand_lines or ["Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå"])
    lines.append("")
    lines.append("üîÅ *Ïù∏Ïï§ÏïÑÏõÉ(ÍµêÏ≤¥)*")
    lines.append(f"- ÏùºÌèâÍ∑† {inout_avg}Í∞ú")
    lines.append("")
    lines.append("üÜï *Ïã†Í∑ú ÌûàÏñ¥Î°ú(‚â•3Ïùº Ïú†ÏßÄ)*")
    lines.append("ÏóÜÏùå" if not heroes else "¬∑ " + " ¬∑ ".join(heroes[:8]))
    lines.append("‚ú® *Î∞òÏßù ÏïÑÏù¥ÌÖú(‚â§2Ïùº)*")
    lines.append("ÏóÜÏùå" if not flashes else "¬∑ " + " ¬∑ ".join(flashes[:8]))
    lines.append("")
    lines.append("üìå *ÌÜµÍ≥Ñ*")
    lines.append(f"- Top{SRC_SPECS[src]['topn']} Îì±Í∑π SKU : {unique_cnt}Í∞ú")
    lines.append(f"- Top {SRC_SPECS[src]['topn']} Ïú†ÏßÄ ÌèâÍ∑† : {keep_days_mean:.1f}Ïùº")
    lines.append("")
    lines.append(kw_text)
    return "\n".join(lines)


# ------------------------------ Î©îÏù∏ ------------------------------
def run_for_source(src: str, data_dir: str) -> Dict[str, any]:
    spec = SRC_SPECS[src]
    topn = spec["topn"]

    start, end = last_complete_week()
    prev_start, prev_end = prev_week_range(start, end)
    range_str = f"{start:%Y-%m-%d}-{end:%Y-%m-%d}"

    cur_df  = load_week_df(src, data_dir, start, end, topn)
    prev_df = load_week_df(src, data_dir, prev_start, prev_end, topn)

    cur_stats  = build_stats(src, cur_df,  topn)
    prev_stats = build_stats(src, prev_df, topn)

    deltas = compare_prev(cur_stats, prev_stats)

    top10_lines, top10_html_items = top10_for_display(cur_stats, deltas)
    brand_lines = format_brand_lines(brand_daily_avg(cur_df))
    inout_avg   = inout_avg_per_day(cur_df, src)
    heroes, flashes = hero_and_flash(cur_stats, prev_stats)

    kw  = kw_summary(src, cur_df)
    kw_text = format_kw_for_slack(kw)

    unique_cnt = len(cur_stats)
    keep_days_mean = 0.0
    if cur_df.shape[0] > 0 and len(cur_stats) > 0:
        keep_days_mean = sum(st.days for st in cur_stats.values()) / max(1, len(cur_stats))

    slack_text = build_slack(
        src, range_str, top10_lines, brand_lines, inout_avg, heroes, flashes,
        kw_text, unique_cnt, round(keep_days_mean, 1)
    )
    with open(f"slack_{src}.txt", "w", encoding="utf-8") as f:
        f.write(slack_text)

    summary = {
        "range": range_str,
        "title": SRC_SPECS[src]["title"],
        "topn": topn,
        "top10_items": top10_html_items,
        "brand_lines": brand_lines or ["Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå"],
        "inout_avg": inout_avg,
        "heroes": heroes,
        "flashes": flashes,
        "kw": kw,
        "unique_cnt": unique_cnt,
        "keep_days_mean": round(keep_days_mean, 1),
    }
    with open(f"weekly_summary_{src}.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    return summary

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--src", choices=list(SRC_SPECS.keys()) + ["all"], required=True)
    ap.add_argument("--data-dir", default="./data/daily")
    args = ap.parse_args()

    os.makedirs(args.data_dir, exist_ok=True)

    if args.src == "all":
        results = {}
        for s in SRC_SPECS.keys():
            print(f"[run] {s}")
            results[s] = run_for_source(s, args.data_dir)
        print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        res = run_for_source(args.src, args.data_dir)
        print(json.dumps(res, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
