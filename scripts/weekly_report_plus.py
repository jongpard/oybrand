# scripts/weekly_report_plus.py
# -*- coding: utf-8 -*-

"""
ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± (SKU í‘œì¤€í™” í¬í•¨)
- ì‚¬ìš© ì˜ˆ: python scripts/weekly_report_plus.py --src all --data-dir ./data/daily
- ì¶œë ¥:
  - slack_oy_kor.txt, slack_oy_global.txt, slack_amazon_us.txt, slack_qoo10_jp.txt, slack_daiso_kr.txt
  - weekly_summary_oy_kor.json, ... (ì†ŒìŠ¤ë³„)
"""

from __future__ import annotations
import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
from urllib.parse import urlparse, parse_qs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = ["oy_kor", "oy_global", "amazon_us", "qoo10_jp", "daiso_kr"]

# íŒŒì¼ëª… íŒ¨í„´ (ì¼ì YYYY-MM-DD íŒŒì‹±ìš©)
FILENAME_PATTERNS = {
    "oy_kor":       r"^ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "oy_global":    r"^ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "amazon_us":    r"^ì•„ë§ˆì¡´US_ë·°í‹°_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "qoo10_jp":     r"^íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "daiso_kr":     r"^ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_(\d{4}-\d{2}-\d{2})\.csv$",
}

# ì†ŒìŠ¤ë³„ Slack/JSON ì œëª©
TITLES = {
    "oy_kor":    "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",
    "oy_global": "ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",
    "amazon_us": "ì•„ë§ˆì¡´ US Top100",
    "qoo10_jp":  "íí… ì¬íŒ¬ ë·°í‹° Top200",
    "daiso_kr":  "ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200",
}

TOPN = {
    "oy_kor": 100,
    "oy_global": 100,
    "amazon_us": 100,
    "qoo10_jp": 200,
    "daiso_kr": 200,
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def this_week_range(today: Optional[datetime] = None) -> tuple[datetime, datetime]:
    """
    ì´ë²ˆ ì£¼ ì›”ìš”ì¼~ì¼ìš”ì¼ ë²”ìœ„(ë¡œì»¬ ë‚ ì§œ ê¸°ì¤€).
    (GitHub Actionsì—ì„œëŠ” UTC, í•œêµ­ê¸°ì¤€ì´ë©´ KST ë³´ì •ì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê³„ì‚°)
    """
    if today is None:
        today = datetime.utcnow()
    # ì›”ìš”ì¼=0
    monday = today - timedelta(days=today.weekday())
    monday = datetime(monday.year, monday.month, monday.day)
    sunday = monday + timedelta(days=6)
    return monday, sunday


def parse_date_from_filename(name: str, src: str) -> Optional[datetime]:
    pat = FILENAME_PATTERNS.get(src)
    if not pat:
        return None
    m = re.match(pat, name)
    if not m:
        return None
    try:
        return datetime.strptime(m.group(1), "%Y-%m-%d")
    except Exception:
        return None


def read_csv_safe(path: Path) -> Optional[pd.DataFrame]:
    try:
        return pd.read_csv(path, encoding="utf-8-sig", low_memory=False)
    except Exception:
        try:
            return pd.read_csv(path, encoding="utf-8", low_memory=False)
        except Exception:
            return None


def rename_common_columns(df: pd.DataFrame, src: str) -> pd.DataFrame:
    """
    ì†ŒìŠ¤ë³„ë¡œ ë“¤ì­‰ë‚ ì­‰í•œ ì»¬ëŸ¼ëª…ì„ ê³µí†µ í‚¤ë¡œ ë§ì¶˜ë‹¤.
    ìµœì†Œí•œ ì•„ë˜ í‚¤ë§Œ ë§ì¶˜ë‹¤:
    - date
    - rank
    - product_name
    - brand
    - url
    - asin / product_code (ê·¸ëŒ€ë¡œ ìœ ì§€)
    """
    df = df.copy()

    # name -> product_name
    if "product_name" not in df.columns and "name" in df.columns:
        df = df.rename(columns={"name": "product_name"})

    # rank í˜•ì‹ ë³´ì •
    if "rank" in df.columns:
        df["rank"] = pd.to_numeric(df["rank"], errors="coerce")
    # date í˜•ì‹ ë³´ì •
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date

    # brand ì—†ì„ ìˆ˜ë„ ìˆë‹¤ (ê·¸ëƒ¥ ë‘ )
    # url ì—†ëŠ” ê²½ìš°ëŠ” ê±°ì˜ ì—†ìŒ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    if "url" not in df.columns:
        df["url"] = None

    return df


def extract_query_param(url: str, key: str) -> Optional[str]:
    if not isinstance(url, str) or not url:
        return None
    try:
        q = parse_qs(urlparse(url).query)
        v = q.get(key)
        return v[0] if v else None
    except Exception:
        return None


def ensure_sku_column(df: pd.DataFrame, src: str) -> pd.DataFrame:
    """
    ì†ŒìŠ¤ë³„ë¡œ ê³µí†µ sku ì»¬ëŸ¼ ìƒì„±:
      oy_kor    -> url?goodsNo
      oy_global -> url?productId
      amazon_us -> asin
      qoo10_jp  -> product_code
      daiso_kr  -> url?pdNo
    """
    if df is None or len(df) == 0:
        return df

    df = df.copy()

    if src == "oy_kor":
        if "sku" not in df.columns:
            df["sku"] = df["url"].apply(lambda u: extract_query_param(u, "goodsNo"))
    elif src == "oy_global":
        if "sku" not in df.columns:
            df["sku"] = df["url"].apply(lambda u: extract_query_param(u, "productId"))
    elif src == "amazon_us":
        if "sku" not in df.columns:
            df["sku"] = df.get("asin")
    elif src == "qoo10_jp":
        if "sku" not in df.columns:
            df["sku"] = df.get("product_code")
    elif src == "daiso_kr":
        if "sku" not in df.columns:
            df["sku"] = df["url"].apply(lambda u: extract_query_param(u, "pdNo"))

    # ë¬¸ìì—´ ì •ë¦¬
    if "sku" in df.columns:
        df["sku"] = df["sku"].astype(str).str.strip()
        df["sku"] = df["sku"].replace({"": None, "None": None})
    else:
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ urlì„ fallback (ì¤‘ë³µ ê°€ëŠ¥ì„± í¼)
        df["sku"] = df.get("url")

    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œë”©/í•„í„°ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_source_files(data_dir: Path, src: str, start: datetime, end: datetime) -> List[Path]:
    files: List[Path] = []
    for p in sorted(data_dir.glob("*.csv")):
        d = parse_date_from_filename(p.name, src)
        if d is None:
            continue
        if start.date() <= d.date() <= end.date():
            files.append(p)
    return files


def load_week_df(src: str, data_dir: Path, start: datetime, end: datetime) -> pd.DataFrame:
    files = list_source_files(data_dir, src, start, end)
    frames: List[pd.DataFrame] = []
    for f in files:
        df = read_csv_safe(f)
        if df is None or len(df) == 0:
            continue
        df = rename_common_columns(df, src)

        # íŒŒì¼ëª… ì¼ìë¥¼ date ê²°ì¸¡ì— ì±„ì›Œë„£ê¸° (ì—†ìœ¼ë©´)
        if "date" in df.columns:
            # ì¼ë¶€ íŒŒì¼ì— dateê°€ ì „ë¶€ NaTì¸ ê²½ìš° ë³´ì™„
            if df["date"].isna().all():
                d = parse_date_from_filename(f.name, src)
                if d:
                    df["date"] = d.date()
        else:
            d = parse_date_from_filename(f.name, src)
            df["date"] = d.date() if d else None

        df = ensure_sku_column(df, src)

        # ê¸°ë³¸ ì •ë ¬/í•„í„°
        if "rank" in df.columns:
            df = df[df["rank"].notna()]
        frames.append(df)

    if not frames:
        return pd.DataFrame()

    out = pd.concat(frames, ignore_index=True)
    # ë²”ìœ„ë¡œ í•œ ë²ˆ ë” í•„í„°ë§
    out = out[(pd.to_datetime(out["date"], errors="coerce") >= start.date()) &
              (pd.to_datetime(out["date"], errors="coerce") <= end.date())]
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µê³„/ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class WeeklyStats:
    unique_cnt: int
    keep_days_mean: float
    topn_items: List[Dict]  # [{sku, name, brand, avg_rank, latest_rank, days}, ...]


def calc_weekly_stats(df: pd.DataFrame, src: str) -> WeeklyStats:
    if df is None or len(df) == 0:
        return WeeklyStats(unique_cnt=0, keep_days_mean=0.0, topn_items=[])

    # product_name ë³´ì •
    if "product_name" not in df.columns:
        df["product_name"] = None
    if "brand" not in df.columns:
        df["brand"] = None

    # sku ê¸°ì¤€ìœ¼ë¡œ ì¼ìˆ˜/í‰ê· ìˆœìœ„ ê³„ì‚°
    g = df.groupby("sku", dropna=True)
    days = g["date"].nunique().rename("days")
    avg_rank = g["rank"].mean().rename("avg_rank")

    # ìµœì‹ (ìµœì‹ ì¼ì) product_name/brand/rank
    df["dt"] = pd.to_datetime(df["date"], errors="coerce")
    latest_idx = df.sort_values(["sku", "dt"], ascending=[True, False]).groupby("sku").head(1).set_index("sku")
    latest_name = latest_idx["product_name"].rename("latest_name")
    latest_brand = latest_idx["brand"].rename("latest_brand")
    latest_rank = latest_idx["rank"].rename("latest_rank")

    stat_df = pd.concat([days, avg_rank, latest_name, latest_brand, latest_rank], axis=1).reset_index()
    stat_df = stat_df[stat_df["sku"].notna()]

    # ìƒìœ„ TOPN ì„ ì • (avg_rank ì˜¤ë¦„ì°¨ìˆœ)
    n = TOPN.get(src, 100)
    stat_df = stat_df.sort_values(["avg_rank", "latest_rank"], ascending=[True, True]).head(10)

    items = []
    for _, row in stat_df.iterrows():
        items.append({
            "sku": row.get("sku"),
            "name": row.get("latest_name"),
            "brand": row.get("latest_brand"),
            "avg_rank": float(row.get("avg_rank")) if pd.notna(row.get("avg_rank")) else None,
            "latest_rank": int(row.get("latest_rank")) if pd.notna(row.get("latest_rank")) else None,
            "days": int(row.get("days")) if pd.notna(row.get("days")) else 0,
        })

    # ì „ì²´ ìœ ë‹ˆí¬ / í‰ê·  ìœ ì§€ì¼
    unique_cnt = df["sku"].nunique(dropna=True)
    keep_days_mean = float(days.mean()) if not days.empty else 0.0

    return WeeklyStats(unique_cnt=unique_cnt, keep_days_mean=keep_days_mean, topn_items=items)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶œë ¥ (Slack í…ìŠ¤íŠ¸ / JSON)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_top10_lines(stats: WeeklyStats) -> List[str]:
    lines: List[str] = []
    if not stats.topn_items:
        lines.append("ë°ì´í„° ì—†ìŒ")
        return lines

    for i, it in enumerate(stats.topn_items, start=1):
        name = it.get("name") or "-"
        brand = it.get("brand") or "-"
        avg_rank = it.get("avg_rank")
        latest_rank = it.get("latest_rank")
        days = it.get("days", 0)
        # ì˜ˆ: "1. ë¸Œëœë“œ ì œí’ˆëª… (ìœ ì§€ 7ì¼ Â· í‰ê·  3.2ìœ„)"
        s = f"{i}. {name} (ìœ ì§€ {days}ì¼ Â· í‰ê·  {avg_rank:.1f}ìœ„)" if avg_rank else f"{i}. {name}"
        lines.append(s)
    return lines


def build_slack_text(src: str, start: datetime, end: datetime, stats: WeeklyStats) -> str:
    title = TITLES.get(src, src)
    period = f"{start.date()}~{end.date()}"
    top10_lines = format_top10_lines(stats)

    body = []
    body.append(f"ğŸ“ˆ ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title} ({period})")
    body.append("ğŸ† Top10")
    for ln in top10_lines:
        body.append(ln)
    body.append("")
    body.append("ğŸ“¦ í†µê³„")
    body.append(f"- Top{TOPN.get(src,100)} ë“±ê·¹ SKU : {stats.unique_cnt}ê°œ")
    body.append(f"- Top {TOPN.get(src,100)} ìœ ì§€ í‰ê·  : {stats.keep_days_mean:.1f}ì¼")
    return "\n".join(body)


def save_text(path: Path, text: str):
    path.write_text(text, encoding="utf-8")


def save_json(path: Path, obj: dict):
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ë³¸ì²´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_for_source(src: str, data_dir: Path, start: datetime, end: datetime):
    print(f"[run] {src}")
    cur_df = load_week_df(src, data_dir, start, end)
    stats = calc_weekly_stats(cur_df, src)

    # Slack í…ìŠ¤íŠ¸
    slack_text = build_slack_text(src, start, end, stats)
    save_text(Path(f"slack_{src}.txt"), slack_text)

    # ìš”ì•½ JSON
    summary = {
        "title": TITLES.get(src, src),
        "range": f"{start.date()}~{end.date()}",
        "topn": TOPN.get(src, 100),
        "top10_items": stats.topn_items,
        "unique_cnt": stats.unique_cnt,
        "keep_days_mean": round(stats.keep_days_mean, 2),
    }
    save_json(Path(f"weekly_summary_{src}.json"), summary)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--src", default="all", help="oy_kor,oy_global,amazon_us,qoo10_jp,daiso_kr,all")
    ap.add_argument("--data-dir", default="./data/daily")
    args = ap.parse_args()

    if args.src == "all":
        targets = SOURCES
    else:
        targets = [s.strip() for s in args.src.split(",") if s.strip() in SOURCES]
        if not targets:
            print("ìœ íš¨í•œ --src ê°€ ì•„ë‹™ë‹ˆë‹¤. ê°€ëŠ¥: ", SOURCES, file=sys.stderr)
            sys.exit(1)

    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        print(f"[ERR] DATA_DIR ì—†ìŒ: {data_dir}", file=sys.stderr)
        sys.exit(1)

    start, end = this_week_range()
    print(f"[scan] ê¸°ê°„ {start.date()} ~ {end.date()} | dir={data_dir}")

    for s in targets:
        run_for_source(s, data_dir, start, end)


if __name__ == "__main__":
    main()
