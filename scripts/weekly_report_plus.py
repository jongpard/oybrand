# -*- coding: utf-8 -*-
"""
Weekly ranking report generator (Slack + JSON)
- ì›”~ì¼ ê°€ì¥ ìµœê·¼ ì™„ê²° 7ì¼ ì§‘ê³„, ì§ì „ ì£¼ì™€ ë¹„êµ
- ì†ŒìŠ¤: oy_kor, oy_global, amazon_us, qoo10_jp, daiso_kr
- ì‚°ì¶œë¬¼:
  - slack_{src}.txt
  - weekly_summary_{src}.json
ì‚¬ìš©:
  python scripts/weekly_report_plus.py --src oy_kor --split --data-dir ./data/daily --min-days 3
"""

import argparse
import json
import os
import re
import sys
from collections import defaultdict, Counter
from dataclasses import dataclass
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional, Tuple

import pandas as pd

# ------------------------------- ê¸°ë³¸ ì„¤ì • -------------------------------

SRC_SPECS = {
    "oy_kor":    {"title": "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",    "topn": 100},
    "oy_global": {"title": "ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",  "topn": 100},
    "amazon_us": {"title": "ì•„ë§ˆì¡´ US Top100",        "topn": 100},
    "qoo10_jp":  {"title": "íí… ì¬íŒ¬ ë·°í‹° Top200",   "topn": 200},
    "daiso_kr":  {"title": "ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200","topn": 200},
}

# íŒŒì¼ëª…ìœ¼ë¡œ ì†ŒìŠ¤ ì‹ë³„ (ëŠìŠ¨í•œ í•œê¸€ í¬í•¨)
FILENAME_HINTS = {
    "oy_kor":    ["ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹", "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´", "oy_kor"],
    "oy_global": ["ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ", "oy_global"],
    "amazon_us": ["ì•„ë§ˆì¡´US", "amazon_us"],
    "qoo10_jp":  ["íí…ì¬íŒ¬", "íí… ì¬íŒ¬", "qoo10_jp"],
    "daiso_kr":  ["ë‹¤ì´ì†Œëª°", "daiso_kr"],
}

SKU_KEY_CANDIDATES = [
    "goodsNo", "productId", "asin", "product_code", "pdNo",
    "item_id", "id", "sku", "url_key"
]

RANK_COL_CAND = ["rank", "ìˆœìœ„", "ë­í‚¹", "ranking", "Rank"]
BRAND_COL_CAND = ["brand", "ë¸Œëœë“œ", "Brand"]
NAME_COL_CAND = ["raw_name", "ì œí’ˆëª…", "ìƒí’ˆëª…", "name", "title"]
URL_COL_CAND  = ["url", "URL", "link", "ì£¼ì†Œ", "ë§í¬"]

# -------------------------- ì˜¬ì˜í”½/PICK/ì„±ë¶„ íŒŒì„œ -------------------------

RE_OY_PICK  = re.compile(r"(ì˜¬ì˜í”½|ì˜¬ë¦¬ë¸Œì˜\s*í”½)\b", re.I)
RE_INFL_PK  = re.compile(r"([ê°€-í£A-Za-z0-9.&/_-]+)\s*(í”½|Pick)\b", re.I)
EXCLUDE_INFL = {"ì˜¬ì˜", "ì˜¬ë¦¬ë¸Œì˜", "ì›”ì˜¬ì˜", "ì›í”½"}

PAT_MARKETING = {
    "ì˜¬ì˜í”½"   : r"(ì˜¬ì˜í”½|ì˜¬ë¦¬ë¸Œì˜\s*í”½)",
    "íŠ¹ê°€"     : r"(íŠ¹ê°€|í•«ë”œ|ì„¸ì¼|í• ì¸)",
    "ì„¸íŠ¸"     : r"(ì„¸íŠ¸|êµ¬ì„±|íŠ¸ë¦¬ì˜¤|ë“€ì˜¤|íŒ¨í‚¤ì§€|í‚·\b|í‚¤íŠ¸\b)",
    "ê¸°íš"     : r"(ê¸°íš|ê¸°íšì „)",
    "1+1/ì¦ì •" : r"(1\+1|1\+2|ë¤|ì¦ì •|ì‚¬ì€í’ˆ)",
    "í•œì •/NEW" : r"(í•œì •|ë¦¬ë¯¸í‹°ë“œ|NEW|ë‰´\b)",
    "ì¿ í°/ë”œ"  : r"(ì¿ í°|ë”œ\b|ë”œê°€|í”„ë¡œëª¨ì…˜|í”„ë¡œëª¨\b)",
}
PAT_MARKETING = {k: re.compile(v, re.I) for k, v in PAT_MARKETING.items()}

DEFAULT_INGRS = [
    "íˆì•Œë£¨ë¡ ì‚°","ì„¸ë¼ë§ˆì´ë“œ","ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ","ë ˆí‹°ë†€","í©íƒ€ì´ë“œ","ì½œë¼ê²",
    "ë¹„íƒ€ë¯¼C","BHA","AHA","PHA","íŒí…Œë†€","ì„¼í…”ë¼","ë§ˆë°ì¹´ì†Œì‚¬ì´ë“œ",
]

def load_ingredients_from_file() -> List[str]:
    path = os.path.join("configs", "ingredients.txt")
    if not os.path.exists(path):
        return DEFAULT_INGRS[:]
    words = []
    with open(path, "r", encoding="utf-8") as f:
        for ln in f:
            ln = ln.strip()
            if not ln or ln.startswith("#"):
                continue
            words.append(ln)
    return words or DEFAULT_INGRS[:]

INGR_WORDS = load_ingredients_from_file()

def parse_marketing_and_infl(raw_name: str):
    name = raw_name or ""
    mk = {k: bool(p.search(name)) for k, p in PAT_MARKETING.items()}
    infl = None
    m = RE_INFL_PK.search(name)
    if m:
        cand = re.sub(r"[\[\](),.|Â·]", "", m.group(1)).strip()
        if cand and cand not in EXCLUDE_INFL and not RE_OY_PICK.search(name):
            infl = cand
    return mk, infl

def extract_ingredients(raw_name: str, ingr_list=None):
    name = raw_name or ""
    ingr_list = ingr_list or INGR_WORDS
    out = []
    for w in ingr_list:
        if re.search(re.escape(w), name, re.I):
            out.append(w)
    return out

# ------------------------------- ìœ í‹¸ -------------------------------

def find_existing_col(cols: List[str], cands: List[str]) -> Optional[str]:
    for c in cands:
        if c in cols:
            return c
    # ëŒ€ì†Œë¬¸ì/ê³µë°±/í•œì˜ í˜¼ìš© ë³´ì •
    lowered = {c.lower(): c for c in cols}
    for c in cands:
        key = c.lower()
        if key in lowered:
            return lowered[key]
    return None

def parse_query_param(url: str, key: str) -> Optional[str]:
    if not url:
        return None
    try:
        # ë¹ ë¥¸ ì •ê·œì‹ íŒŒì„œ
        m = re.search(r"[?&]" + re.escape(key) + r"=([^&#]+)", url)
        if m:
            return m.group(1)
    except Exception:
        pass
    return None

def extract_sku(row: Dict, src: str, url_col: Optional[str]) -> str:
    # 1) ëª…ì‹œ í•„ë“œ ìš°ì„ 
    for k in SKU_KEY_CANDIDATES:
        if k in row and pd.notna(row[k]) and str(row[k]).strip():
            return str(row[k]).strip()
    url = str(row.get(url_col, "") or "")
    if src in ("oy_kor",):
        return parse_query_param(url, "goodsNo") or url
    if src in ("oy_global",):
        return parse_query_param(url, "productId") or url
    if src in ("amazon_us",):
        # ì•„ë§ˆì¡´ì€ asin í•„ë“œ ìˆê±°ë‚˜ URL pathì—ì„œ ì¶”ì¶œ
        asin = row.get("asin")
        if asin: return str(asin)
        m = re.search(r"/([A-Z0-9]{10})(?:[/?]|$)", url)
        return m.group(1) if m else url
    if src in ("qoo10_jp",):
        return parse_query_param(url, "product_code") or url
    if src in ("daiso_kr",):
        return parse_query_param(url, "pdNo") or url
    return url

def parse_date_from_filename(fn: str) -> Optional[date]:
    m = re.search(r"(\d{4}-\d{2}-\d{2})", fn)
    if not m:
        return None
    try:
        return datetime.strptime(m.group(1), "%Y-%m-%d").date()
    except Exception:
        return None

def last_complete_week(today: Optional[date] = None) -> Tuple[date, date]:
    """
    ìµœê·¼ ì™„ê²° ì£¼(ì›”~ì¼). ì˜¤ëŠ˜ì´ ì›”~ì¼ ì¤‘ ì–´ë””ë“  ìƒê´€ì—†ì´,
    ì§ì „ 'ì¼ìš”ì¼'ê¹Œì§€ì˜ í•œ ì£¼ë¥¼ ë°˜í™˜.
    """
    today = today or date.today()
    # ì›”=0 ... ì¼=6
    weekday = today.weekday()
    # ì§€ë‚œ ì¼ìš”ì¼
    last_sunday = today - timedelta(days=(weekday + 1))
    start = last_sunday - timedelta(days=6)  # ì›”ìš”ì¼
    end = last_sunday
    return start, end

def prev_week_range(start: date, end: date) -> Tuple[date, date]:
    delta = timedelta(days=7)
    return (start - delta, end - delta)

def within(d: date, start: date, end: date) -> bool:
    return start <= d <= end

def ensure_int(x) -> Optional[int]:
    try:
        v = int(float(x))
        return v
    except Exception:
        return None

# ---------------------- ë°ì´í„° ì ì¬ & ì „ì²˜ë¦¬ ----------------------

def guess_src_from_filename(fn: str) -> Optional[str]:
    for src, hints in FILENAME_HINTS.items():
        if any(h in fn for h in hints):
            return src
    return None

def load_daily_files_for_range(src: str, data_dir: str, start: date, end: date) -> List[str]:
    outs = []
    for fn in os.listdir(data_dir):
        full = os.path.join(data_dir, fn)
        if not os.path.isfile(full):
            continue
        d = parse_date_from_filename(fn)
        if not d or not within(d, start, end):
            continue
        guessed = guess_src_from_filename(fn)
        if guessed == src:
            outs.append(full)
    return sorted(outs)

def read_csv_any(path: str) -> pd.DataFrame:
    try:
        return pd.read_csv(path, encoding="utf-8")
    except Exception:
        try:
            return pd.read_csv(path, encoding="cp949")
        except Exception:
            return pd.read_csv(path, encoding="latin1")

def unify_columns(df: pd.DataFrame) -> pd.DataFrame:
    # ê¸°ë³¸ ì»¬ëŸ¼ ì¡´ì¬ ë³´ì •
    cols = list(df.columns)
    rank_col  = find_existing_col(cols, RANK_COL_CAND)
    brand_col = find_existing_col(cols, BRAND_COL_CAND)
    name_col  = find_existing_col(cols, NAME_COL_CAND)
    url_col   = find_existing_col(cols, URL_COL_CAND)

    # ì•ˆì „ ë³µì‚¬
    out = pd.DataFrame()
    if rank_col:  out["rank"] = df[rank_col]
    if brand_col: out["brand"] = df[brand_col]
    if name_col:  out["raw_name"] = df[name_col]
    if url_col:   out["url"] = df[url_col]

    # ìˆ«ì ë³€í™˜
    if "rank" in out.columns:
        out["rank"] = pd.to_numeric(out["rank"], errors="coerce")

    return out

def load_week_dataframe(src: str, data_dir: str, start: date, end: date, topn: int) -> pd.DataFrame:
    files = load_daily_files_for_range(src, data_dir, start, end)
    frames = []
    for path in files:
        d = parse_date_from_filename(os.path.basename(path))
        df = unify_columns(read_csv_any(path))
        if "rank" not in df.columns:
            continue
        df = df[df["rank"].notnull()]
        df = df.sort_values("rank").head(topn).copy()
        df["date"] = pd.to_datetime(d)
        df["date_str"] = df["date"].dt.strftime("%Y-%m-%d")
        frames.append(df)
    if not frames:
        return pd.DataFrame(columns=["rank","brand","raw_name","url","date","date_str"])
    out = pd.concat(frames, ignore_index=True)
    # ë¹ˆ brand/raw_name ì±„ì›€
    out["brand"] = out.get("brand", pd.Series(dtype=str)).fillna("").astype(str)
    out["raw_name"] = out.get("raw_name", pd.Series(dtype=str)).fillna("").astype(str)
    out["url"] = out.get("url", pd.Series(dtype=str)).fillna("").astype(str)
    return out

# --------------------------- ì£¼ê°„ ì§‘ê³„ ---------------------------

@dataclass
class ItemStat:
    sku: str
    raw_name: str
    brand: str
    days: int
    avg_rank: float
    min_rank: float
    first_rank: float

def build_week_stats(src: str, df: pd.DataFrame, topn: int) -> Tuple[pd.DataFrame, Dict[str, ItemStat]]:
    if df.empty:
        return df, {}

    # ì‹ë³„ì ìƒì„±
    url_col = "url"
    df["sku"] = df.apply(lambda r: extract_sku(r, src, url_col), axis=1)

    # ì£¼ê°„ item í†µê³„
    g = df.groupby("sku")
    stats: Dict[str, ItemStat] = {}
    for sku, sub in g:
        raw = sub["raw_name"].mode().iloc[0] if not sub["raw_name"].isna().all() else ""
        br  = sub["brand"].mode().iloc[0] if not sub["brand"].isna().all() else ""
        days = sub["date_str"].nunique()
        avg_rank = float(sub["rank"].mean())
        min_rank = float(sub["rank"].min())
        first_rank = float(sub.sort_values("date")["rank"].iloc[0])
        stats[sku] = ItemStat(sku, raw, br, days, avg_rank, min_rank, first_rank)
    return df, stats

def compare_prev_week(curr: Dict[str, ItemStat], prev: Dict[str, ItemStat]) -> Dict[str, Optional[float]]:
    """ì´ì „ ì£¼ í‰ê· ê³¼ì˜ ì°¨ì´ (prev_avg - curr_avg >0ì´ë©´ ê°œì„ )"""
    deltas: Dict[str, Optional[float]] = {}
    for sku, st in curr.items():
        if sku in prev:
            d = prev[sku].avg_rank - st.avg_rank
            deltas[sku] = d
        else:
            deltas[sku] = None  # NEW
    return deltas

def brand_daily_counts(df: pd.DataFrame) -> Dict[str, float]:
    """ì¼ë³„ ë¸Œëœë“œ ê°œìˆ˜ â†’ ì¼í‰ê· """
    if df.empty:
        return {}
    outs = []
    for d, sub in df.groupby("date_str"):
        counts = Counter([str(b) for b in sub["brand"].fillna("").tolist()])
        outs.append(counts)
    total = Counter()
    for c in outs:
        total.update(c)
    days = max(1, len(outs))
    avg = {k: round(v / days, 1) for k, v in total.items() if k.strip()}
    return dict(sorted(avg.items(), key=lambda x: (-x[1], x[0])))

def inout_daily_average(df: pd.DataFrame, src: str) -> float:
    """ì¼ì¼ êµì²´(IN=OUT) í‰ê·  ê°œìˆ˜"""
    if df.empty: return 0.0
    url_col = "url"
    df["sku"] = df.apply(lambda r: extract_sku(r, src, url_col), axis=1)
    days = sorted(df["date_str"].unique())
    if len(days) <= 1: return 0.0
    changes = []
    prev_set = set()
    for d in days:
        now_set = set(df[df["date_str"]==d]["sku"])
        if prev_set:
            changes.append(len(now_set - prev_set))  # IN == OUT
        prev_set = now_set
    return round(sum(changes)/len(changes), 1) if changes else 0.0

def hero_flash_lists(stats: Dict[str, ItemStat], prev_stats: Dict[str, ItemStat]) -> Tuple[List[str], List[str]]:
    """
    íˆì–´ë¡œ: 3ì¼ ì´ìƒ ìœ ì§€ & ì§€ë‚œì£¼ì—” ì—†ì—ˆìŒ
    ë°˜ì§: 2ì¼ ì´í•˜ ìœ ì§€
    """
    heroes, flashes = [], []
    for sku, st in stats.items():
        if st.days >= 3 and sku not in prev_stats:
            heroes.append(st.raw_name)
        if st.days <= 2:
            flashes.append(st.raw_name)
    return heroes[:10], flashes[:10]

def kw_summary(df: pd.DataFrame) -> Dict[str, any]:
    """ë§ˆì¼€íŒ…/ì¸í”Œ/ì„±ë¶„ ìš”ì•½ (ì£¼ê°„ ìœ ë‹ˆí¬ SKU ê¸°ì¤€)"""
    out = {
        "unique": 0,
        "marketing": defaultdict(int),
        "influencers": defaultdict(int),
        "ingredients": defaultdict(int),
    }
    if df.empty: return {"unique": 0, "marketing":{}, "influencers":{}, "ingredients":{}}

    url_col = "url"
    df["sku"] = df.apply(lambda r: extract_sku(r, "oy_kor", url_col), axis=1)  # src ë¬´ê´€: skuë§Œ í•„ìš”
    uniq = set()
    seen_mk = set()  # (sku, key) 1íšŒë§Œ ì¹´ìš´íŠ¸
    for _, r in df.iterrows():
        sku = r["sku"]
        raw = (r.get("raw_name") or "").strip()
        uniq.add(sku)
        mk, infl = parse_marketing_and_infl(raw)
        for k, v in mk.items():
            if v and (sku, k) not in seen_mk:
                out["marketing"][k] += 1
                seen_mk.add((sku,k))
        if infl:
            out["influencers"][infl] += 1
        for ing in extract_ingredients(raw, INGR_WORDS):
            out["ingredients"][ing] += 1
    out["unique"] = len(uniq)
    # ì •ë ¬
    out["marketing"]   = dict(sorted(out["marketing"].items(),   key=lambda x: (-x[1], x[0])))
    out["influencers"] = dict(sorted(out["influencers"].items(), key=lambda x: (-x[1], x[0])))
    out["ingredients"] = dict(sorted(out["ingredients"].items(), key=lambda x: (-x[1], x[0])))
    return out

# --------------------------- í¬ë§·íŒ…(Slack) ---------------------------

def arrow_from_delta(d: Optional[float]) -> str:
    if d is None: return "NEW"
    val = int(round(abs(d)))
    if val == 0: return "ìœ ì§€"
    return f"â†‘{val}" if d > 0 else f"â†“{val}"

def format_top10(stats: Dict[str, ItemStat], deltas: Dict[str, Optional[float]]) -> List[str]:
    # í‰ê·  ìˆœìœ„ ë‚®ìŒ(ì¢‹ìŒ) ìš°ì„ 
    items = sorted(stats.values(), key=lambda s: (s.avg_rank, s.min_rank))[:10]
    out = []
    for i, st in enumerate(items, 1):
        line = f"{i}. {st.raw_name} (ìœ ì§€ {st.days}ì¼ Â· í‰ê·  {st.avg_rank:.1f}ìœ„) {arrow_from_delta(deltas.get(st.sku))}"
        out.append(line)
    return out

def format_brand_lines(avg_counts: Dict[str, float], limit: int = 15) -> List[str]:
    lines = []
    for k, v in list(avg_counts.items())[:limit]:
        lines.append(f"{k} {v}ê°œ/ì¼")
    return lines

def format_kw_block(kw: Dict[str, any]) -> str:
    if kw.get("unique",0) == 0:
        return "ë°ì´í„° ì—†ìŒ"
    lines = []
    lines.append("ğŸ“Š *ì£¼ê°„ í‚¤ì›Œë“œ ë¶„ì„*")
    lines.append(f"- ìœ ë‹ˆí¬ SKU: {kw['unique']}ê°œ")
    if kw["marketing"]:
        lines.append("â€¢ *ë§ˆì¼€íŒ… í‚¤ì›Œë“œ*")
        for k, cnt in kw["marketing"].items():
            ratio = round(cnt * 100.0 / max(1, kw["unique"]), 1)
            lines.append(f"  - {k}: {cnt}ê°œ ({ratio}%)")
    if kw["influencers"]:
        lines.append("â€¢ *ì¸í”Œë£¨ì–¸ì„œ*")
        for k, cnt in list(kw["influencers"].items())[:20]:
            lines.append(f"  - {k}: {cnt}ê°œ")
    if kw["ingredients"]:
        lines.append("â€¢ *ì„±ë¶„ í‚¤ì›Œë“œ*")
        for k, cnt in list(kw["ingredients"].items())[:20]:
            lines.append(f"  - {k}: {cnt}ê°œ")
    return "\n".join(lines)

def build_slack_message(src: str,
                        range_str: str,
                        top10_lines: List[str],
                        brand_lines: List[str],
                        inout_avg: float,
                        heroes: List[str],
                        flashes: List[str],
                        kw_text: str) -> str:
    title = SRC_SPECS[src]["title"]
    lines = []
    lines.append(f"ğŸ“ˆ *ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title} ({range_str})*")
    lines.append("")
    # Top10
    lines.append("ğŸ† *Top10*")
    if top10_lines:
        lines += [f"{ln}" for ln in top10_lines]
    else:
        lines.append("ë°ì´í„° ì—†ìŒ")
    lines.append("")
    # ë¸Œëœë“œ
    lines.append("ğŸ“¦ *ë¸Œëœë“œ ê°œìˆ˜(ì¼í‰ê· )*")
    if brand_lines:
        lines += [f"{ln}" for ln in brand_lines]
    else:
        lines.append("ë°ì´í„° ì—†ìŒ")
    lines.append("")
    # ì¸ì•¤ì•„ì›ƒ
    lines.append("ğŸ” *ì¸ì•¤ì•„ì›ƒ(êµì²´)*")
    lines.append(f"- ì¼í‰ê·  {inout_avg}ê°œ")
    lines.append("")
    # ì‹ ê·œ/ë°˜ì§
    lines.append("ğŸ†• *ì‹ ê·œ íˆì–´ë¡œ(â‰¥3ì¼ ìœ ì§€)*")
    lines.append("ì—†ìŒ" if not heroes else "Â· " + " Â· ".join(heroes[:8]))
    lines.append("âœ¨ *ë°˜ì§ ì•„ì´í…œ(â‰¤2ì¼)*")
    lines.append("ì—†ìŒ" if not flashes else "Â· " + " Â· ".join(flashes[:8]))
    lines.append("")
    # í‚¤ì›Œë“œ ë¸”ë¡
    lines.append(kw_text)
    return "\n".join(lines)

# --------------------------- ë©”ì¸ íŒŒì´í”„ë¼ì¸ ---------------------------

def run_for_source(src: str, data_dir: str, min_days: int = 3) -> Dict[str, any]:
    spec = SRC_SPECS[src]
    topn = spec["topn"]
    # ì£¼ì°¨
    start, end = last_complete_week()
    prev_start, prev_end = prev_week_range(start, end)
    range_str = f"{start.strftime('%Y-%m-%d')}-{end.strftime('%Y-%m-%d')}"

    # ë°ì´í„° ì ì¬
    cur_df = load_week_dataframe(src, data_dir, start, end, topn)
    prev_df = load_week_dataframe(src, data_dir, prev_start, prev_end, topn)

    # í†µê³„
    cur_df, cur_stats = build_week_stats(src, cur_df, topn)
    _, prev_stats = build_week_stats(src, prev_df, topn)
    deltas = compare_prev_week(cur_stats, prev_stats)

    # Top10
    top10_lines = format_top10(cur_stats, deltas)

    # ë¸Œëœë“œ ì¼í‰ê· 
    brand_avg = brand_daily_counts(cur_df)
    brand_lines = format_brand_lines(brand_avg)

    # ì¸ì•¤ì•„ì›ƒ
    inout_avg = inout_daily_average(cur_df, src)

    # íˆì–´ë¡œ/ë°˜ì§
    heroes, flashes = hero_flash_lists(cur_stats, prev_stats)

    # í‚¤ì›Œë“œ ë¶„ì„(ì£¼ê°„ ì „ì²´ df ê¸°ì¤€)
    kw = kw_summary(cur_df)
    kw_text = format_kw_block(kw)

    # ìŠ¬ë™ ë©”ì‹œì§€
    slack_text = build_slack_message(
        src, range_str, top10_lines, brand_lines, inout_avg, heroes, flashes, kw_text
    )

    # íŒŒì¼ ì €ì¥
    with open(f"slack_{src}.txt", "w", encoding="utf-8") as f:
        f.write(slack_text)

    summary = {
        "range": range_str,
        "top10_lines": top10_lines or ["ë°ì´í„° ì—†ìŒ"],
        "brand_lines": brand_lines or ["ë°ì´í„° ì—†ìŒ"],
        "inout_avg": inout_avg,
        "heroes": heroes,
        "flashes": flashes,
        "kw": kw,  # ì›ìë£Œ(ë¹„ìœ¨ì€ uniqueë¡œ ê³„ì‚°)
    }
    with open(f"weekly_summary_{src}.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    return summary

# --------------------------- CLI ---------------------------

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--src", choices=list(SRC_SPECS.keys()) + ["all"], required=True)
    p.add_argument("--data-dir", default="./data/daily")
    p.add_argument("--min-days", type=int, default=3, help="íˆì–´ë¡œ íŒì • ìµœì†Œ ìœ ì§€ì¼(ê¸°ë³¸ 3)")
    p.add_argument("--split", action="store_true", help="(í˜¸í™˜ìš©) ì˜ë¯¸ ì—†ìŒ")
    args = p.parse_args()

    if args.src == "all":
        results = {}
        for s in SRC_SPECS.keys():
            print(f"[run] {s}")
            results[s] = run_for_source(s, args.data_dir, args.min_days)
        print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        res = run_for_source(args.src, args.data_dir, args.min_days)
        print(json.dumps(res, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
