# scripts/weekly_report_plus.py
# -*- coding: utf-8 -*-

"""
ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± (SKU í‘œì¤€í™” + ë‚ ì§œ ë¹„êµ ë²„ê·¸ Fix)
- ì‚¬ìš© ì˜ˆ: python scripts/weekly_report_plus.py --src all --data-dir ./data/daily
- ì¶œë ¥:
  - slack_oy_kor.txt, slack_oy_global.txt, slack_amazon_us.txt, slack_qoo10_jp.txt, slack_daiso_kr.txt
  - weekly_summary_oy_kor.json, ... (ì†ŒìŠ¤ë³„)
"""

from __future__ import annotations
import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
from urllib.parse import urlparse, parse_qs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = ["oy_kor", "oy_global", "amazon_us", "qoo10_jp", "daiso_kr"]

# íŒŒì¼ëª… íŒ¨í„´ (ì¼ì YYYY-MM-DD íŒŒì‹±ìš©)
FILENAME_PATTERNS = {
    "oy_kor":       r"^ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "oy_global":    r"^ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "amazon_us":    r"^ì•„ë§ˆì¡´US_ë·°í‹°_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "qoo10_jp":     r"^íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹_(\d{4}-\d{2}-\d{2})\.csv$",
    "daiso_kr":     r"^ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„_(\d{4}-\d{2}-\d{2})\.csv$",
}

# ì†ŒìŠ¤ë³„ Slack/JSON ì œëª©
TITLES = {
    "oy_kor":    "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",
    "oy_global": "ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",
    "amazon_us": "ì•„ë§ˆì¡´ US Top100",
    "qoo10_jp":  "íí… ì¬íŒ¬ ë·°í‹° Top200",
    "daiso_kr":  "ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200",
}

TOPN = {
    "oy_kor": 100,
    "oy_global": 100,
    "amazon_us": 100,
    "qoo10_jp": 200,
    "daiso_kr": 200,
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def this_week_range(today: Optional[datetime] = None) -> tuple[datetime, datetime]:
    """
    ì´ë²ˆ ì£¼ ì›”ìš”ì¼~ì¼ìš”ì¼ ë²”ìœ„(UTC ê¸°ì¤€ ë‹¨ìˆœ ê³„ì‚°).
    """
    if today is None:
        today = datetime.utcnow()
    monday = today - timedelta(days=today.weekday())  # ì›”ìš”ì¼
    monday = datetime(monday.year, monday.month, monday.day)
    sunday = monday + timedelta(days=6)
    return monday, sunday


def parse_date_from_filename(name: str, src: str) -> Optional[datetime]:
    pat = FILENAME_PATTERNS.get(src)
    if not pat:
        return None
    m = re.match(pat, name)
    if not m:
        return None
    try:
        return datetime.strptime(m.group(1), "%Y-%m-%d")
    except Exception:
        return None


def read_csv_safe(path: Path) -> Optional[pd.DataFrame]:
    try:
        return pd.read_csv(path, encoding="utf-8-sig", low_memory=False)
    except Exception:
        try:
            return pd.read_csv(path, encoding="utf-8", low_memory=False)
        except Exception:
            return None


def rename_common_columns(df: pd.DataFrame, src: str) -> pd.DataFrame:
    """
    ì»¬ëŸ¼ ê³µí†µí™”: date, rank, product_name, brand, url ìµœì†Œ ë³´ì •
    """
    df = df.copy()

    # name -> product_name
    if "product_name" not in df.columns and "name" in df.columns:
        df = df.rename(columns={"name": "product_name"})

    # rank í˜•ì‹ ë³´ì •
    if "rank" in df.columns:
        df["rank"] = pd.to_numeric(df["rank"], errors="coerce")

    # date í˜•ì‹(ë¬¸ìâ†’datetime) ë³´ì •
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
    else:
        df["date"] = pd.NaT

    # url ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
    if "url" not in df.columns:
        df["url"] = None

    return df


def extract_query_param(url: str, key: str) -> Optional[str]:
    if not isinstance(url, str) or not url:
        return None
    try:
        q = parse_qs(urlparse(url).query)
        v = q.get(key)
        return v[0] if v else None
    except Exception:
        return None


def ensure_sku_column(df: pd.DataFrame, src: str) -> pd.DataFrame:
    """
    ì†ŒìŠ¤ë³„ ê³µí†µ sku ìƒì„±:
      oy_kor    -> url?goodsNo
      oy_global -> url?productId
      amazon_us -> asin
      qoo10_jp  -> product_code
      daiso_kr  -> url?pdNo
    """
    if df is None or len(df) == 0:
        return df

    df = df.copy()

    if src == "oy_kor":
        if "sku" not in df.columns:
            df["sku"] = df["url"].apply(lambda u: extract_query_param(u, "goodsNo"))
    elif src == "oy_global":
        if "sku" not in df.columns:
            df["sku"] = df["url"].apply(lambda u: extract_query_param(u, "productId"))
    elif src == "amazon_us":
        if "sku" not in df.columns:
            df["sku"] = df.get("asin")
    elif src == "qoo10_jp":
        if "sku" not in df.columns:
            df["sku"] = df.get("product_code")
    elif src == "daiso_kr":
        if "sku" not in df.columns:
            df["sku"] = df["url"].apply(lambda u: extract_query_param(u, "pdNo"))

    # ë¬¸ìì—´ ì •ë¦¬
    if "sku" in df.columns:
        df["sku"] = df["sku"].astype(str).str.strip()
        df["sku"] = df["sku"].replace({"": None, "None": None})
    else:
        df["sku"] = df.get("url")

    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œë”©/í•„í„°ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def list_source_files(data_dir: Path, src: str, start: datetime, end: datetime) -> List[Path]:
    files: List[Path] = []
    for p in sorted(data_dir.glob("*.csv")):
        d = parse_date_from_filename(p.name, src)
        if d is None:
            continue
        if start.date() <= d.date() <= end.date():
            files.append(p)
    return files


def load_week_df(src: str, data_dir: Path, start: datetime, end: datetime) -> pd.DataFrame:
    files = list_source_files(data_dir, src, start, end)
    frames: List[pd.DataFrame] = []
    for f in files:
        df = read_csv_safe(f)
        if df is None or len(df) == 0:
            continue
        df = rename_common_columns(df, src)

        # íŒŒì¼ëª… ì¼ìë¥¼ date ê²°ì¸¡ì— ì±„ì›Œë„£ê¸°
        if df["date"].isna().all():
            d = parse_date_from_filename(f.name, src)
            if d:
                df["date"] = pd.to_datetime(d)  # datetime64[ns]

        df = ensure_sku_column(df, src)

        # ìˆœìœ„ë§Œ ë‚¨ê¸°ê¸°
        if "rank" in df.columns:
            df = df[df["rank"].notna()]
        frames.append(df)

    if not frames:
        return pd.DataFrame()

    out = pd.concat(frames, ignore_index=True)

    # âœ… ë‚ ì§œ ë¹„êµ ë²„ê·¸ FIX: ì–‘ìª½ ëª¨ë‘ Timestampë¡œ ë§ì¶°ì„œ ë¹„êµ
    col_dt = pd.to_datetime(out["date"], errors="coerce").dt.normalize()
    start_ts = pd.Timestamp(start.date())
    end_ts   = pd.Timestamp(end.date())
    mask = (col_dt >= start_ts) & (col_dt <= end_ts)
    out = out[mask]

    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í†µê³„/ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class WeeklyStats:
    unique_cnt: int
    keep_days_mean: float
    topn_items: List[Dict]  # [{sku, name, brand, avg_rank, latest_rank, days}, ...]


def calc_weekly_stats(df: pd.DataFrame, src: str) -> WeeklyStats:
    if df is None or len(df) == 0:
        return WeeklyStats(unique_cnt=0, keep_days_mean=0.0, topn_items=[])

    # í•„ìˆ˜ ì»¬ëŸ¼ ë³´ì •
    if "product_name" not in df.columns:
        df["product_name"] = None
    if "brand" not in df.columns:
        df["brand"] = None

    # sku ê¸°ì¤€ìœ¼ë¡œ ì¼ìˆ˜/í‰ê· ìˆœìœ„ ê³„ì‚°
    g = df.groupby("sku", dropna=True)
    # ë‚ ì§œê°€ datetime64[ns]ë¼ê³  ê°€ì •
    days = g["date"].nunique().rename("days")
    avg_rank = g["rank"].mean().rename("avg_rank")

    # ìµœì‹  ìŠ¤ëƒ…ìƒ·
    df["__dt"] = pd.to_datetime(df["date"], errors="coerce")
    latest_idx = df.sort_values(["sku", "__dt"], ascending=[True, False]).groupby("sku").head(1).set_index("sku")
    latest_name = latest_idx["product_name"].rename("latest_name")
    latest_brand = latest_idx["brand"].rename("latest_brand")
    latest_rank = latest_idx["rank"].rename("latest_rank")

    stat_df = pd.concat([days, avg_rank, latest_name, latest_brand, latest_rank], axis=1).reset_index()
    stat_df = stat_df[stat_df["sku"].notna()]

    # ìƒìœ„ 10ê°œ (í‰ê· ìˆœìœ„ ì˜¤ë¦„ì°¨ìˆœ)
    stat_df = stat_df.sort_values(["avg_rank", "latest_rank"], ascending=[True, True]).head(10)

    items = []
    for _, row in stat_df.iterrows():
        items.append({
            "sku": row.get("sku"),
            "name": row.get("latest_name"),
            "brand": row.get("latest_brand"),
            "avg_rank": float(row.get("avg_rank")) if pd.notna(row.get("avg_rank")) else None,
            "latest_rank": int(row.get("latest_rank")) if pd.notna(row.get("latest_rank")) else None,
            "days": int(row.get("days")) if pd.notna(row.get("days")) else 0,
        })

    unique_cnt = df["sku"].nunique(dropna=True)
    keep_days_mean = float(days.mean()) if not days.empty else 0.0

    return WeeklyStats(unique_cnt=unique_cnt, keep_days_mean=keep_days_mean, topn_items=items)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶œë ¥ (Slack í…ìŠ¤íŠ¸ / JSON)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_top10_lines(stats: WeeklyStats) -> List[str]:
    lines: List[str] = []
    if not stats.topn_items:
        lines.append("ë°ì´í„° ì—†ìŒ")
        return lines

    for i, it in enumerate(stats.topn_items, start=1):
        name = it.get("name") or "-"
        avg_rank = it.get("avg_rank")
        days = it.get("days", 0)
        s = f"{i}. {name} (ìœ ì§€ {days}ì¼ Â· í‰ê·  {avg_rank:.1f}ìœ„)" if avg_rank else f"{i}. {name}"
        lines.append(s)
    return lines


def build_slack_text(src: str, start: datetime, end: datetime, stats: WeeklyStats) -> str:
    title = TITLES.get(src, src)
    period = f"{start.date()}~{end.date()}"
    top10_lines = format_top10_lines(stats)

    body = []
    body.append(f"ğŸ“ˆ ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title} ({period})")
    body.append("ğŸ† Top10")
    for ln in top10_lines:
        body.append(ln)
    body.append("")
    body.append("ğŸ“¦ í†µê³„")
    body.append(f"- Top{TOPN.get(src,100)} ë“±ê·¹ SKU : {stats.unique_cnt}ê°œ")
    body.append(f"- Top {TOPN.get(src,100)} ìœ ì§€ í‰ê·  : {stats.keep_days_mean:.1f}ì¼")
    return "\n".join(body)


def save_text(path: Path, text: str):
    path.write_text(text, encoding="utf-8")


def save_json(path: Path, obj: dict):
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ë³¸ì²´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_for_source(src: str, data_dir: Path, start: datetime, end: datetime):
    print(f"[run] {src}")
    cur_df = load_week_df(src, data_dir, start, end)
    stats = calc_weekly_stats(cur_df, src)

    # Slack í…ìŠ¤íŠ¸
    slack_text = build_slack_text(src, start, end, stats)
    save_text(Path(f"slack_{src}.txt"), slack_text)

    # ìš”ì•½ JSON
    summary = {
        "title": TITLES.get(src, src),
        "range": f"{start.date()}~{end.date()}",
        "topn": TOPN.get(src, 100),
        "top10_items": stats.topn_items,
        "unique_cnt": stats.unique_cnt,
        "keep_days_mean": round(stats.keep_days_mean, 2),
    }
    save_json(Path(f"weekly_summary_{src}.json"), summary)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--src", default="all", help="oy_kor,oy_global,amazon_us,qoo10_jp,daiso_kr,all")
    ap.add_argument("--data-dir", default="./data/daily")
    args = ap.parse_args()

    if args.src == "all":
        targets = SOURCES
    else:
        targets = [s.strip() for s in args.src.split(",") if s.strip() in SOURCES]
        if not targets:
            print("ìœ íš¨í•œ --src ê°€ ì•„ë‹™ë‹ˆë‹¤. ê°€ëŠ¥: ", SOURCES, file=sys.stderr)
            sys.exit(1)

    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        print(f"[ERR] DATA_DIR ì—†ìŒ: {data_dir}", file=sys.stderr)
        sys.exit(1)

    start, end = this_week_range()
    print(f"[scan] ê¸°ê°„ {start.date()} ~ {end.date()} | dir={data_dir}")

    for s in targets:
        run_for_source(s, data_dir, start, end)


if __name__ == "__main__":
    main()
