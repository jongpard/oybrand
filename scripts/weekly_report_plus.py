# scripts/weekly_report_plus.py
# -*- coding: utf-8 -*-
import os, re, glob, json, math, argparse
import pandas as pd
from collections import Counter
from datetime import timedelta, timezone

KST = timezone(timedelta(hours=9))

# --------- ì„¤ì • ---------
SRC_INFO = {
    'oy_kor':    {'topn':100, 'hints':['ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹','oliveyoung_kor','oy_kor'], 'currency':'KRW'},
    'oy_global': {'topn':100, 'hints':['ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ','oliveyoung_global','oy_global'], 'currency':'KRW'},
    'amazon_us': {'topn':100, 'hints':['ì•„ë§ˆì¡´US','amazon_us','amazonUS'],       'currency':'USD'},
    'qoo10_jp':  {'topn':200, 'hints':['íí…ì¬íŒ¬','qoo10','Qoo10'],               'currency':'JPY'},
    'daiso_kr':  {'topn':200, 'hints':['ë‹¤ì´ì†Œëª°','daiso'],                       'currency':'KRW'},
}
ALL_SRCS = list(SRC_INFO.keys())

# ë°˜ì§ ê¸°ì¤€(ìµœëŒ€ ìœ ì§€ì¼ìˆ˜)
FLASH_MAX_DAYS = int(os.getenv("FLASH_MAX_DAYS", "2"))

# ì»¬ëŸ¼ í›„ë³´
COLS = {
    'rank':        ['rank','ìˆœìœ„','ranking','ë­í‚¹'],
    'raw_name':    ['raw_name','raw','rawProduct','rawTitle'],
    'product':     ['product','ì œí’ˆëª…','ìƒí’ˆëª…','name','title','goods_name','goodsNm','prdNm','prdtName','displayName','itemNm','ìƒí’ˆ','item_name','item'],
    'brand':       ['brand','ë¸Œëœë“œ','brand_name','brandNm','ë¸Œëœë“œëª…'],
    'url':         ['url','ë§í¬','product_url','link','detail_url'],
    'date':        ['date','ë‚ ì§œ','ìˆ˜ì§‘ì¼','crawl_date','created_at'],
    'price':       ['price','ê°€ê²©','sale_price','selling_price'],
    'orig_price':  ['orig_price','ì •ê°€','original_price','ì†Œë¹„ìê°€','list_price'],
    'discount':    ['discount_rate','í• ì¸ìœ¨','discount','discountPercent'],
    'goodsNo':     ['goodsNo','goods_no','goodsno','ìƒí’ˆë²ˆí˜¸','ìƒí’ˆì½”ë“œ'],
    'productId':   ['productId','product_id','prdtNo','ìƒí’ˆID','ìƒí’ˆì•„ì´ë””','ìƒí’ˆì½”ë“œ'],
    'asin':        ['asin','ASIN'],
    'product_code':['product_code','productCode','ìƒí’ˆì½”ë“œ','item_code'],
    'pdNo':        ['pdNo','pdno','ìƒí’ˆë²ˆí˜¸','ìƒí’ˆì½”ë“œ'],
}

# í‚¤ì›Œë“œ ë£°
KW_PRODUCT = {
    'íŒ¨ë“œ': r'(íŒ¨ë“œ|pad)',
    'ë§ˆìŠ¤í¬íŒ©': r'(ë§ˆìŠ¤í¬íŒ©|ë§ˆìŠ¤í¬|sheet\s*mask|mask\s*pack)',
    'ì•°í”Œ/ì„¸ëŸ¼': r'(ì•°í”Œ|ì„¸ëŸ¼|ampoule|serum)',
    'í† ë„ˆ/ìŠ¤í‚¨': r'(í† ë„ˆ|ìŠ¤í‚¨|toner)',
    'í¬ë¦¼': r'(í¬ë¦¼|cream|moisturizer)',
    'í´ë Œì €': r'(í´ë Œì§•|í´ë Œì €|cleanser|ì›Œì‹œ|wash)',
    'ì„ ì¼€ì–´': r'(ì„ í¬ë¦¼|sunscreen|sun\s*cream|uv)',
    'ë¦½': r'(ë¦½|í‹´íŠ¸|lip|tint|balm)',
}
KW_EFFICACY = {
    'ì§„ì •': r'(ì§„ì •|soothing|calming)',
    'ë³´ìŠµ': r'(ë³´ìŠµ|ìˆ˜ë¶„|hydration|moistur)',
    'ë¯¸ë°±/í†¤ì—…': r'(ë¯¸ë°±|í†¤ì—…|whiten|brighten)',
    'íƒ„ë ¥/ë¦¬í”„íŒ…': r'(íƒ„ë ¥|ë¦¬í”„íŒ…|firming|lifting|elastic)',
    'íŠ¸ëŸ¬ë¸”/ì—¬ë“œë¦„': r'(íŠ¸ëŸ¬ë¸”|ì—¬ë“œë¦„|acne|blemish)',
    'ëª¨ê³µ': r'(ëª¨ê³µ|pore)',
    'ê°ì§ˆ/í•„ë§': r'(ê°ì§ˆ|í•„ë§|peel|AHA|BHA|PHA)',
    'ì£¼ë¦„': r'(ì£¼ë¦„|wrinkle|anti[-\s]?aging)',
}
KW_MARKETING = {
    'ê¸°íš/ì„¸íŠ¸': r'(ê¸°íš|ì„¸íŠ¸|set|kit|bundle)',
    '1+1/ì¦ì •': r'(1\+1|2\+1|ì¦ì •|ì¦ëŸ‰|ë¤)',
    'í•œì •/NEW': r'(í•œì •|ë¦¬ë¯¸í‹°ë“œ|limited|NEW|new\b|ì‹ ìƒ)',
    'ì¿ í°/ë”œ': r'(ì¿ í°|coupon|ë”œ|deal|íŠ¹ê°€|sale|ì„¸ì¼|event|í”„ë¡œëª¨ì…˜|promotion)',
}
# ì„±ë¶„ í‚¤ì›Œë“œ
KW_INGREDIENT = {
    'íˆì•Œë£¨ë¡ ì‚°': r'(íˆì•Œë£¨ë¡ |hyaluronic|HA\b)',
    'ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œ': r'(ë‹ˆì•„ì‹ |niacinamide|vitamin\s*B3)',
    'ë¹„íƒ€ë¯¼C': r'(ë¹„íƒ€ë¯¼\s*C|ascorbic\s*acid|AA\b|VC\b)',
    'ì„¼í…”ë¼/ì‹œì¹´': r'(ì„¼í…”ë¼|ë³‘í’€|cica|madecassoside|asiaticoside)',
    'ì„¸ë¼ë§ˆì´ë“œ': r'(ì„¸ë¼ë§ˆì´ë“œ|ceramide)',
    'ì½œë¼ê²': r'(ì½œë¼ê²|collagen)',
    'í©íƒ€ì´ë“œ': r'(í©íƒ€ì´ë“œ|peptide)',
    'ë ˆí‹°ë†€/ë ˆí‹°ë‚ ': r'(ë ˆí‹°ë†€|ë ˆí‹°ë‚ |retinol|retinal)',
    'íŒí…Œë†€': r'(íŒí…Œë†€|panthenol)',
    'PDRN': r'(\bPDRN\b)',
    'ì‚´ë¦¬ì‹¤ì‚°/BHA': r'(ì‚´ë¦¬ì‹¤|salicylic|BHA\b)',
    'AHA/PHA': r'(\bAHA\b|\bPHA\b)',
    'ì§•í¬ì˜¥ì‚¬ì´ë“œ': r'(ì§•í¬\s*ì˜¥ì‚¬ì´ë“œ|zinc\s*oxide)',
}

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘(ê°„ë‹¨ ë£°)
CATEGORY_RULES = [
    ("ë§ˆìŠ¤í¬íŒ©", r"(ë§ˆìŠ¤í¬íŒ©|íŒ©|sheet\s*mask|mask\s*pack)"),
    ("ì„ ì¼€ì–´", r"(ì„ í¬ë¦¼|ìì™¸ì„ |sun\s*cream|sunscreen|uv)"),
    ("í´ë Œì €", r"(í´ë Œì§•|í´ë Œì €|foam|cleanser|wash)"),
    ("í† ë„ˆ/ìŠ¤í‚¨", r"(í† ë„ˆ|ìŠ¤í‚¨|toner)"),
    ("ì—ì„¼ìŠ¤/ì„¸ëŸ¼", r"(ì—ì„¼ìŠ¤|ì„¸ëŸ¼|ì•°í”Œ|serum|essence|ampoule)"),
    ("ë¡œì…˜/ì—ë©€ì „", r"(ë¡œì…˜|ì—ë©€ì „|lotion|emulsion)"),
    ("í¬ë¦¼", r"(í¬ë¦¼|cream|moisturizer)"),
    ("ë¦½", r"(ë¦½|í‹´íŠ¸|ë¦½ë°¤|lip|tint|balm)"),
    ("ì•„ì´", r"(ì•„ì´í¬ë¦¼|eye\s*cream|ì•„ì´|ì•„ì´íŒ¨ì¹˜)"),
    ("í—¤ì–´", r"(ìƒ´í‘¸|íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸|í—¤ì–´|hair)"),
    ("ë°”ë””", r"(ë°”ë””|body|ë°”ìŠ¤)"),
    ("í–¥ìˆ˜", r"(í–¥ìˆ˜|í¼í“¸|eau|parfum|perfume)"),
    ("ë„êµ¬/ê¸°ê¸°", r"(ê¸°ê¸°|ë””ë°”ì´ìŠ¤|ë¡¤ëŸ¬|device|tool)"),
]

# --------- ìœ í‹¸ ---------
def pick(df, names):
    for c in names:
        if c in df.columns: return c
    return None

def pick_loose_product(df):
    rn = pick(df, COLS['raw_name'])
    if rn: return rn
    pn = pick(df, COLS['product'])
    if pn: return pn
    patt = re.compile(r"(product|name|title|ìƒí’ˆ|ì œí’ˆ|í’ˆëª…|ì•„ì´í…œ)", re.IGNORECASE)
    for c in df.columns:
        if patt.search(str(c)): return c
    return None

def infer_source(path:str):
    base = os.path.basename(path).lower()
    parent = os.path.basename(os.path.dirname(path)).lower()
    for src, info in SRC_INFO.items():
        for h in info['hints']:
            if h.lower() in base or h.lower() in parent:
                return src
    return None

def infer_date_from_filename(fn:str):
    m = re.search(r'(\d{4}-\d{2}-\d{2})', os.path.basename(fn))
    return pd.to_datetime(m.group(1)) if m else pd.NaT

def read_csv_any(path:str)->pd.DataFrame:
    for enc in ('utf-8-sig','cp949','utf-8','euc-kr'):
        try: return pd.read_csv(path, encoding=enc)
        except Exception: pass
    return pd.read_csv(path)

def extract_key(src:str, row, url:str|None):
    u = str(url or "")
    if src == 'oy_kor':
        for c in COLS['goodsNo']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'goodsNo=([0-9A-Za-z\-]+)', u);  return m.group(1) if m else None
    if src == 'oy_global':
        for c in COLS['productId']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'(?:productId|prdtNo)=([0-9A-Za-z\-]+)', u); return m.group(1) if m else None
    if src == 'amazon_us':
        for c in COLS['asin']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip().upper()
        m = re.search(r'/([A-Z0-9]{10})(?:[/?#]|$)', u.upper()); return m.group(1) if m else None
    if src == 'qoo10_jp':
        for c in COLS['product_code']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'product_code=([0-9A-Za-z\-]+)', u)
        if m: return m.group(1)
        m2 = re.search(r'/(\d{6,})', u)
        return m2.group(1) if m2 else None
    if src == 'daiso_kr':
        for c in COLS['pdNo']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'pdNo=([0-9A-Za-z\-]+)', u); return m.group(1) if m else None
    return None

def fmt_money(v, src):
    if v is None or (isinstance(v,float) and (pd.isna(v) or math.isnan(v))): return None
    cur = SRC_INFO[src]['currency']
    if cur == 'USD':  return f"${v:,.0f}"
    if cur == 'JPY':  return f"Â¥{v:,.0f}"
    return f"â‚©{v:,.0f}"

# í”„ë¡œëª¨ì…˜/ì¸í”Œë£¨ì–¸ì„œ
PROMO_RE = re.compile(
    r"(ì˜¬ì˜í”½|íŠ¹ê°€|ê¸°íš|ì¦ì •|ì„¸ì¼|sale|event|í–‰ì‚¬|í•œì •|ë¦¬ë¯¸í‹°ë“œ|1\+1|2\+1|ë”ë¸”\s*ê¸°íš|ì¦ëŸ‰|ì¿ í°|coupon|deal|ë”œ|gift|bundle|promotion|NEW\b|ì‹ ìƒ)",
    re.IGNORECASE
)
INF_BLACK = {'ì˜¬ì˜','ì›”ì˜¬ì˜','ì›ë”','MD','ì—ë””í„°','ë¸Œëœë“œ','editor','brand','oliveyoung','ìŠ¤í† ì–´','ê³µì‹','í”½'}

def is_promo(name:str)->bool:
    n = (name or "")
    if re.search(r'í”½\b', n):  # í•œê¸€ 'í”½'ì€ ëª¨ë‘ í”„ë¡œëª¨ì…˜ ì·¨ê¸‰
        return True
    return bool(PROMO_RE.search(n))

def extract_influencers_dynamic(name:str):
    """ì˜ì–´ pick/PICK ì• ë‹¨ì–´ or Ã—/with/ì½œë¼ë³´ íŒ¨í„´ë§Œ ì¸í”Œë¡œ ì¸ì‹"""
    if not name: return set()
    t = str(name)
    out = set()
    # ì˜ì–´ pick ì• ë‹¨ì–´
    for m in re.finditer(r'([ê°€-í£A-Za-z]{2,20})\s*(?:pick|PICK)\b', t):
        nm = m.group(1).strip()
        if nm and nm not in INF_BLACK and nm.lower() not in {x.lower() for x in INF_BLACK}:
            out.add(nm)
    # Ã— / with / ì½œë¼ë³´
    for m in re.finditer(r'([ê°€-í£A-Za-z]{2,20})\s*(?:Ã—|x|X|with|ì½œë¼ë³´|collab(?:oration)?)\s*([ê°€-í£A-Za-z]{2,20})', t, re.IGNORECASE):
        for nm in (m.group(1), m.group(2)):
            nm = nm.strip()
            if nm and nm not in INF_BLACK and nm.lower() not in {x.lower() for x in INF_BLACK}:
                if not re.search(r'\d|ml|g|pack|set|ê¸°íš|íŠ¹ê°€|ì„¸íŠ¸', nm, re.IGNORECASE):
                    out.add(nm)
    return out

def daily_topn_base(df, topn):
    d = (df.sort_values(['date','rank'])
           .drop_duplicates(['date','key'])
           .groupby(df['date'].dt.date, group_keys=False)
           .apply(lambda x: x.nsmallest(topn, 'rank')))
    d['day'] = d['date'].dt.date
    return d

def week_range_for_source(ud:pd.DataFrame, src:str):
    dts = ud.loc[ud['source'].eq(src), 'date']
    if dts.empty: return None
    last = pd.to_datetime(dts.max())
    end = last + pd.Timedelta(days=(6 - last.weekday()))  # ê·¸ ì£¼ ì¼ìš”ì¼
    start = end - pd.Timedelta(days=6)                    # ê·¸ ì£¼ ì›”ìš”ì¼
    return start.normalize(), end.normalize()

def _arrow_rank(diff_rank: float) -> str:
    if diff_rank is None or (isinstance(diff_rank,float) and math.isnan(diff_rank)): return "â€”"
    d = int(round(diff_rank))
    if d > 0:  return f"â†‘{d}"
    if d < 0:  return f"â†“{abs(d)}"
    return "â€”"

# --------- ë¡œë”© ---------
def load_unified(data_dir:str)->pd.DataFrame:
    paths = glob.glob(os.path.join(data_dir,'**','*.csv'), recursive=True)
    rows=[]
    for p in paths:
        src = infer_source(p)
        if not src: continue
        try:
            df = read_csv_any(p)
        except Exception:
            continue

        date_col = pick(df, COLS['date'])
        dates = pd.to_datetime(df[date_col], errors='coerce') if date_col else pd.Series([infer_date_from_filename(p)]*len(df))
        rank_col = pick(df, COLS['rank'])
        if not rank_col: continue

        prod_col = pick_loose_product(df)
        brand_col = pick(df, COLS['brand'])
        url_col   = pick(df, COLS['url'])
        price_col = pick(df, COLS['price'])
        orig_col  = pick(df, COLS['orig_price'])
        disc_col  = pick(df, COLS['discount'])

        for i,r in df.iterrows():
            nm = str(r.get(prod_col)) if prod_col else None
            br = str(r.get(brand_col)) if brand_col else None
            # ì•„ë§ˆì¡´ 'Amazon' â†’ 'Amazon Basics'ë¡œ í†µì¼
            if src == 'amazon_us' and br and br.strip().lower()=='amazon':
                br='Amazon Basics'
            rows.append({
                'source': src,
                'date': dates.iloc[i],
                'rank': pd.to_numeric(r.get(rank_col), errors='coerce'),
                'product': nm,
                'brand': br,
                'url': r.get(url_col) if url_col else None,
                'price': pd.to_numeric(r.get(price_col), errors='coerce') if price_col else None,
                'orig_price': pd.to_numeric(r.get(orig_col), errors='coerce') if orig_col else None,
                'discount_rate': pd.to_numeric(r.get(disc_col), errors='coerce') if disc_col else None,
                'key': extract_key(src, r, r.get(url_col) if url_col else None),
                'promo': is_promo(nm),
                'infl': list(extract_influencers_dynamic(nm)),
            })
    ud = pd.DataFrame(rows, columns=['source','date','rank','product','brand','url','price','orig_price','discount_rate','key','promo','infl'])
    ud = ud.dropna(subset=['source','date','rank','key'])
    return ud

# --------- ì§‘ê³„ ---------
def summarize_week(ud:pd.DataFrame, src:str, min_days:int=3):
    res = {'range':'ë°ì´í„° ì—†ìŒ','top10_lines':['ë°ì´í„° ì—†ìŒ'],'brand_lines':['ë°ì´í„° ì—†ìŒ'],
           'inout':'','heroes':[],'flash':[],'discount_all':None,'discount_promo':None,
           'discount_nonpromo':None,'discount_delta_same':None,'median_price':None,
           'cat_top5':[],'kw_lines':[],'insights':[],'stats':{}}
    rng = week_range_for_source(ud, src)
    if not rng: return res
    start,end = rng; topn = SRC_INFO[src]['topn']

    cur = ud[(ud['source'].eq(src)) & (ud['date']>=start) & (ud['date']<=end) & (ud['rank']<=topn)].copy()
    if cur.empty:
        res['range']=f"{start.date()}~{end.date()}"; return res
    prev = ud[(ud['source'].eq(src)) & (ud['date']>=start-pd.Timedelta(days=7)) & (ud['date']<=end-pd.Timedelta(days=7)) & (ud['rank']<=topn)].copy()
    hist = ud[(ud['source'].eq(src)) & (ud['date']>=start-pd.Timedelta(days=28)) & (ud['date']<=start-pd.Timedelta(days=1)) & (ud['rank']<=topn)].copy()

    cur_base  = daily_topn_base(cur, topn)
    prev_base = daily_topn_base(prev, topn)

    # ì£¼ê°„ í¬ì¸íŠ¸ í…Œì´ë¸” ë‘ ë²Œ(ë°˜ì§ ë²„ê·¸ ë°©ì§€)
    tmp = cur_base.copy()
    tmp['__pts'] = topn + 1 - tmp['rank']
    pts_all = (tmp.groupby('key', as_index=False)
                 .agg(points=('__pts','sum'),
                      days=('rank','count'),
                      best=('rank','min'),
                      mean_rank=('rank','mean')))
    pts_stable = pts_all[pts_all['days'] >= min_days]

    # ìµœì‹  ì •ë³´
    latest = (cur_base.sort_values('date')
                .groupby('key', as_index=False)
                .agg(product=('product','last'),
                     brand=('brand','last'),
                     url=('url','last')))

    # ì´ì „ì£¼ í‰ê· ìˆœìœ„/ì¼ìˆ˜ ë§µ
    prev_tbl=None; prev_mean_map={}; prev_days_map={}
    if not prev_base.empty:
        ptmp=prev_base.copy(); ptmp['__pts']=topn+1-ptmp['rank']
        prev_tbl=(ptmp.groupby('key', as_index=False)
                    .agg(points=('__pts','sum'), days=('rank','count'), best=('rank','min'), mean_rank=('rank','mean')))
        prev_mean_map=dict(zip(prev_tbl['key'], prev_tbl['mean_rank']))
        prev_days_map=dict(zip(prev_tbl['key'], prev_tbl['days']))

    # Top10
    top = (pts_stable.merge(latest,on='key',how='left')
             .sort_values(['points','days','best'], ascending=[False,False,True]).head(10))

    top_lines=[]
    for i,r in enumerate(top.itertuples(),1):
        key=getattr(r,'key'); nm=getattr(r,'product') or getattr(r,'brand') or key
        u=getattr(r,'url') or ''
        label=f"<{u}|{nm}>" if u else nm
        cur_mean=getattr(r,'mean_rank'); prev_mean=prev_mean_map.get(key); prev_days=prev_days_map.get(key,0)
        delta_txt="NEW" if (prev_mean is None or prev_days<min_days) else _arrow_rank(prev_mean-cur_mean)
        mean_txt=f"{round(float(cur_mean),1)}ìœ„" if cur_mean is not None else "-"
        top_lines.append(f"{i}. {label} (ìœ ì§€ {int(getattr(r,'days'))}ì¼ Â· í‰ê·  {mean_txt}) ({delta_txt})")

    # ë¸Œëœë“œ(ì¼í‰ê·  ê°œìˆ˜, ì¦ê°)
    def brand_daily_avg(base):
        return (base.groupby(['day','brand']).size().groupby('brand').mean().reset_index(name='per_day'))
    b_now = brand_daily_avg(cur_base).rename(columns={'per_day':'now'})
    b_prev = brand_daily_avg(prev_base).rename(columns={'per_day':'prev'}) if not prev_base.empty else pd.DataFrame(columns=['brand','prev'])
    b = (b_now.merge(b_prev,on='brand',how='left').fillna(0.0)
            .assign(delta=lambda x:x['now']-x['prev'])
            .sort_values(['now','delta'], ascending=[False,False]).head(12))
    brand_lines=[]
    for r in b.itertuples():
        sign = f"â†‘{round(r.delta,1)}" if r.delta>0 else (f"â†“{abs(round(r.delta,1))}" if r.delta<0 else "â€”")
        brand_lines.append(f"{r.brand} {round(r.now,1)}ê°œ/ì¼ ({sign})")

    # IN(êµì²´ ìˆ˜) â€“ OUTì€ í‘œì‹œí•˜ì§€ ì•ŠìŒ
    days = sorted(cur_base['day'].unique())
    prev_days_set = set(prev_base['day'].unique())
    total_in = 0
    valid = 0
    for d in days:
        pd_ = pd.to_datetime(d) - pd.Timedelta(days=1)
        if pd_.date() not in prev_days_set: continue
        cur_set  = set(cur_base.loc[cur_base['day'].eq(d),'key'])
        prev_set = set(prev_base.loc[prev_base['day'].eq(pd_.date()),'key'])
        total_in += len(cur_set - prev_set)
        valid += 1
    in_avg = round(total_in/valid, 1) if valid else 0.0
    inout_line = "ë¹„êµ ê¸°ì¤€ ì—†ìŒ" if valid==0 else f"ì¼í‰ê·  {in_avg:.1f}ê°œ"

    # íˆì–´ë¡œ / ë°˜ì§
    hist_keys=set(hist['key'].unique()) if not hist.empty else set()
    heroes=(pts_stable[~pts_stable['key'].isin(hist_keys)].merge(latest,on='key',how='left')
              .sort_values(['points','days','best'], ascending=[False,False,True]).head(5))
    flash=(pts_all[(pts_all['days']<=FLASH_MAX_DAYS)].merge(latest,on='key',how='left')
              .sort_values(['points','days','best'], ascending=[False,False,True]).head(5))

    def to_links(df):
        if df is None or df.empty: return []
        out=[]
        for r in df.itertuples():
            nm=getattr(r,'product') or getattr(r,'brand') or getattr(r,'key'); u=getattr(r,'url') or ''
            out.append(f"<{u}|{nm}>" if u else nm)
        return out

    # ê°€ê²©/í• ì¸
    wk=(cur_base.groupby('key').agg(price_med=('price','median'), disc_avg=('discount_rate','mean')).reset_index())
    med_price=int(wk['price_med'].dropna().median()) if wk['price_med'].notna().any() else None

    promo_base=cur_base[cur_base['promo']==True]; non_base=cur_base[cur_base['promo']!=True]
    def _mean_disc(df): return round(float(df['discount_rate'].dropna().mean()),2) if df['discount_rate'].notna().any() else None
    disc_all=_mean_disc(cur_base); disc_promo=_mean_disc(promo_base); disc_non=_mean_disc(non_base)

    both=(cur_base.groupby(['key','promo'])['discount_rate'].mean().reset_index()
                .pivot(index='key', columns='promo', values='discount_rate').dropna(how='all'))
    disc_delta_same=None
    if not both.empty and True in both.columns and False in both.columns:
        diff=(both[True]-both[False]).dropna()
        if not diff.empty: disc_delta_same=round(float(diff.mean()),2)

    # ì¹´í…Œê³ ë¦¬ ìƒìœ„(ìœ ë‹ˆí¬ ì œí’ˆ ëŒ€ë¹„ %)
    uniq = cur_base.sort_values('date').drop_duplicates('key')[['key','product']]
    def map_cat(name:str)->str:
        nm=(name or "").lower()
        for cat,pat in CATEGORY_RULES:
            if re.search(pat,nm,re.IGNORECASE): return cat
        return "ê¸°íƒ€"
    cats = uniq.copy(); cats['__cat']=cats['product'].map(map_cat)
    cat_cnt = cats['__cat'].value_counts()
    total_uniq = len(uniq)
    cat_pairs = [f"{c} {round(n*100/total_uniq,1)}%" for c,n in cat_cnt.head(5).items()]

    # í‚¤ì›Œë“œ(ìœ ë‹ˆí¬ ì œí’ˆ ê¸°ì¤€ %)
    def share_unique(base, rules):
        keys = base.sort_values('date').drop_duplicates('key')[['key','product']]
        total = len(keys); cnt = Counter()
        for row in keys.itertuples():
            name=getattr(row,'product') or ''
            for label,pat in rules.items():
                if re.search(pat, name, re.IGNORECASE):
                    cnt[label]+=1
        if total==0: return []
        return [f"{k} {round(v*100/total,1)}%" for k,v in cnt.most_common(6)]

    kw_lines=[]
    p_items=share_unique(cur_base, KW_PRODUCT)
    e_items=share_unique(cur_base, KW_EFFICACY)
    m_items=share_unique(cur_base, KW_MARKETING)
    ing_items=share_unique(cur_base, KW_INGREDIENT)
    if p_items: kw_lines.append("â€¢ ì œí’ˆí˜•íƒœ: " + ", ".join(p_items))
    if e_items: kw_lines.append("â€¢ íš¨ëŠ¥: " + ", ".join(e_items))
    if m_items: kw_lines.append("â€¢ ë§ˆì¼€íŒ…: " + ", ".join(m_items))
    if ing_items: kw_lines.append("â€¢ ì„±ë¶„: " + ", ".join(ing_items))

    # ì¸í”Œë£¨ì–¸ì„œ(ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ë§Œ)
    if src=='oy_kor':
        icnt=Counter()
        for names in cur_base['infl'].dropna():
            for n in names: icnt[n]+=1
        infl_names=[n for n,_ in icnt.most_common(8)]
        if infl_names: kw_lines.append("â€¢ ì¸í”Œë£¨ì–¸ì„œ: " + ", ".join(infl_names))

    # ê°€ê²©ëŒ€ ë²„í‚·(ì†ŒìŠ¤ë³„)
    def price_bucket(src, med):
        if med is None: return None
        if src=='amazon_us':
            if med<10:  return "$10 ë¯¸ë§Œ"
            if med<20:  return "$10ëŒ€"
            if med<30:  return "$20ëŒ€"
            if med<40:  return "$30ëŒ€"
            return "$40ëŒ€+"
        if src=='qoo10_jp':
            if med<1000:  return "Â¥1ì²œ ë¯¸ë§Œ"
            if med<2000:  return "Â¥1ì²œëŒ€"
            if med<3000:  return "Â¥2ì²œëŒ€"
            if med<4000:  return "Â¥3ì²œëŒ€"
            return "Â¥4ì²œëŒ€+"
        if src=='daiso_kr':
            if med<2000: return "2ì²œ ë¯¸ë§Œ"
            if med<3000: return "2ì²œëŒ€"
            if med<5000: return "3~4ì²œëŒ€"
            return "5ì²œ+"
        # default KRW
        if med<10000: return "1ë§Œ ë¯¸ë§Œ"
        if med<20000: return "1ë§ŒëŒ€"
        if med<30000: return "2ë§ŒëŒ€"
        if med<40000: return "3ë§ŒëŒ€"
        return "4ë§Œ+"
    price_bucket_txt = price_bucket(src, med_price)

    # ì¸ì‚¬ì´íŠ¸ (ì¤‘ë³µ ì œê±°: TopN ë“±ê·¹ SKUëŠ” ì œì™¸)
    keep_mean = round(float(pts_all['days'].mean()), 1) if not pts_all.empty else 0.0
    keep_med  = int(pts_stable['days'].median()) if not pts_stable.empty else 0
    g_up=b.sort_values('delta',ascending=False).head(1)
    g_dn=b.sort_values('delta',ascending=True).head(1)
    up_txt = f"{g_up.iloc[0]['brand']}(+{round(g_up.iloc[0]['delta'],1)}/ì¼)" if not g_up.empty and g_up.iloc[0]['delta']>0 else None
    dn_txt = f"{g_dn.iloc[0]['brand']}(-{abs(round(g_dn.iloc[0]['delta'],1))}/ì¼)" if not g_dn.empty and g_dn.iloc[0]['delta']<0 else None
    promo_effect=None
    if (disc_promo is not None) and (disc_non is not None):
        diff=round(disc_promo-disc_non,2)
        if abs(diff)>=2.0: promo_effect=f"í”„ë¡œëª¨ì…˜ í‰ê·  í• ì¸ìœ¨ì´ ì¼ë°˜ ëŒ€ë¹„ {('+' if diff>0 else '')}{diff}%p"

    insights=[f"Top {topn} ìœ ì§€ í‰ê·  {keep_mean}ì¼"]  # â† ìš”ì²­ëŒ€ë¡œ í‘œê¸°
    if up_txt or dn_txt:
        bits=[]
        if up_txt: bits.append("ìƒìŠ¹ "+up_txt)
        if dn_txt: bits.append("í•˜ë½ "+dn_txt)
        insights.append(", ".join(bits))
    if price_bucket_txt: insights.append(f"ì£¼ìš” ê°€ê²©ëŒ€ {price_bucket_txt}")
    if promo_effect: insights.append(promo_effect)

    res.update({
        'range': f"{start.date()}~{end.date()}",
        'top10_lines': top_lines,
        'brand_lines': brand_lines,
        'inout': inout_line,           # ğŸ” ì¸ì•¤ì•„ì›ƒ(êµì²´): "ì¼í‰ê·  n.nê°œ"
        'heroes': to_links(heroes),
        'flash': to_links(flash),
        'discount_all': disc_all,
        'discount_promo': disc_promo,
        'discount_nonpromo': disc_non,
        'discount_delta_same': disc_delta_same,
        'median_price': med_price,
        'cat_top5': [f"{x}" for x in cat_pairs],
        'kw_lines': kw_lines,
        'insights': insights,
        'stats': {
            'unique_items': total_uniq,
            'keep_days_mean': keep_mean,
            'keep_days_median': keep_med,
            'topn': topn
        }
    })
    return res

# --------- ìŠ¬ë™ í¬ë§· ---------
def format_slack_block(src:str, s:dict)->str:
    title_map={'oy_kor':"ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",'oy_global':"ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",
               'amazon_us':"ì•„ë§ˆì¡´ US Top100",'qoo10_jp':"íí… ì¬íŒ¬ ë·°í‹° Top200",'daiso_kr':"ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200"}
    L=[]
    L.append(f"ğŸ“Š ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title_map.get(src,src)} ({s['range']})")
    L.append("ğŸ† Top10"); L.extend(s.get('top10_lines') or ["ë°ì´í„° ì—†ìŒ"]); L.append("")
    L.append("ğŸ ë¸Œëœë“œ ê°œìˆ˜(ì¼í‰ê· )"); L.extend(s.get('brand_lines') or ["ë°ì´í„° ì—†ìŒ"]); L.append("")
    # ì¸ì•¤ì•„ì›ƒ(êµì²´): ì¼í‰ê· ë§Œ
    L.append(f"ğŸ” ì¸ì•¤ì•„ì›ƒ(êµì²´): {s.get('inout','ë¹„êµ ê¸°ì¤€ ì—†ìŒ')}")
    # íˆì–´ë¡œ/ë°˜ì§ â€“ ê¸°ì¤€ ì„¤ëª… ì¶”ê°€
    L.append("ğŸ†• ì‹ ê·œ íˆì–´ë¡œ(3ì¼ ì´ìƒ ë­í¬ ìœ ì§€): " + (", ".join(s.get('heroes') or []) if s.get('heroes') else "ì—†ìŒ"))
    L.append("âœ¨ ë°˜ì§ ì•„ì´í…œ(2ì¼ ì´ë‚´ ë­í¬ ì•„ì›ƒ): " + (", ".join(s.get('flash')  or []) if s.get('flash')  else "ì—†ìŒ"))
    if s.get('cat_top5'): L.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ ìƒìœ„: " + " Â· ".join(s['cat_top5']))
    if s.get('kw_lines'):
        L.append("ğŸ” ì£¼ê°„ í‚¤ì›Œë“œ ë¶„ì„"); L.extend(s['kw_lines'])
    tail=[]
    if s.get('median_price') is not None: tail.append("ì¤‘ìœ„ê°€ê²© " + (fmt_money(s['median_price'], src) or ""))
    disc=[]
    if s.get('discount_all')       is not None: disc.append(f"ì „ì²´ {s['discount_all']:.2f}%")
    if s.get('discount_promo')     is not None: disc.append(f"í”„ë¡œëª¨ì…˜ {s['discount_promo']:.2f}%")
    if s.get('discount_nonpromo')  is not None: disc.append(f"ì¼ë°˜ {s['discount_nonpromo']:.2f}%")
    if s.get('discount_delta_same') is not None: disc.append(f"(ë™ì¼ìƒí’ˆ ì°¨ì´ {('+' if s['discount_delta_same']>=0 else '')}{s['discount_delta_same']:.2f}%p)")
    if disc: tail.append("í‰ê·  í• ì¸ìœ¨ " + " Â· ".join(disc))
    if tail: L.append("ğŸ’µ " + " / ".join(tail))
    if s.get('insights'):
        L.append(""); L.append("ğŸ§  ìµœì¢… ì¸ì‚¬ì´íŠ¸")
        for x in s['insights']: L.append(f"- {x}")
    return "\n".join(L)

# --------- ì—”íŠ¸ë¦¬ ---------
def _parse_src_args(arg_src):
    """--srcê°€ ì½¤ë§ˆ/ê³µë°± í˜¼ìš©ë¼ë„ ì•ˆì „í•˜ê²Œ íŒŒì‹±"""
    if isinstance(arg_src, list):
        raw = arg_src
    else:
        raw = [arg_src]
    tokens=[]
    for t in raw:
        tokens += [p for p in re.split(r'[,\s]+', str(t)) if p]
    targets=[]
    for s in tokens:
        if s == "all":
            targets.extend(ALL_SRCS)
        elif s in ALL_SRCS:
            targets.append(s)
    # ì¤‘ë³µ ì œê±°
    return [t for i,t in enumerate(targets) if t not in targets[:i]]

def main():
    parser=argparse.ArgumentParser()
    parser.add_argument("--src", nargs="+", default=os.getenv("ONLY_SRC","all"))
    parser.add_argument("--split", action="store_true", default=True, help="ì†ŒìŠ¤ë³„ ê°œë³„ íŒŒì¼ ìƒì„±(ê¸°ë³¸ ON)")
    parser.add_argument("--data-dir", default=os.getenv("DATA_DIR","./data/daily"))
    parser.add_argument("--min-days", type=int, default=int(os.getenv("MIN_DAYS","3")))
    args=parser.parse_args()

    targets=_parse_src_args(args.src)
    if not targets: targets = ALL_SRCS

    ud=load_unified(args.data_dir)
    combined={}; combined_txt=[]
    for src in targets:
        s=summarize_week(ud, src, min_days=args.min_days)
        combined[src]=s
        text=format_slack_block(src, s)
        with open(f"weekly_summary_{src}.json","w",encoding="utf-8") as f: json.dump(s,f,ensure_ascii=False,indent=2)
        with open(f"slack_{src}.txt","w",encoding="utf-8") as f: f.write(text)
        if not args.split: combined_txt.append(text)

    print(json.dumps(combined, ensure_ascii=False, indent=2))
    if not args.split:
        with open("weekly_slack_message.txt","w",encoding="utf-8") as f:
            f.write("\n\nâ€” â€” â€”\n\n".join(combined_txt))

if __name__=="__main__":
    main()
