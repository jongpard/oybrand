# scripts/weekly_report_plus.py
# -*- coding: utf-8 -*-
import os, re, glob, json, math
import pandas as pd
from collections import Counter
from datetime import timedelta, timezone

KST = timezone(timedelta(hours=9))

SRC_INFO = {
    'oy_kor':    {'topn':100, 'hints':['ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹','oliveyoung_kor','oy_kor'], 'currency':'KRW'},
    'oy_global': {'topn':100, 'hints':['ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ','oliveyoung_global','oy_global'], 'currency':'KRW'},
    'amazon_us': {'topn':100, 'hints':['ì•„ë§ˆì¡´US','amazon_us','amazonUS'],       'currency':'USD'},
    'qoo10_jp':  {'topn':200, 'hints':['íí…ì¬íŒ¬','qoo10','Qoo10'],               'currency':'JPY'},
    'daiso_kr':  {'topn':200, 'hints':['ë‹¤ì´ì†Œëª°','daiso'],                       'currency':'KRW'},
}
ALL_SRCS = list(SRC_INFO.keys())

# === ì»¬ëŸ¼ ë™ì˜ì–´ (ì œí’ˆëª…ì€ raw_name ìµœìš°ì„ , ê·¸ëŒ€ë¡œ í‘œê¸°) ===
COLS = {
    'rank':        ['rank','ìˆœìœ„','ranking','ë­í‚¹'],
    'raw_name':    ['raw_name','raw','rawProduct','rawTitle'],
    'product':     ['product','ì œí’ˆëª…','ìƒí’ˆëª…','name','title','goods_name','goodsNm',
                    'prdNm','prdtName','displayName','itemNm','ìƒí’ˆ','item_name','item'],
    'brand':       ['brand','ë¸Œëœë“œ','brand_name','brandNm','ë¸Œëœë“œëª…'],
    'url':         ['url','ë§í¬','product_url','link','detail_url'],
    'date':        ['date','ë‚ ì§œ','ìˆ˜ì§‘ì¼','crawl_date','created_at'],
    'price':       ['price','ê°€ê²©','sale_price','selling_price'],
    'orig_price':  ['orig_price','ì •ê°€','original_price','ì†Œë¹„ìê°€','list_price'],
    'discount':    ['discount_rate','í• ì¸ìœ¨','discount','discountPercent'],
    'goodsNo':     ['goodsNo','goods_no','goodsno','ìƒí’ˆë²ˆí˜¸','ìƒí’ˆì½”ë“œ'],
    'productId':   ['productId','product_id','prdtNo','ìƒí’ˆID','ìƒí’ˆì•„ì´ë””','ìƒí’ˆì½”ë“œ'],
    'asin':        ['asin','ASIN'],
    'product_code':['product_code','productCode','ìƒí’ˆì½”ë“œ','item_code'],
    'pdNo':        ['pdNo','pdno','ìƒí’ˆë²ˆí˜¸','ìƒí’ˆì½”ë“œ'],
}

STOPWORDS = set("""
ì˜ ê°€ ì´ ì€ ëŠ” ì„ ë¥¼ ì— ì—ì„œ ìœ¼ë¡œ ë„ ê³¼ ì™€ ë° ( ) , . : Â· - & x X + the and or for of with
ì„¸íŠ¸ 1+1 2+1 10ê°œì… 20ë§¤ 30g 50ml 100ml 200ml pack set
""".split())

CATEGORY_RULES = [
    ("ë§ˆìŠ¤í¬íŒ©", r"(ë§ˆìŠ¤í¬íŒ©|íŒ©|sheet\s*mask|mask\s*pack)"),
    ("ì„ ì¼€ì–´", r"(ì„ í¬ë¦¼|ìì™¸ì„ |sun\s*cream|sunscreen|uv)"),
    ("í´ë Œì €", r"(í´ë Œì§•|í´ë Œì €|foam|cleanser|wash)"),
    ("í† ë„ˆ/ìŠ¤í‚¨", r"(í† ë„ˆ|ìŠ¤í‚¨|toner)"),
    ("ì—ì„¼ìŠ¤/ì„¸ëŸ¼", r"(ì—ì„¼ìŠ¤|ì„¸ëŸ¼|ì•°í”Œ|serum|essence|ampoule)"),
    ("ë¡œì…˜/ì—ë©€ì „", r"(ë¡œì…˜|ì—ë©€ì „|lotion|emulsion)"),
    ("í¬ë¦¼", r"(í¬ë¦¼|cream|moisturizer)"),
    ("ë¦½", r"(ë¦½|í‹´íŠ¸|ë¦½ë°¤|lip|tint|balm)"),
    ("ì•„ì´", r"(ì•„ì´í¬ë¦¼|eye\s*cream|ì•„ì´|ì•„ì´íŒ¨ì¹˜)"),
    ("í—¤ì–´", r"(ìƒ´í‘¸|íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸|í—¤ì–´|hair)"),
    ("ë°”ë””", r"(ë°”ë””|body|ë°”ìŠ¤)"),
    ("í–¥ìˆ˜", r"(í–¥ìˆ˜|í¼í“¸|eau|parfum|perfume)"),
    ("ë„êµ¬/ê¸°ê¸°", r"(ê¸°ê¸°|ë””ë°”ì´ìŠ¤|ë¡¤ëŸ¬|device|tool)"),
]

# ---------- ìœ í‹¸ ----------
def pick(df, names):
    for c in names:
        if c in df.columns: return c
    return None

def pick_loose_product(df):
    rn = pick(df, COLS['raw_name'])
    if rn: return rn
    pn = pick(df, COLS['product'])
    if pn: return pn
    patt = re.compile(r"(product|name|title|ìƒí’ˆ|ì œí’ˆ|í’ˆëª…|ì•„ì´í…œ)", re.IGNORECASE)
    for c in df.columns:
        if patt.search(str(c)): return c
    return None

def infer_source(path:str):
    base = os.path.basename(path).lower()
    parent = os.path.basename(os.path.dirname(path)).lower()
    for src, info in SRC_INFO.items():
        for h in info['hints']:
            h = h.lower()
            if h in base or h in parent:
                return src
    return None

def infer_date_from_filename(fn:str):
    m = re.search(r'(\d{4}-\d{2}-\d{2})', os.path.basename(fn))
    return pd.to_datetime(m.group(1)) if m else pd.NaT

def read_csv_any(path:str)->pd.DataFrame:
    for enc in ('utf-8-sig','cp949','utf-8','euc-kr'):
        try: return pd.read_csv(path, encoding=enc)
        except Exception: pass
    return pd.read_csv(path)

def extract_key(src:str, row, url:str|None):
    u = str(url or "")
    if src == 'oy_kor':
        for c in COLS['goodsNo']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'goodsNo=([0-9A-Za-z\-]+)', u);  return m.group(1) if m else None
    if src == 'oy_global':
        for c in COLS['productId']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'(?:productId|prdtNo)=([0-9A-Za-z\-]+)', u); return m.group(1) if m else None
    if src == 'amazon_us':
        for c in COLS['asin']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip().upper()
        m = re.search(r'/([A-Z0-9]{10})(?:[/?#]|$)', u.upper()); return m.group(1) if m else None
    if src == 'qoo10_jp':
        for c in COLS['product_code']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'product_code=([0-9A-Za-z\-]+)', u)
        if m: return m.group(1)
        m2 = re.search(r'/(\d{6,})', u)
        return m2.group(1) if m2 else None
    if src == 'daiso_kr':
        for c in COLS['pdNo']:
            if c in row and pd.notna(row[c]): return str(row[c]).strip()
        m = re.search(r'pdNo=([0-9A-Za-z\-]+)', u); return m.group(1) if m else None
    return None

def fmt_money(v, src):
    if v is None or (isinstance(v,float) and (pd.isna(v) or math.isnan(v))): return None
    cur = SRC_INFO[src]['currency']
    if cur == 'USD':  return f"${v:,.0f}"
    if cur == 'JPY':  return f"Â¥{v:,.0f}"
    # default KRW
    return f"â‚©{v:,.0f}"

def week_range_for_source(ud:pd.DataFrame, src:str):
    dts = ud.loc[ud['source'].eq(src), 'date']
    if dts.empty: return None
    last = pd.to_datetime(dts.max())
    end = last + pd.Timedelta(days=(6 - last.weekday()))  # ê·¸ ì£¼ ì¼ìš”ì¼
    start = end - pd.Timedelta(days=6)                    # ê·¸ ì£¼ ì›”ìš”ì¼
    return start.normalize(), end.normalize()

def _arrow_rank(diff_rank: float) -> str:
    # diff_rank = prev_mean - cur_mean (ê°œì„ ì´ë©´ ì–‘ìˆ˜)
    if diff_rank is None or (isinstance(diff_rank,float) and math.isnan(diff_rank)): return "â€”"
    d = int(round(diff_rank))
    if d > 0:  return f"â†‘{d}"
    if d < 0:  return f"â†“{abs(d)}"
    return "â€”"

def _day_set(df, day, topn):
    d = df[df['date'].dt.date.eq(day)].sort_values('rank').drop_duplicates('key')
    return set(d.head(topn)['key'])

def _inout_daily(cur, prev, topn, start):
    days = sorted(set(cur['date'].dt.date))
    if not days: return 0, 0, 0.0
    prev_last = _day_set(prev, (start - pd.Timedelta(days=1)).date(), topn) if not prev.empty else set()
    total_in = total_out = 0
    last_set = prev_last
    for d in days:
        cur_set = _day_set(cur, d, topn)
        enter = cur_set - last_set
        leave = last_set - cur_set
        total_in  += len(enter)
        total_out += len(leave)
        last_set = cur_set
    if total_in != total_out:
        m = max(total_in, total_out)
        total_in = total_out = m
    return total_in, total_out, round(total_in/len(days), 2)

def _weekly_points_table(df, topn):
    tmp = df.copy()
    tmp['__pts'] = topn + 1 - tmp['rank']
    return (tmp.groupby('key', as_index=False)
                .agg(points=('__pts','sum'),
                     days=('rank','count'),
                     best=('rank','min'),
                     mean_rank=('rank','mean')))

# ---------- ë¡œë”© ----------
def load_unified(data_dir:str)->pd.DataFrame:
    paths = glob.glob(os.path.join(data_dir,'**','*.csv'), recursive=True)
    rows = []
    for p in paths:
        src = infer_source(p)
        if not src: continue
        try:
            df = read_csv_any(p)
        except Exception:
            continue

        date_col = pick(df, COLS['date'])
        dates = pd.to_datetime(df[date_col], errors='coerce') if date_col else pd.Series([infer_date_from_filename(p)]*len(df))
        rank_col = pick(df, COLS['rank'])
        if not rank_col: continue

        prod_col = pick_loose_product(df)     # raw_name ìš°ì„ , ê·¸ëŒ€ë¡œ ì‚¬ìš©
        brand_col = pick(df, COLS['brand'])
        url_col   = pick(df, COLS['url'])
        price_col = pick(df, COLS['price'])
        orig_col  = pick(df, COLS['orig_price'])
        disc_col  = pick(df, COLS['discount'])

        for i, r in df.iterrows():
            nm = str(r.get(prod_col)) if prod_col else None
            rows.append({
                'source': src,
                'date': dates.iloc[i],
                'rank': pd.to_numeric(r.get(rank_col), errors='coerce'),
                'product': nm,
                'brand': str(r.get(brand_col)) if brand_col else None,
                'url': r.get(url_col) if url_col else None,
                'price': pd.to_numeric(r.get(price_col), errors='coerce') if price_col else None,
                'orig_price': pd.to_numeric(r.get(orig_col), errors='coerce') if orig_col else None,
                'discount_rate': pd.to_numeric(r.get(disc_col), errors='coerce') if disc_col else None,
                'key': extract_key(src, r, r.get(url_col) if url_col else None),
            })

    ud = pd.DataFrame(rows, columns=['source','date','rank','product','brand','url','price','orig_price','discount_rate','key'])
    ud = ud.dropna(subset=['source','date','rank','key'])
    return ud

# ---------- ì§‘ê³„ ----------
def summarize_week(ud:pd.DataFrame, src:str, min_days:int=3):
    res = {
        'range': 'ë°ì´í„° ì—†ìŒ',
        'top10_lines': ['ë°ì´í„° ì—†ìŒ'],
        'brand_lines': ['ë°ì´í„° ì—†ìŒ'],
        'inout': '',
        'heroes': [],
        'flash': [],
        'discount': None,
        'median_price': None,
        'cat_top5': [],
        'kw_top10': [],
        'insights': [],
        'stats': {},   # ì´ ìœ ë‹ˆí¬ ìˆ˜ ë“±
    }
    rng = week_range_for_source(ud, src)
    if not rng: return res
    start, end = rng
    topn = SRC_INFO[src]['topn']

    cur = ud[(ud['source'].eq(src)) & (ud['date']>=start) & (ud['date']<=end) & (ud['rank']<=topn)].copy()
    if cur.empty:
        res['range'] = f"{start.date()}~{end.date()}"
        return res

    prev = ud[(ud['source'].eq(src)) &
              (ud['date']>=start - pd.Timedelta(days=7)) &
              (ud['date']<=end - pd.Timedelta(days=7)) &
              (ud['rank']<=topn)].copy()

    hist = ud[(ud['source'].eq(src)) &
              (ud['date']>=start - pd.Timedelta(days=28)) &
              (ud['date']<=start - pd.Timedelta(days=1)) &
              (ud['rank']<=topn)].copy()

    # ì£¼ê°„ í…Œì´ë¸”(ì ìˆ˜/í‰ê· ìˆœìœ„ ë‘˜ ë‹¤)
    pts = _weekly_points_table(cur, topn)
    pts = pts[pts['days'] >= min_days]
    latest = (cur.sort_values('date')
                .groupby('key', as_index=False)
                .agg(product=('product','last'),
                     brand=('brand','last'),
                     url=('url','last')))
    prev_tbl = None; prev_pts_map = {}; prev_mean_map = {}
    if not prev.empty:
        prev_tbl = _weekly_points_table(prev, topn)
        prev_pts_map  = dict(zip(prev_tbl['key'],  prev_tbl['points']))
        prev_mean_map = dict(zip(prev_tbl['key'],  prev_tbl['mean_rank']))

    # ì •ë ¬: ì ìˆ˜â†“ â†’ ìœ ì§€ì¼ìˆ˜â†“ â†’ ìµœê³ ìˆœìœ„â†‘
    top = (pts.merge(latest, on='key', how='left')
             .sort_values(['points','days','best'], ascending=[False, False, True])
             .head(10))

    # Top10 ë¼ì¸: (ìœ ì§€ nì¼, (â†‘n/â†“n/NEW/â€”)) â† ê´„í˜¸ í‘œê¸°
    top_lines = []
    for i, r in enumerate(top.itertuples(), 1):
        key = getattr(r,'key')
        nm = getattr(r,'product') or getattr(r,'brand') or key
        u  = getattr(r,'url') or ''
        label = f"<{u}|{nm}>" if u else nm

        # í‰ê· ìˆœìœ„ ë“±ë½: prev_mean - cur_mean (ê°œì„ ì´ë©´ ì–‘ìˆ˜)
        cur_mean = getattr(r,'mean_rank')
        prev_mean = prev_mean_map.get(key)
        if prev_mean is None:
            delta_txt = "NEW"
        else:
            diff = prev_mean - cur_mean
            delta_txt = _arrow_rank(diff)

        top_lines.append(f"{i}. {label} (ìœ ì§€ {int(getattr(r,'days'))}ì¼, {delta_txt})")

    # ë¸Œëœë“œ "ê°œìˆ˜" (ì „ì£¼ ë™ì¼ ìœˆë„ìš° ë¹„êµ), â†‘/â†“/â€”
    b_now = (cur.assign(brand=cur['brand'].fillna('ê¸°íƒ€'))
               .groupby('brand').size().reset_index(name='count'))
    if not prev.empty:
        b_prev = (prev.assign(brand=prev['brand'].fillna('ê¸°íƒ€'))
                    .groupby('brand').size().reset_index(name='prev'))
    else:
        b_prev = pd.DataFrame(columns=['brand','prev'])
    b = b_now.merge(b_prev, on='brand', how='left').fillna(0.0)
    b['delta'] = b['count'] - b['prev']
    b = b.sort_values(['count','delta'], ascending=[False, False]).head(12)
    brand_lines = []
    for r in b.itertuples():
        if r.delta > 0:  sign = f"â†‘{int(r.delta)}"
        elif r.delta < 0: sign = f"â†“{abs(int(r.delta))}"
        else:            sign = "â€”"
        brand_lines.append(f"{r.brand} {int(r.count)}ê°œ {sign}")

    # IN/OUT: ì „ì¼ ëŒ€ë¹„ ì§‘í•© ê¸°ì¤€ â†’ í•­ìƒ ë™ì¹˜
    in_cnt, out_cnt, daily_avg = _inout_daily(cur, prev, topn, start)

    # ì‹ ê·œ íˆì–´ë¡œ / ë°˜ì§
    hist_keys = set(hist['key'].unique()) if not hist.empty else set()
    heroes = (pts[~pts['key'].isin(hist_keys)]
                .merge(latest, on='key', how='left')
                .sort_values(['points','days','best'], ascending=[False, False, True])
                .head(5))
    flash = (pts[(pts['days']<=2)]
                .merge(latest, on='key', how='left')
                .sort_values(['points','days','best'], ascending=[False, False, True])
                .head(5))

    def to_links(df):
        out = []
        for r in df.itertuples():
            nm = getattr(r,'product') or getattr(r,'brand') or getattr(r,'key')
            u  = getattr(r,'url') or ''
            out.append(f"<{u}|{nm}>" if u else nm)
        return out

    # í• ì¸/ê°€ê²©(í‘œì‹œìš© í¬ë§·)
    avg_disc = None; med_price = None
    if cur['discount_rate'].notna().any():
        avg_disc = round(float(cur['discount_rate'].dropna().mean()), 2)
    elif 'orig_price' in cur and 'price' in cur:
        op = cur['orig_price']; sp = cur['price']
        valid = (~op.isna()) & (~sp.isna()) & (op>0)
        if valid.any():
            avg_disc = round(float(((1 - sp[valid]/op[valid])*100).mean()), 2)
    if cur['price'].notna().any():
        med_price = int(cur['price'].dropna().median())

    # ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ
    def map_cat(name:str)->str:
        nm = (name or "").lower()
        for cat, pat in CATEGORY_RULES:
            if re.search(pat, nm, re.IGNORECASE): return cat
        return "ê¸°íƒ€"
    cats = cur.copy()
    cats['__cat'] = cats['product'].map(map_cat)
    cat_top5 = cats.groupby('__cat').size().sort_values(ascending=False).head(5)
    cat_pairs = [f"{c} {int(n)}ê°œ" for c,n in cat_top5.items()]
    toks = []
    for nm in cur['product'].dropna().astype(str):
        txt = re.sub(r"[\(\)\[\]{}Â·\-\+&/,:;!?\|~]", " ", nm)
        for t in txt.split():
            t = t.strip().lower()
            if not t or t in STOPWORDS or len(t)<=1: continue
            toks.append(t)
    kw_top10 = [f"{k} {n}" for k,n in Counter(toks).most_common(10)]

    # ê¸°ë³¸ í†µê³„ & ì¸ì‚¬ì´íŠ¸
    uniq_cnt = cur['key'].nunique()
    keep_med = int(pts['days'].median()) if not pts.empty else 0
    g_up = b.sort_values('delta', ascending=False).head(3)
    g_dn = b.sort_values('delta', ascending=True).head(3)
    movers = []
    if prev_tbl is not None and not prev_tbl.empty:
        join = (pts[['key','mean_rank']]
                .merge(prev_tbl[['key','mean_rank']], on='key', suffixes=('_cur','_prev'), how='left'))
        join['improve'] = join['mean_rank_prev'] - join['mean_rank_cur']
        movers = join.dropna().sort_values('improve', ascending=False).head(3)['key'].tolist()

    insight_lines = []
    insight_lines.append(f"7ì¼ê°„ Top{topn}ì— ë“  ì´ ì œí’ˆ ìˆ˜: {uniq_cnt}ê°œ")
    insight_lines.append(f"Top{topn} ìœ ì§€ì¼ìˆ˜ ì¤‘ì•™ê°’: {keep_med}ì¼")
    if avg_disc is not None:
        insight_lines.append(f"í‰ê·  í• ì¸ìœ¨: {avg_disc:.2f}%")
    if med_price is not None:
        insight_lines.append(f"ì¤‘ìœ„ê°€ê²©: {fmt_money(med_price, src)}")
    if not g_up.empty:
        insight_lines.append("ë¸Œëœë“œ ìƒìŠ¹ Top3: " + ", ".join([f"{r.brand} {int(r.delta)}â†‘" for r in g_up.itertuples() if r.delta>0]) )
    if not g_dn.empty:
        insight_lines.append("ë¸Œëœë“œ í•˜ë½ Top3: " + ", ".join([f"{r.brand} {abs(int(r.delta))}â†“" for r in g_dn.itertuples() if r.delta<0]) )
    if movers:
        mv_names = []
        for k in movers:
            row = latest[latest['key']==k].iloc[-1] if not latest.empty else None
            nm = (row['product'] if row is not None else k)
            mv_names.append(nm)
        insight_lines.append("ê¸‰ìƒìŠ¹ ì•„ì´í…œ: " + ", ".join(mv_names))

    # ê²°ê³¼
    res.update({
        'range': f"{start.date()}~{end.date()}",
        'top10_lines': top_lines,
        'brand_lines': brand_lines,
        'inout': f"IN {in_cnt} / OUT {out_cnt} (ì¼í‰ê·  {daily_avg})",
        'heroes': to_links(heroes),
        'flash': to_links(flash),
        'discount': avg_disc,
        'median_price': med_price,
        'cat_top5': cat_pairs,
        'kw_top10': kw_top10,
        'insights': insight_lines,
        'stats': {'unique_items': uniq_cnt, 'keep_days_median': keep_med, 'topn': topn}
    })
    return res

# ---------- ìŠ¬ë™ í¬ë§· ----------
def format_slack_block(src:str, s:dict)->str:
    title_map = {
        'oy_kor':   "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",
        'oy_global':"ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",
        'amazon_us':"ì•„ë§ˆì¡´ US Top100",
        'qoo10_jp': "íí… ì¬íŒ¬ ë·°í‹° Top200",
        'daiso_kr': "ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200",
    }
    lines = []
    lines.append(f"ğŸ“Š ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title_map.get(src, src)} ({s['range']})")
    lines.append("ğŸ† Top10")
    lines.extend(s['top10_lines'] or ["ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append("ğŸ ë¸Œëœë“œ ê°œìˆ˜")   # í¼ì„¼íŠ¸ ì•„ë‹˜
    lines.extend(s['brand_lines'] or ["ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append(f"ğŸ” ì¸ì•¤ì•„ì›ƒ: {s['inout']}")
    if s['heroes']:
        lines.append("ğŸ†• ì‹ ê·œ íˆì–´ë¡œ: " + ", ".join(s['heroes']))
    if s['flash']:
        lines.append("âœ¨ ë°˜ì§ ì•„ì´í…œ: " + ", ".join(s['flash']))
    if s['cat_top5']:
        lines.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ ìƒìœ„: " + " Â· ".join(s['cat_top5']))
    if s['kw_top10']:
        lines.append("#ï¸âƒ£ í‚¤ì›Œë“œ Top10: " + ", ".join(s['kw_top10']))
    # ìµœì¢… ì¸ì‚¬ì´íŠ¸
    if s.get('insights'):
        lines.append("")
        lines.append("ğŸ§  ìµœì¢… ì¸ì‚¬ì´íŠ¸")
        for ln in s['insights']:
            lines.append(f"- {ln}")
    # ê°€ê²©/í• ì¸ (í†µí™”Â·ì½¤ë§ˆ í‘œê¸°)
    if s.get('median_price') is not None or s.get('discount') is not None:
        mp = s.get('median_price')
        price_txt = fmt_money(mp, src) if mp is not None else None
        disc_txt  = f"{s['discount']:.2f}%" if s.get('discount') is not None else None
        tail = " Â· ".join([t for t in [("ì¤‘ìœ„ê°€ê²© " + price_txt) if price_txt else None,
                                       ("í‰ê·  í• ì¸ìœ¨ " + disc_txt) if disc_txt else None] if t])
        if tail:
            lines.append("ğŸ’µ " + tail)
    return "\n".join(lines)

# ---------- ì—”íŠ¸ë¦¬ ----------
def main():
    data_dir = os.getenv('DATA_DIR','./data/daily')
    min_days = int(os.getenv('MIN_DAYS','3'))
    ud = load_unified(data_dir)
    result = {}
    slack_texts = []
    for src in ALL_SRCS:
        s = summarize_week(ud, src, min_days=min_days)
        result[src] = s
        slack_texts.append(format_slack_block(src, s))
    print(json.dumps(result, ensure_ascii=False, indent=2))
    with open("weekly_slack_message.txt","w",encoding="utf-8") as f:
        f.write("\n\nâ€” â€” â€”\n\n".join(slack_texts))

if __name__ == "__main__":
    main()
