#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
weekly_report_plus.py
- ì†ŒìŠ¤ë³„(oy_kor, oy_global, amazon_us, qoo10_jp, daiso_kr) ì£¼ê°„ ì§‘ê³„ & Slack ë©”ì‹œì§€ ìƒì„±
- ì…ë ¥: --src {oy_kor|oy_global|amazon_us|qoo10_jp|daiso_kr|all} --data-dir ./data/daily
- ì¶œë ¥:
  - slack_{src}.txt                     (ìŠ¬ë™ì— ë°”ë¡œ ì „ì†¡ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸)
  - weekly_summary_{src}.json          (HTML ë Œë”/ê²€ìˆ˜ìš© ìš”ì•½ ë°ì´í„°)
ì„¤ê³„ ìš”ì 
- ìµœê·¼ ì£¼(ì›”~ì¼)ì™€ ì „ì£¼(ì›”~ì¼)ë¥¼ ìë™ ì‚°ì¶œ
- TopN: ì†ŒìŠ¤ë³„ 100/200
- Top10: (ìœ ì§€ ì¼ìˆ˜ desc, í‰ê· ìˆœìœ„ asc) ì •ë ¬
- ë¸Œëœë“œ ê°œìˆ˜(ì¼í‰ê· )
- ì¸ì•¤ì•„ì›ƒ(êµì²´): ì¼í‰ê·  IN ê°œìˆ˜ë§Œ í‘œê¸°  ex) "ì¼í‰ê·  31.0ê°œ"
- íˆì–´ë¡œ(>=3ì¼ ìœ ì§€, ì „ì£¼ ë¯¸ë“±ì¥), ë°˜ì§(<=2ì¼)
- í‚¤ì›Œë“œ: ë§ˆì¼€íŒ…/ì¸í”Œë£¨ì–¸ì„œ/ì„±ë¶„  â†’ ê°€ë¡œ ë‚˜ì—´ ( Â· êµ¬ë¶„)
  Â· ë§ˆì¼€íŒ… í‚¤ì›Œë“œ ì™„ì „ ë¶„ë¦¬: ì˜¬ì˜í”½, PICK, íŠ¹ê°€, ì„¸íŠ¸, ê¸°íš, 1+1, ì¦ì •, í•œì •, NEW
  Â· ì¸í”Œë£¨ì–¸ì„œ: oy_korë§Œ ì§‘ê³„
- íˆì–´ë¡œ/ë°˜ì§ì€ ì„¸ë¡œ í‘œê¸°, ê° í’ˆëª©ì— URL í•˜ì´í¼ë§í¬ê°€ ë¶™ë„ë¡ JSONì—ë„ url ì €ì¥
"""

from __future__ import annotations
import argparse
import dataclasses
import json
import os
import re
from collections import defaultdict, Counter
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Set

import pandas as pd
from dateutil.parser import parse as dtparse

# -----------------------------
# ì†ŒìŠ¤ ìŠ¤í™/íŒŒì¼ëª… íŒ¨í„´/TopN
# -----------------------------

SRC_SPECS = {
    "oy_kor": {
        "title": "ì˜¬ë¦¬ë¸Œì˜ êµ­ë‚´ Top100",
        "topn": 100,
        "file_hint": ["ì˜¬ë¦¬ë¸Œì˜_ë­í‚¹"],
        "key": "goodsNo",  # URL query key
    },
    "oy_global": {
        "title": "ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ Top100",
        "topn": 100,
        "file_hint": ["ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹"],
        "key": "productId",
    },
    "amazon_us": {
        "title": "ì•„ë§ˆì¡´ US Top100",
        "topn": 100,
        "file_hint": ["ì•„ë§ˆì¡´US_ë·°í‹°_ë­í‚¹"],
        "key": "asin",
    },
    "qoo10_jp": {
        "title": "íí… ì¬íŒ¬ ë·°í‹° Top200",
        "topn": 200,
        "file_hint": ["íí…ì¬íŒ¬_ë·°í‹°_ë­í‚¹"],
        "key": "product_code",
    },
    "daiso_kr": {
        "title": "ë‹¤ì´ì†Œëª° ë·°í‹°/ìœ„ìƒ Top200",
        "topn": 200,
        "file_hint": ["ë‹¤ì´ì†Œëª°_ë·°í‹°ìœ„ìƒ_ì¼ê°„"],
        "key": "pdNo",
    },
}

# -----------------------------
# ë§ˆì¼€íŒ…/ì¸í”Œ/ì„±ë¶„ í‚¤ì›Œë“œ (ì •ê·œì‹)
#  - 1+1, ì¦ì • ì™„ì „ ë¶„ë¦¬
#  - PICK(ì½œë¼ë³´) vs ì˜¬ì˜í”½ ë¶„ë¦¬
# -----------------------------

def rx(p): return re.compile(p, re.I)

PAT_MARKETING = {
    "ì˜¬ì˜í”½": rx(r"(?:^|\s)ì˜¬ì˜í”½(?:\s|$)|ì˜¬ë¦¬ë¸Œì˜\s*í”½"),
    "PICK":   rx(r"\bPICK\b"),
    "íŠ¹ê°€":   rx(r"(íŠ¹ê°€|í•«ë”œ|ë”œ|ì„¸ì¼|í• ì¸)"),
    "ì„¸íŠ¸":   rx(r"(ì„¸íŠ¸|íŒ¨í‚¤ì§€|íŠ¸ë¦¬ì˜¤|ë“€ì˜¤|ì„¸íŠ¸í‚·|í‚¤íŠ¸|í‚·\b)"),
    "ê¸°íš":   rx(r"(ê¸°íš|ê¸°íšì „)"),
    "1+1":    rx(r"(?:^|\s)1\+1(?:\s|$)"),
    "ì¦ì •":   rx(r"(ì¦ì •|ì‚¬ì€í’ˆ)"),
    "í•œì •":   rx(r"(í•œì •|ë¦¬ë¯¸í‹°ë“œ)"),
    "NEW":    rx(r"\bNEW\b|(?<!ë¦¬)ë‰´\b"),
}

# ì¸í”Œë£¨ì–¸ì„œ: oy_korë§Œ
INFLUENCERS = [
    # ì˜ˆì‹œ(ì§€ì† í™•ì¥ ê°€ëŠ¥, 'Pick'ê³¼ëŠ” ë¬´ê´€í•˜ê²Œ ì´ë¦„ë§Œ ì¡ì•„ë„ ì§‘ê³„)
    "ìœ ì¸", "ì–´í”„ì–´í”„", "Olad", "ì˜¬ë¼ë“œ", "ë°•ë³´ì˜", "ìœˆí„°", "í•˜ì¸ íˆ¬í•˜ì¸ ", "í•˜ë£¨",
    "í—ˆìœ¤ì§„", "ì´ìœ ì •", "ë¬¸ê°€ì˜", "ì¹´ë¦¬ë‚˜", "ì¥ì›ì˜", "ìˆ˜ì§€", "ì•„ì´ë¸Œ", "ë‰´ì§„ìŠ¤",
]

# ì„±ë¶„(ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ + ëŒ€ë¬¸ì ì•½ì–´ë¥˜)
INGREDIENTS = [
    "ë ˆí‹°ë†€", "ë¹„íƒ€ë¯¼C", "ì½œë¼ê²", "íˆì•Œë£¨ë¡ ì‚°", "ì„¸ë¼ë§ˆì´ë“œ", "íŒí…Œë†€",
    "ë§ˆë°ì¹´ì†Œì‚¬ì´ë“œ", "ë³‘í’€", "í‹°íŠ¸ë¦¬", "ë…¹ì°¨", "ì‘¥", "ì—°ì–´", "í©íƒ€ì´ë“œ",
    "PDRN", "AHA", "BHA", "PHA", "ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œ", "ì•„ì ¤ë¼ìµ", "ì§•í¬", "ìœ ì‚°ê· ",
]


# -----------------------------
# ë°ì´í„° ëª¨ë¸
# -----------------------------

@dataclass
class ItemStat:
    sku: str
    raw_name: str
    url: str
    days: int
    ranks: List[int]

    @property
    def avg_rank(self) -> float:
        return float(sum(self.ranks)) / max(1, len(self.ranks))

    @property
    def min_rank(self) -> int:
        return min(self.ranks) if self.ranks else 9999


# -----------------------------
# ìœ í‹¸
# -----------------------------

def to_date(s: str) -> datetime:
    try:
        return dtparse(s).date()
    except Exception:
        return datetime.strptime(s, "%Y-%m-%d").date()

def monday_of(date_: datetime) -> datetime:
    return date_ - timedelta(days=date_.weekday())  # Monday=0

def daterange(start: datetime, end: datetime) -> List[datetime]:
    d = start
    out = []
    while d <= end:
        out.append(d)
        d += timedelta(days=1)
    return out

def get_week_range_for_latest(dates: List[datetime]) -> Tuple[List[datetime], List[datetime]]:
    """íŒŒì¼ë“¤ ì•ˆì˜ ê°€ì¥ ìµœì‹  ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 'ê·¸ ì£¼ ì›”~ì¼', 'ê·¸ ì „ì£¼ ì›”~ì¼' ë²”ìœ„ ë°˜í™˜."""
    if not dates:
        return [], []
    last = max(dates)
    cur_mon = monday_of(last)
    cur_sun = cur_mon + timedelta(days=6)
    prev_mon = cur_mon - timedelta(days=7)
    prev_sun = prev_mon + timedelta(days=6)
    return daterange(cur_mon, cur_sun), daterange(prev_mon, prev_sun)

def extract_id_from_url(url: str, key: str) -> str:
    if not isinstance(url, str):
        return ""
    # ë‹¨ìˆœ query search
    m = re.search(r"[?&]{}=([^&#]+)".format(re.escape(key)), url)
    if m:
        return m.group(1)
    # ì•„ë§ˆì¡´ asinì´ pathì— ìˆì„ ìˆ˜ ìˆìŒ
    if key == "asin":
        m2 = re.search(r"/([A-Z0-9]{10})(?:[/?#]|$)", url)
        if m2:
            return m2.group(1)
    return ""

def pick_first(*vals):
    for v in vals:
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""


# -----------------------------
# ë¡œë”©/ìŠ¤ìº”
# -----------------------------

def discover_files(data_dir: str, src: str) -> Dict[datetime, str]:
    """data_dirì—ì„œ srcì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì„ ë‚ ì§œë³„ë¡œ ë§¤í•‘."""
    hints = SRC_SPECS[src]["file_hint"]
    files = {}
    for fn in os.listdir(data_dir):
        if not fn.endswith(".csv"):
            continue
        if not any(h in fn for h in hints):
            continue
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        m = re.search(r"(\d{4}-\d{2}-\d{2})", fn)
        if not m:
            continue
        d = to_date(m.group(1))
        files[d] = os.path.join(data_dir, fn)
    return dict(sorted(files.items()))

def load_day_df(path: str, src: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    # í‘œì¤€í™” ì»¬ëŸ¼ ì¶”ë¡ 
    url_col = None
    for c in ["url", "URL", "link", "ìƒí’ˆURL", "ìƒí’ˆë§í¬"]:
        if c in df.columns:
            url_col = c; break
    name_col = None
    for c in ["raw_name", "name", "ìƒí’ˆëª…", "title", "Title"]:
        if c in df.columns:
            name_col = c; break
    brand_col = None
    for c in ["brand", "ë¸Œëœë“œ", "Brand"]:
        if c in df.columns:
            brand_col = c; break
    rank_col = None
    for c in ["rank", "ìˆœìœ„", "ë­í‚¹"]:
        if c in df.columns:
            rank_col = c; break

    if rank_col is None:
        raise KeyError("rank/ìˆœìœ„ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: " + path)

    df["_rank"] = df[rank_col].astype(int)

    df["_url"] = df[url_col] if url_col else ""
    df["_raw_name"] = df[name_col] if name_col else ""
    df["_brand"] = df[brand_col] if brand_col else ""

    # SKU í‚¤ ê²°ì •
    key = SRC_SPECS[src]["key"]
    def _id(row):
        return extract_id_from_url(str(row["_url"]), key) or str(row.get(key, "") or "")
    df["_sku"] = df.apply(_id, axis=1)

    # TopN ì œí•œ
    topn = SRC_SPECS[src]["topn"]
    df = df.sort_values("_rank").head(topn).reset_index(drop=True)
    return df[["_sku", "_raw_name", "_url", "_brand", "_rank"]]


# -----------------------------
# ì£¼ê°„ ì§‘ê³„
# -----------------------------

def weekly_stats(day_dfs: Dict[datetime, pd.DataFrame]) -> Dict[str, ItemStat]:
    """ì¼ìë³„ DF -> SKUë³„ ì£¼ê°„ ìŠ¤íƒ¯"""
    by_sku: Dict[str, ItemStat] = {}
    for d, df in day_dfs.items():
        for row in df.itertuples(index=False):
            sku = row._sku
            if not sku:
                # URL í‚¤ê°€ ì—†ìœ¼ë©´ raw_name ê¸°ì¤€ fallback(í¬ê·€ ì¼€ì´ìŠ¤)
                sku = f"RAW::{row._raw_name}"
            it = by_sku.get(sku)
            if not it:
                it = ItemStat(sku=sku, raw_name=row._raw_name, url=row._url, days=0, ranks=[])
                by_sku[sku] = it
            it.days += 1
            it.ranks.append(int(row._rank))
            # URL/ì´ë¦„ ê°±ì‹ (ê°€ì¥ ìµœê·¼ ê²ƒ ìš°ì„ )
            if row._url:
                it.url = row._url
            if row._raw_name:
                it.raw_name = row._raw_name
    return by_sku

def daily_sets(day_dfs: Dict[datetime, pd.DataFrame]) -> List[Tuple[datetime, Set[str]]]:
    out = []
    for d, df in sorted(day_dfs.items()):
        out.append((d, set(df["_sku"].tolist())))
    return out

def average_inout(day_sets: List[Tuple[datetime, Set[str]]]) -> float:
    if len(day_sets) < 2:
        return 0.0
    changes = []
    for (d1, s1), (d2, s2) in zip(day_sets, day_sets[1:]):
        ins = len(s2 - s1)
        changes.append(ins)
    if not changes:
        return 0.0
    return round(sum(changes) / len(changes), 1)

def brand_daily_average(day_dfs: Dict[datetime, pd.DataFrame]) -> List[str]:
    # ì¼ìë³„ ë¸Œëœë“œ ì¹´ìš´íŠ¸ â†’ ì£¼ê°„ í‰ê· 
    per_day = []
    for d, df in day_dfs.items():
        c = Counter([str(x).strip() for x in df["_brand"].fillna("").tolist() if str(x).strip()])
        per_day.append(c)
    if not per_day:
        return ["ë°ì´í„° ì—†ìŒ"]

    brands = set()
    for c in per_day:
        brands.update(c.keys())
    avg = []
    for b in brands:
        avg_cnt = sum(c.get(b, 0) for c in per_day) / len(per_day)
        avg.append((b, avg_cnt))
    avg.sort(key=lambda x: (-x[1], x[0]))
    lines = [f"{b} {round(v,1)}ê°œ/ì¼" for b, v in avg[:20]]  # ìƒìœ„ 20ê°œë§Œ
    return lines or ["ë°ì´í„° ì—†ìŒ"]

def top10_lines_from_stats(stats: Dict[str, ItemStat]) -> List[str]:
    # (ìœ ì§€ì¼ìˆ˜ desc, í‰ê· ìˆœìœ„ asc) ì •ë ¬
    arr = sorted(stats.values(), key=lambda s: (-s.days, s.avg_rank, s.min_rank))
    out = []
    for i, it in enumerate(arr[:10], start=1):
        tail = f"(ìœ ì§€ {it.days}ì¼ Â· í‰ê·  {it.avg_rank:.1f}ìœ„)"
        if it.url:
            nm = f"<{it.url}|{it.raw_name}>"
        else:
            nm = it.raw_name
        out.append(f"{i}. {nm} {tail}")
    return out or ["ë°ì´í„° ì—†ìŒ"]

def hero_and_flash(stats: Dict[str, ItemStat], prev_stats: Dict[str, ItemStat]) -> Tuple[List[ItemStat], List[ItemStat]]:
    # íˆì–´ë¡œ: ì´ë²ˆì£¼ 3ì¼ ì´ìƒ & ì „ì£¼ì— ì—†ë˜ SKU
    heroes = [s for sku, s in stats.items() if s.days >= 3 and sku not in prev_stats]
    flashes = [s for sku, s in stats.items() if s.days <= 2]
    heroes.sort(key=lambda s: (-s.days, s.avg_rank, s.min_rank))
    flashes.sort(key=lambda s: (s.days, s.avg_rank, s.min_rank))
    return heroes[:10], flashes[:10]


# -----------------------------
# í‚¤ì›Œë“œ ì§‘ê³„
# -----------------------------

def keyword_stats(stats: Dict[str, ItemStat], src: str) -> Dict[str, Dict[str, int]]:
    """
    unique: ì£¼ê°„ ìœ ë‹ˆí¬ SKU ìˆ˜
    marketing: {í‚¤ì›Œë“œ: ì¹´ìš´íŠ¸} (SKU ê¸°ì¤€)
    influencers: {ì´ë¦„: ì¹´ìš´íŠ¸}  (oy_korë§Œ)
    ingredients: {ì„±ë¶„: ì¹´ìš´íŠ¸}
    """
    unique = len(stats)
    mk = Counter()
    infl = Counter()
    ing = Counter()

    for s in stats.values():
        name = s.raw_name or ""
        # ë§ˆì¼€íŒ…
        for k, r in PAT_MARKETING.items():
            if r.search(name):
                mk[k] += 1
        # ì¸í”Œ (oy_korë§Œ)
        if src == "oy_kor":
            for p in INFLUENCERS:
                if re.search(re.escape(p), name, re.I):
                    infl[p] += 1
        # ì„±ë¶„
        for p in INGREDIENTS:
            if re.search(r"\b" + re.escape(p) + r"\b", name, re.I):
                ing[p] += 1

    # ì •ë ¬
    mk = Counter(dict(sorted(mk.items(), key=lambda x: (-x[1], x[0]))))
    infl = Counter(dict(sorted(infl.items(), key=lambda x: (-x[1], x[0]))))
    ing = Counter(dict(sorted(ing.items(), key=lambda x: (-x[1], x[0]))))

    return {
        "unique": unique,
        "marketing": dict(mk),
        "influencers": dict(infl),
        "ingredients": dict(ing),
    }

def format_kw_for_slack(kw: Dict[str, any]) -> str:
    if kw.get("unique", 0) == 0:
        return "ë°ì´í„° ì—†ìŒ"

    def pct(cnt: int) -> float:
        return round(cnt * 100.0 / max(1, kw["unique"]), 1)

    lines = []
    lines.append("ğŸ“Š *ì£¼ê°„ í‚¤ì›Œë“œ ë¶„ì„*")
    lines.append(f"- ìœ ë‹ˆí¬ SKU: {kw['unique']}ê°œ")

    if kw["marketing"]:
        mk_parts = [f"{k} {v}ê°œ({pct(v)}%)" for k, v in kw["marketing"].items()]
        lines.append("â€¢ *ë§ˆì¼€íŒ… í‚¤ì›Œë“œ* " + " Â· ".join(mk_parts))

    if kw["influencers"]:
        infl_parts = [f"{k} {v}ê°œ" for k, v in kw["influencers"].items()]
        lines.append("â€¢ *ì¸í”Œë£¨ì–¸ì„œ* " + " Â· ".join(infl_parts))

    if kw["ingredients"]:
        ing_parts = [f"{k} {v}ê°œ" for k, v in kw["ingredients"].items()]
        lines.append("â€¢ *ì„±ë¶„ í‚¤ì›Œë“œ* " + " Â· ".join(ing_parts))

    return "\n".join(lines)


# -----------------------------
# Slack/JSON ë¹Œë”
# -----------------------------

def build_slack(
    src: str,
    range_str: str,
    top10_lines: List[str],
    brand_lines: List[str],
    inout_avg: float,
    heroes: List[ItemStat],
    flashes: List[ItemStat],
    kw_text: str,
    unique_cnt: int,
    keep_days_mean: float,
) -> str:
    title = SRC_SPECS[src]["title"]
    lines: List[str] = []
    lines.append(f"ğŸ“ˆ *ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {title} ({range_str})*")
    lines.append("")
    lines.append("ğŸ† *Top10*")
    lines += (top10_lines or ["ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append("ğŸ“¦ *ë¸Œëœë“œ ê°œìˆ˜(ì¼í‰ê· )*")
    lines += (brand_lines or ["ë°ì´í„° ì—†ìŒ"])
    lines.append("")
    lines.append("ğŸ” *ì¸ì•¤ì•„ì›ƒ(êµì²´)*")
    lines.append(f"- ì¼í‰ê·  {inout_avg}ê°œ")
    lines.append("")
    # íˆì–´ë¡œ/ë°˜ì§: ì„¸ë¡œ + ë§í¬
    lines.append("ğŸ†• *ì‹ ê·œ íˆì–´ë¡œ(â‰¥3ì¼ ìœ ì§€)*")
    if not heroes:
        lines.append("ì—†ìŒ")
    else:
        for st in heroes:
            nm = f"<{st.url}|{st.raw_name}>" if st.url else st.raw_name
            lines.append(f"- {nm} (ìœ ì§€ {st.days}ì¼ Â· í‰ê·  {st.avg_rank:.1f}ìœ„)")
    lines.append("âœ¨ *ë°˜ì§ ì•„ì´í…œ(â‰¤2ì¼)*")
    if not flashes:
        lines.append("ì—†ìŒ")
    else:
        for st in flashes:
            nm = f"<{st.url}|{st.raw_name}>" if st.url else st.raw_name
            lines.append(f"- {nm} (ìœ ì§€ {st.days}ì¼ Â· í‰ê·  {st.avg_rank:.1f}ìœ„)")
    lines.append("")
    lines.append("ğŸ“Œ *í†µê³„*")
    lines.append(f"- Top{SRC_SPECS[src]['topn']} ë“±ê·¹ SKU : {unique_cnt}ê°œ")
    lines.append(f"- Top {SRC_SPECS[src]['topn']} ìœ ì§€ í‰ê·  : {keep_days_mean:.1f}ì¼")
    lines.append("")
    if kw_text:
        lines.append(kw_text)
    return "\n".join(lines)

def item_to_dict(st: ItemStat) -> Dict[str, any]:
    return {
        "name": st.raw_name,
        "url": st.url,
        "days": st.days,
        "avg": round(st.avg_rank, 1),
    }


# -----------------------------
# íŒŒì´í”„ë¼ì¸
# -----------------------------

def run_for_source(src: str, args) -> Tuple[str, str]:
    """
    ë°˜í™˜: (ìŠ¬ë™ í…ìŠ¤íŠ¸, ìš”ì•½ json ë¬¸ìì—´)
    """
    files = discover_files(args.data_dir, src)
    if not files:
        # ë¹ˆ íŒŒì¼ ìƒì„±
        empty = {
            "range": "ë°ì´í„° ì—†ìŒ",
            "title": SRC_SPECS[src]["title"],
            "topn": SRC_SPECS[src]["topn"],
            "top10_items": [],
            "brand_lines": ["ë°ì´í„° ì—†ìŒ"],
            "inout_avg": 0.0,
            "heroes": [],
            "flashes": [],
            "kw": {"unique": 0, "marketing": {}, "influencers": {}, "ingredients": {}},
            "unique_cnt": 0,
            "keep_days_mean": 0.0,
        }
        slack = f"ğŸ“ˆ *ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {SRC_SPECS[src]['title']}*  \në°ì´í„° ì—†ìŒ"
        return slack, json.dumps(empty, ensure_ascii=False, indent=2)

    # ë‚ ì§œ ë²”ìœ„ ì‚°ì¶œ (ìµœê·¼ ì£¼ & ì „ì£¼)
    all_dates = list(files.keys())
    cur_week, prev_week = get_week_range_for_latest(all_dates)

    def pick_week(day_list: List[datetime]) -> Dict[datetime, pd.DataFrame]:
        m = {}
        for d in day_list:
            if d in files:
                m[d] = load_day_df(files[d], src)
        return m

    cur_daydfs = pick_week(cur_week)
    prev_daydfs = pick_week(prev_week)

    range_str = f"{cur_week[0]}~{cur_week[-1]}" if cur_week else "ë°ì´í„° ì—†ìŒ"
    if not cur_daydfs:
        # ë°ì´í„° ì—†ìŒ ì²˜ë¦¬
        empty = {
            "range": range_str,
            "title": SRC_SPECS[src]["title"],
            "topn": SRC_SPECS[src]["topn"],
            "top10_items": [],
            "brand_lines": ["ë°ì´í„° ì—†ìŒ"],
            "inout_avg": 0.0,
            "heroes": [],
            "flashes": [],
            "kw": {"unique": 0, "marketing": {}, "influencers": {}, "ingredients": {}},
            "unique_cnt": 0,
            "keep_days_mean": 0.0,
        }
        slack = f"ğŸ“ˆ *ì£¼ê°„ ë¦¬í¬íŠ¸ Â· {SRC_SPECS[src]['title']} ({range_str})*\në°ì´í„° ì—†ìŒ"
        return slack, json.dumps(empty, ensure_ascii=False, indent=2)

    # ì£¼ê°„ ìŠ¤íƒ¯
    cur_stats = weekly_stats(cur_daydfs)
    prev_stats = weekly_stats(prev_daydfs) if prev_daydfs else {}

    top10_lines = top10_lines_from_stats(cur_stats)
    brand_lines = brand_daily_average(cur_daydfs)
    inout_avg = average_inout(daily_sets(cur_daydfs))
    unique_cnt = len(cur_stats)
    keep_days_mean = round(sum(s.days for s in cur_stats.values()) / max(1, unique_cnt), 1)

    heroes, flashes = hero_and_flash(cur_stats, prev_stats)
    kw = keyword_stats(cur_stats, src)
    kw_text = format_kw_for_slack(kw)

    slack_text = build_slack(
        src=src,
        range_str=range_str,
        top10_lines=top10_lines,
        brand_lines=brand_lines,
        inout_avg=inout_avg,
        heroes=heroes,
        flashes=flashes,
        kw_text=kw_text,
        unique_cnt=unique_cnt,
        keep_days_mean=keep_days_mean,
    )

    summary = {
        "range": range_str,
        "title": SRC_SPECS[src]["title"],
        "topn": SRC_SPECS[src]["topn"],
        "top10_items": top10_lines,
        "brand_lines": brand_lines,
        "inout_avg": inout_avg,
        "heroes": [item_to_dict(x) for x in heroes],
        "flashes": [item_to_dict(x) for x in flashes],
        "kw": kw,
        "unique_cnt": unique_cnt,
        "keep_days_mean": keep_days_mean,
    }

    return slack_text, json.dumps(summary, ensure_ascii=False, indent=2)


# -----------------------------
# main/CLI
# -----------------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--src", default="all",
                        choices=["oy_kor", "oy_global", "amazon_us", "qoo10_jp", "daiso_kr", "all"])
    parser.add_argument("--data-dir", default="./data/daily")
    args = parser.parse_args()

    targets = [args.src] if args.src != "all" else list(SRC_SPECS.keys())

    os.makedirs(".", exist_ok=True)
    for src in targets:
        slack_text, summary_json = run_for_source(src, args)

        # íŒŒì¼ ì¶œë ¥
        slack_fn = f"slack_{src}.txt"
        json_fn = f"weekly_summary_{src}.json"
        with open(slack_fn, "w", encoding="utf-8") as f:
            f.write(slack_text)
        with open(json_fn, "w", encoding="utf-8") as f:
            f.write(summary_json)

        print(f"[OK] {src}: {slack_fn}, {json_fn} ìƒì„±")


if __name__ == "__main__":
    main()
